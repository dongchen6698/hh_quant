{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import akshare as ak\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from database.downloader.downloader_base import DownloaderBase\n",
    "import database.database_config as db_config\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 05:50:06.604091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 05:50:07.433172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.13.1\n",
      "TensorFlow GPU version is installed\n",
      "GPU devices available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 只使用CPU进行训练\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# 打印Tensorflow版本\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "\n",
    "# 检查是否有可用的GPU设备\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"TensorFlow GPU version is installed\")\n",
    "else:\n",
    "    print(\"TensorFlow CPU version is installed\")\n",
    "\n",
    "# 检查TensorFlow是否能够访问GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU devices available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU devices found. Running on CPU.\")\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# 绘图相关函数\n",
    "def plot_series_dist(series):\n",
    "    data = series\n",
    "    plt.figure(figsize=(5,5))\n",
    "    # 使用matplotlib画直方图\n",
    "    plt.hist(data, bins=60, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Data')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'mean_absolute_error', 'mean_squared_error']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.ylim([0, plt.ylim()[1]])\n",
    "    plt.legend()\n",
    "\n",
    "def plot_cm(true_labels, pred_labels):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"g\", cmap='Blues')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    recall = recall_score(true_labels, pred_labels)\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "\n",
    "def plot_close_label(df, label_name='label'):\n",
    "    # 设置图像的大小\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    # 绘制收盘价曲线\n",
    "    plt.plot(df['datetime'], df['close'], label='Close Price', color='blue')\n",
    "    # 提取买入和卖出点\n",
    "    buy_points = df[df[label_name] == 1]\n",
    "    sell_points = df[df[label_name] == 2]\n",
    "    # 在买入点绘制上升三角形标记\n",
    "    plt.scatter(buy_points['datetime'], buy_points['close'], label='Buy', color='green', marker='^', alpha=1)\n",
    "    # 在卖出点绘制下降三角形标记\n",
    "    plt.scatter(sell_points['datetime'], sell_points['close'], label='Sell', color='red', marker='v', alpha=1)\n",
    "    # 增加标题和标签\n",
    "    plt.title('Stock Price with Buy and Sell Signals')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    # 显示图例\n",
    "    plt.legend()\n",
    "    # 展示图形\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "    def __init__(self, db_downloader:DownloaderBase) -> None:\n",
    "        self.db_downloader = db_downloader\n",
    "\n",
    "    def _build_cls_label(self, stock_dataframe):\n",
    "        \"\"\"\n",
    "        明日开始未来N天内优先触发止盈 = 1, 触发止损=2, 其他=0\n",
    "        \"\"\"\n",
    "        def calculate_atr(df, period=14):\n",
    "            df['high-low'] = df['high'] - df['low']\n",
    "            df['high-close_prev'] = abs(df['high'] - df['close'].shift(1))\n",
    "            df['low-close_prev'] = abs(df['low'] - df['close'].shift(1))\n",
    "            df['tr'] = df[['high-low', 'high-close_prev', 'low-close_prev']].max(axis=1)\n",
    "            atr = df['tr'].rolling(window=period, min_periods=1).mean()\n",
    "            return atr\n",
    "\n",
    "        # 初始化标签参数\n",
    "        N = 15 # 时间周期\n",
    "        ATR_period = 14 # ATR计算周期\n",
    "        ATR_take_profit_factor = 2.5 # 止盈参数\n",
    "        ATR_stop_loss_factor = 1.5\n",
    "        # 开始构建标签\n",
    "        df = stock_dataframe.copy()\n",
    "        # 计算标签构建所需要的指标\n",
    "        df['atr'] = calculate_atr(df, period=ATR_period) # 计算每一天的ATR\n",
    "        df['label'] = 0  # 初始化标签列\n",
    "        df['return'] = np.NaN # 初始化收益率列\n",
    "        # 轮询判断先止盈还是先止损\n",
    "        for index in range(len(df)-N-1):\n",
    "            buy_price = df.at[index + 1, 'open'] # 第二天的开盘价作为买入价\n",
    "            buy_atr = df.at[index, 'atr'] # 获取目前的ATR\n",
    "            take_profit_price = buy_price + ATR_take_profit_factor * buy_atr # 提前确定止盈价格\n",
    "            stop_loss_price = buy_price - ATR_stop_loss_factor * buy_atr # 提前确定止损价格\n",
    "            for day in range(2, N+2):\n",
    "                future_day_close = df.at[index+day, 'close'] # 买入后每天的收盘价\n",
    "                # 检查价格是否触发止盈或止损条件\n",
    "                if future_day_close > take_profit_price:\n",
    "                    df.at[index, 'label'] = 1  # 未来N日走势上升 + 突破止盈\n",
    "                    df.at[index, 'return'] = (future_day_close / buy_price) - 1\n",
    "                    break  # 退出内循环\n",
    "                elif future_day_close < stop_loss_price:\n",
    "                    df.at[index, 'label'] = 2  # 未来N日走势下降 + 突破止损\n",
    "                    df.at[index, 'return'] = (future_day_close / buy_price) - 1\n",
    "                    break  # 退出内循环\n",
    "            else:\n",
    "                df.at[index, 'return'] = (future_day_close / buy_price) - 1\n",
    "        # 过滤第二天一字涨停情况\n",
    "        # df = df[df['high'].shift(-1) != df['low'].shift(-1)]\n",
    "        return df[['datetime', 'label', 'return']]\n",
    "\n",
    "    def _process_one_stock(self, stock_code, start_date, end_date):\n",
    "        stock_base = self.db_downloader._download_stock_base_info(stock_code) # 获取基础代码\n",
    "        stock_individual = self.db_downloader._download_stock_individual_info(stock_code) # 获取profile信息\n",
    "        stock_history = self.db_downloader._download_stock_history_info(stock_code, start_date, end_date) # 获取历史行情\n",
    "        stock_indicator = self.db_downloader._download_stock_indicator_info(stock_code, start_date, end_date) # 获取指标数据\n",
    "        stock_factor_date = self.db_downloader._download_stock_factor_date_info() # 获取日期特征\n",
    "        stock_factor_qlib = self.db_downloader._download_stock_factor_qlib_info(stock_code, start_date, end_date) # 获取量价特征\n",
    "        stock_label = self._build_cls_label(stock_history, ) # 构建Label\n",
    "        stock_df = stock_base.merge(stock_individual, on=['stock_code']).merge(stock_history, on=['stock_code']).merge(stock_indicator, on=['stock_code', 'datetime']).merge(stock_label, on=['datetime']).merge(stock_factor_date, on=['datetime']).merge(stock_factor_qlib, on=['stock_code', 'datetime']) # 整合数据\n",
    "        stock_df = stock_base \\\n",
    "            .merge(stock_individual, on=['stock_code', 'stock_name']) \\\n",
    "            .merge(stock_history, on=['stock_code']) \\\n",
    "            .merge(stock_indicator, on=['stock_code', 'datetime']) \\\n",
    "            .merge(stock_label, on=['datetime']) \\\n",
    "            .merge(stock_factor_date, on=['datetime']) \\\n",
    "            .merge(stock_factor_qlib, on=['stock_code', 'datetime']) # 整合数据\n",
    "        stock_df = stock_df.dropna()\n",
    "        return stock_df\n",
    "    \n",
    "    def _process_all_stock(self, code_type, start_date, end_date):\n",
    "        # stock_code_list = list(ak.stock_info_a_code_name()['code'].unique()) # 获取A股所有股票列表\n",
    "        stock_code_list = list(ak.index_stock_cons(code_type)['品种代码'].unique()) # 获取沪深300的股票代码列表\n",
    "        stock_df_list = []\n",
    "        for stock_code in tqdm(stock_code_list, desc=f'Process: {code_type} ...'):\n",
    "            stock_df = self._process_one_stock(stock_code, start_date, end_date)\n",
    "            if not stock_df.empty:\n",
    "                stock_df_list.append(stock_df)\n",
    "        return pd.concat(stock_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据库初始化\n",
    "db_conn = sqlite3.connect('../database/hh_quant.db')\n",
    "db_downloader = DownloaderBase(db_conn, db_config)\n",
    "proprocessor = PreProcessing(db_downloader=db_downloader)\n",
    "# df = proprocessor._process_all_stock('000016', '20120101', '20171231')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.stock_code.unique())\n",
    "# sample_df = df[df['stock_code'] == '601601']\n",
    "# plot_close_label(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_data_period(backtest_start_date, backtest_duration=5, train_period=6, val_period=0.5, test_period=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        backtest_start_date (_type_): _description_\n",
    "        backtest_duration (int, optional): _description_. Defaults to 5.\n",
    "        train_period (int, optional): _description_. Defaults to 6.\n",
    "        val_period (float, optional): _description_. Defaults to 0.5.\n",
    "        test_period (float, optional): _description_. Defaults to 0.5.\n",
    "    Returns:\n",
    "        result: _description_\n",
    "    \"\"\"\n",
    "    backtest_start_date = datetime.strptime(backtest_start_date, '%Y%m%d')\n",
    "    backtest_end_date = backtest_start_date + relativedelta(years=backtest_duration) # 回测5年数据\n",
    "    train_period = relativedelta(years=train_period) # 使用6年的训练数据\n",
    "    val_period = relativedelta(months=(12 * val_period)) # 使用半年的验证数据\n",
    "    test_period = relativedelta(months=(12 * test_period)) # 使用半年的测试数据(半年模型一更新)\n",
    "\n",
    "    result = []\n",
    "    rolling_flag = True\n",
    "    bench_date = backtest_start_date\n",
    "    while rolling_flag:\n",
    "        if bench_date < backtest_end_date:\n",
    "            test_start, test_end = bench_date, (bench_date + test_period - relativedelta(days=1))\n",
    "            val_start, val_end = (test_start - relativedelta(days=1) - val_period), (test_start - relativedelta(days=1))\n",
    "            train_start, train_end =(val_start - relativedelta(days=1) - train_period), (val_start - relativedelta(days=1))\n",
    "            result.append({\n",
    "                \"train\": [train_start.strftime(\"%Y%m%d\"), train_end.strftime(\"%Y%m%d\")],\n",
    "                \"val\": [val_start.strftime(\"%Y%m%d\"), val_end.strftime(\"%Y%m%d\")],\n",
    "                \"test\": [test_start.strftime(\"%Y%m%d\"), test_end.strftime(\"%Y%m%d\")]\n",
    "            })\n",
    "            bench_date += test_period\n",
    "        else:\n",
    "            rolling_flag = False \n",
    "    return result\n",
    "\n",
    "def extract_train_val_data(df, train_start_date, train_end_date, val_start_date, val_end_date, test_start_date, test_end_date):\n",
    "    train_start_date = pd.to_datetime(train_start_date)\n",
    "    train_end_date = pd.to_datetime(train_end_date)\n",
    "    val_start_date = pd.to_datetime(val_start_date)\n",
    "    val_end_date = pd.to_datetime(val_end_date)\n",
    "    test_start_date = pd.to_datetime(test_start_date)\n",
    "    test_end_date = pd.to_datetime(test_end_date)\n",
    "\n",
    "    train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "    val_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "    test_data = df[(pd.to_datetime(df['datetime']) >= test_start_date) & (pd.to_datetime(df['datetime']) <= test_end_date)]\n",
    "\n",
    "    print(f\"train_data_size: {train_data.shape}\")\n",
    "    print(f\"validation_data_size: {val_data.shape}\")\n",
    "    print(f\"test_data_size: {test_data.shape}\")\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=True, batch_size=32):\n",
    "    features = dataframe[feature_cols]\n",
    "    labels = tuple([dataframe[col] for col in label_cols])\n",
    "    # labels = dataframe[label_cols]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(features), 10000))\n",
    "    ds = ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关闭滚动回测...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train': ['20120101', '20171231'],\n",
       "  'val': ['20180101', '20181231'],\n",
       "  'test': ['20190101', '20231231']}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 相关配置\n",
    "rolling_flag = False\n",
    "benchmark = '000905'\n",
    "feature_config = {\n",
    "    \"target_features\": ['label', 'return'],\n",
    "    \"numeric_features\": ['turnover_rate', 'pe_ttm', 'ps_ttm', 'pcf_ncf_ttm', 'pb_mrq', 'KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60'],\n",
    "    \"integer_categorical_features\": ['month'],\n",
    "    \"string_categorical_features\": ['industry', 'season'],\n",
    "}\n",
    "batch_size = 1024\n",
    "\n",
    "# 是否开启滚动训练&回测\n",
    "if rolling_flag:\n",
    "    print(\"开启滚动回测...\")\n",
    "    backtest_period = get_rolling_data_period(\n",
    "        backtest_start_date='20200101', # 回测开始日期\n",
    "        backtest_duration=4, # 一共回测多久的数据（单位：年）\n",
    "        train_period=6, # 使用过去多久的时间进行训练（单位：年）\n",
    "        val_period=1, # 验证数据周期（单位：年）\n",
    "        test_period=1, # 测试数据周期（单位：年）\n",
    "    )\n",
    "else:\n",
    "    print(\"关闭滚动回测...\")\n",
    "    backtest_period = [\n",
    "        {\n",
    "            'train': ['20120101', '20171231'],\n",
    "            'val': ['20180101', '20181231'],\n",
    "            'test': ['20190101', '20231231']\n",
    "        }\n",
    "    ]\n",
    "\n",
    "backtest_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_columns = feature_config.get('numeric_features', []) + feature_config.get('integer_categorical_features', []) + feature_config.get('string_categorical_features', [])\n",
    "# label_columns = feature_config.get('target_features', [])\n",
    "# ds = df_to_dataset(df, feature_columns, label_columns, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['20120101', '20171231'], 'val': ['20180101', '20181231'], 'test': ['20190101', '20231231']}\n",
      "开始加载原始数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000905 ...: 100%|██████████| 430/430 [03:40<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始拆分训练、验证、测试集合...\n",
      "train_data_size: (317172, 208)\n",
      "validation_data_size: (76518, 208)\n",
      "test_data_size: (465480, 208)\n",
      "开始计算类别权重...\n",
      "label\n",
      "2    121646\n",
      "0    103074\n",
      "1     92452\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{2: 1.3036680203212598, 0: 1.5385645264567203, 1: 1.7153333621771296}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_period_params = backtest_period[0]\n",
    "print(date_period_params)\n",
    "train_start_date, train_end_date = date_period_params['train']\n",
    "val_start_date, val_end_date = date_period_params['val']\n",
    "test_start_date, test_end_date = date_period_params['test']\n",
    "# 获取全区间数据\n",
    "print(\"开始加载原始数据...\")\n",
    "df = proprocessor._process_all_stock(code_type=benchmark, start_date=train_start_date, end_date=test_end_date)\n",
    "# 抽取训练验证数据\n",
    "print(\"开始拆分训练、验证、测试集合...\")\n",
    "train_data, val_data, test_data = extract_train_val_data(df, *[train_start_date, train_end_date, val_start_date, val_end_date, test_start_date, test_end_date])\n",
    "# 计算类别权重\n",
    "print(\"开始计算类别权重...\")\n",
    "value_count = train_data['label'].value_counts()\n",
    "print(value_count)\n",
    "total_count = train_data['label'].count()\n",
    "class_weights = ((1 / value_count) * (total_count / 2.0)).to_dict()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始抽取特征数据...\n",
      "开始特征工程处理...\n",
      "开始将DataFrame转换为DataSet...\n"
     ]
    }
   ],
   "source": [
    "# 从data中抽取相关特征数据\n",
    "print(\"开始抽取特征数据...\")\n",
    "feature_columns = feature_config.get('numeric_features', []) + feature_config.get('integer_categorical_features', []) + feature_config.get('string_categorical_features', [])\n",
    "label_columns = feature_config.get('target_features', [])\n",
    "full_feature_columns = feature_columns + label_columns\n",
    "train_df, val_df, test_df = train_data[full_feature_columns], val_data[full_feature_columns], test_data[full_feature_columns]\n",
    "# 对相关特征进行特征工程\n",
    "print(\"开始特征工程处理...\")\n",
    "preprocessing_pipeline = Pipeline([\n",
    "        ('quantile_transformer', QuantileTransformer(output_distribution='uniform', n_quantiles=1000)),\n",
    "    ])\n",
    "numeric_feature_columns = feature_config.get('numeric_features', [])\n",
    "train_df[numeric_feature_columns] = preprocessing_pipeline.fit_transform(train_df[numeric_feature_columns])\n",
    "val_df[numeric_feature_columns] = preprocessing_pipeline.transform(val_df[numeric_feature_columns])\n",
    "test_df[numeric_feature_columns] = preprocessing_pipeline.transform(test_df[numeric_feature_columns])\n",
    "# 转换为tensorflow所使用的dataset\n",
    "print(\"开始将DataFrame转换为DataSet...\")\n",
    "train_ds = df_to_dataset(train_df, feature_columns, label_columns, shuffle=True, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val_df, feature_columns, label_columns, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test_df, feature_columns, label_columns, shuffle=False, batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型初始化 & 训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/310 [00:00<?, ?it/s]2024-04-25 06:22:27.221869: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x147e4c0c0850 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-25 06:22:27.221988: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-04-25 06:22:27.316125: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-25 06:22:27.744146: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Training...:  33%|███▎      | 103/310 [00:23<00:32,  6.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:54<00:00,  5.71it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:09<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: 2.6586, accuracy: 34.9432, mse: 0.7496, val_loss: 1.1964, val_accuracy: 29.1435, val_mse: 0.0509, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:22<00:00, 13.61it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, loss: 1.8817, accuracy: 37.7978, mse: 0.1852, val_loss: 1.1956, val_accuracy: 30.3746, val_mse: 0.0562, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:21<00:00, 14.12it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:03<00:00, 18.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 1.7477, accuracy: 39.5240, mse: 0.0978, val_loss: 1.1647, val_accuracy: 30.7483, val_mse: 0.0243, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:20<00:00, 14.97it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:05<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, loss: 1.6900, accuracy: 40.5610, mse: 0.0607, val_loss: 1.1612, val_accuracy: 30.7941, val_mse: 0.0194, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:21<00:00, 14.61it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:05<00:00, 12.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 1.6570, accuracy: 41.2782, mse: 0.0414, val_loss: 1.1590, val_accuracy: 30.7457, val_mse: 0.0145, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:21<00:00, 14.35it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 17.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, loss: 1.6388, accuracy: 41.8628, mse: 0.0313, val_loss: 1.1612, val_accuracy: 30.7000, val_mse: 0.0132, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:21<00:00, 14.16it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 17.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, loss: 1.6249, accuracy: 42.4372, mse: 0.0246, val_loss: 1.1608, val_accuracy: 30.7601, val_mse: 0.0110, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:21<00:00, 14.51it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 16.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, loss: 1.6154, accuracy: 42.9363, mse: 0.0208, val_loss: 1.1697, val_accuracy: 30.5000, val_mse: 0.0104, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:22<00:00, 13.79it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:03<00:00, 19.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, loss: 1.6060, accuracy: 43.4638, mse: 0.0179, val_loss: 1.1729, val_accuracy: 30.7862, val_mse: 0.0089, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:21<00:00, 14.69it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 16.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, loss: 1.5995, accuracy: 43.8362, mse: 0.0159, val_loss: 1.1798, val_accuracy: 30.6882, val_mse: 0.0086, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:21<00:00, 14.41it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 18.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, loss: 1.5934, accuracy: 44.3933, mse: 0.0142, val_loss: 1.1821, val_accuracy: 30.9692, val_mse: 0.0081, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:22<00:00, 13.57it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:05<00:00, 14.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, loss: 1.5871, accuracy: 44.8961, mse: 0.0127, val_loss: 1.1892, val_accuracy: 31.0489, val_mse: 0.0085, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:22<00:00, 13.96it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, loss: 1.5820, accuracy: 45.3196, mse: 0.0121, val_loss: 1.1959, val_accuracy: 31.3011, val_mse: 0.0077, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:20<00:00, 14.80it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, loss: 1.5755, accuracy: 45.6951, mse: 0.0112, val_loss: 1.1965, val_accuracy: 31.6997, val_mse: 0.0078, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:25<00:00, 12.29it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:04<00:00, 18.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, loss: 1.5692, accuracy: 46.1094, mse: 0.0106, val_loss: 1.2057, val_accuracy: 31.2136, val_mse: 0.0073, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 310/310 [00:23<00:00, 13.23it/s]\n",
      "Validatioin...: 100%|██████████| 75/75 [00:03<00:00, 20.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n",
      "Restoring model weights from the end of the best epoch.\n"
     ]
    }
   ],
   "source": [
    "# 准备模型训练\n",
    "print(\"开始模型初始化 & 训练...\")\n",
    "# 自定义模型\n",
    "from models.multi_task.model_mmoe import QuantModel\n",
    "\n",
    "model_config = {\n",
    "    \"seed\": 1024,\n",
    "    \"feature_embedding_dims\": 4,\n",
    "    \"integer_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get(\"integer_categorical_features\", [])},\n",
    "    \"string_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get(\"string_categorical_features\", [])},\n",
    "}\n",
    "model = QuantModel(config=model_config)\n",
    "\n",
    "# 自定义优化器\n",
    "initial_learning_rate = 5e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate, decay_steps=(len(train_data) // batch_size) * 10, decay_rate=1, staircase=False\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "# 自定义损失函数\n",
    "cls_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # 分类损失\n",
    "reg_loss_object = tf.keras.losses.MeanSquaredError() # 回归损失\n",
    "\n",
    "# 自定义指标Metrics\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_cls_metric = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
    "train_reg_metric = tf.keras.metrics.MeanSquaredError(name=\"train_mse\")\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "val_cls_metric = tf.keras.metrics.SparseCategoricalAccuracy(name=\"val_accuracy\")\n",
    "val_reg_metric = tf.keras.metrics.MeanSquaredError(name=\"val_mse\")\n",
    "\n",
    "# 自定义训练步骤\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        cls_label, reg_label = labels\n",
    "        cls_pred, reg_pred = predictions\n",
    "        cls_sample_weights = tf.gather(\n",
    "            tf.constant([class_weights[ind] for ind in sorted(class_weights.keys())], dtype=tf.float32),\n",
    "            tf.cast(cls_label, dtype=tf.int32)\n",
    "        )\n",
    "        cls_loss = cls_loss_object(cls_label, cls_pred, sample_weight=cls_sample_weights)\n",
    "        reg_loss = reg_loss_object(reg_label, reg_pred)\n",
    "        loss = cls_loss + reg_loss\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_cls_metric(cls_label, cls_pred)\n",
    "    train_reg_metric(reg_label, reg_pred)\n",
    "\n",
    "\n",
    "# 自定义验证步骤\n",
    "@tf.function\n",
    "def val_step(inputs, labels):\n",
    "    predictions = model(inputs, training=False)\n",
    "    cls_label, reg_label = labels\n",
    "    cls_pred, reg_pred = predictions\n",
    "    cls_loss = cls_loss_object(cls_label, cls_pred)\n",
    "    reg_loss = reg_loss_object(reg_label, reg_pred)\n",
    "    loss = cls_loss + reg_loss\n",
    "    val_loss(loss)\n",
    "    val_cls_metric(cls_label, cls_pred)\n",
    "    val_reg_metric(reg_label, reg_pred)\n",
    "\n",
    "# 设定早停参数\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "best_weights=None\n",
    "restore_best_weights=True\n",
    "\n",
    "EPOCHS = 500\n",
    "for epoch in range(EPOCHS):\n",
    "    # 重新初始化Epoch内的参数\n",
    "    train_loss.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    train_cls_metric.reset_states()\n",
    "    train_reg_metric.reset_states()\n",
    "    val_cls_metric.reset_states()\n",
    "    val_reg_metric.reset_states()\n",
    "\n",
    "    # 训练逻辑\n",
    "    for train_inputs, train_labels in tqdm(train_ds, desc=\"Training...\"):\n",
    "        train_step(train_inputs, train_labels)\n",
    "\n",
    "    # 验证逻辑\n",
    "    for val_inputs, val_labels in tqdm(val_ds, desc='Validatioin...'):\n",
    "        val_step(val_inputs, val_labels)\n",
    "\n",
    "    # EarlyStoping逻辑\n",
    "    current_val_loss = val_loss.result()\n",
    "    if current_val_loss <= best_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "        patience_counter = 0\n",
    "        # 可以选择在这里保存模型\n",
    "        best_weights = model.get_weights()\n",
    "        # model.save('path_to_my_model.h5')\n",
    "    else:  # 如果不是，则耐心计数器加1\n",
    "        patience_counter += 1\n",
    "    # 如果耐心计数器超出设定的耐心值，则停止训练\n",
    "    if patience_counter > patience:\n",
    "        print(f'Early stopping at epoch {epoch + 1}')\n",
    "        if restore_best_weights and best_weights is not None:\n",
    "            # 恢复最佳权重\n",
    "            print('Restoring model weights from the end of the best epoch.')\n",
    "            model.set_weights(best_weights)\n",
    "        break\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}, \"\n",
    "        f\"loss: {train_loss.result():.4f}, \"\n",
    "        f\"accuracy: {train_cls_metric.result() * 100:.4f}, \"\n",
    "        f\"mse: {train_reg_metric.result():.4f}, \"\n",
    "        f\"val_loss: {val_loss.result():.4f}, \"\n",
    "        f\"val_accuracy: {val_cls_metric.result() * 100:.4f}, \"\n",
    "        f\"val_mse: {val_reg_metric.result():.4f}, \"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/455 [===================>..........] - ETA: 18s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455/455 [==============================] - 60s 130ms/step\n"
     ]
    }
   ],
   "source": [
    "# test数据处理\n",
    "test_cls_result, test_reg_result = model.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.47      0.43    140712\n",
      "           1       0.30      0.49      0.37    124668\n",
      "           2       0.50      0.22      0.31    200100\n",
      "\n",
      "    accuracy                           0.37    465480\n",
      "   macro avg       0.39      0.39      0.37    465480\n",
      "weighted avg       0.41      0.37      0.36    465480\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "test_true = test_data['label']\n",
    "test_pred = np.argmax(tf.nn.softmax(test_cls_result), axis=-1)\n",
    "print(classification_report(test_true, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = test_data[['stock_code', 'stock_name', 'datetime', 'open', 'high', 'low', 'close', 'volume']]\n",
    "output_df['label_cls'] = test_data['label']\n",
    "output_df['label_reg'] = test_data['return']\n",
    "# 处理分类结果\n",
    "output_df['label_cls_pred'] = test_pred\n",
    "# 处理回归结果\n",
    "output_df['label_reg_pred'] = test_reg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_pickle(f'../../Offline/backtest/backtest_data/test/{benchmark}_stock_selection_results_{test_start_date}_cls.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = output_df[output_df['stock_code'] == '688981']\n",
    "sample_df = sample_df[pd.to_datetime(sample_df['datetime']).dt.year == 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_close_label(sample_df, 'label_cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_close_label(sample_df, 'label_cls_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
