{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import akshare as ak\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from database.downloader.downloader_base import DownloaderBase\n",
    "import database.database_config as db_config\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series_dist(series):\n",
    "    data = series\n",
    "    # 使用matplotlib画直方图\n",
    "    plt.hist(data, bins=60, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Data')\n",
    "    plt.show()\n",
    "\n",
    "class PreProcessing:\n",
    "    def __init__(self, db_downloader:DownloaderBase) -> None:\n",
    "        self.db_downloader = db_downloader\n",
    "\n",
    "    def _build_reg_label(self, stock_dataframe):\n",
    "        N = 10 # 最大持仓周期 = N天，第N+1天开盘卖出\n",
    "        df = stock_dataframe.copy()\n",
    "        # 标签构建\n",
    "        df['label'] = df['close'].shift(-N) / df['open'].shift(-1) - 1 # 计算第N日收益率\n",
    "        # 极值处理 - quantile处理\n",
    "        df['label'] = np.clip(\n",
    "            df['label'], \n",
    "            np.nanquantile(df['label'], 0.01), \n",
    "            np.nanquantile(df['label'], 0.99),\n",
    "            )\n",
    "        # 过滤第二天一字涨停情况\n",
    "        df = df[df['high'].shift(-1) != df['low'].shift(-1)]\n",
    "        return df[['datetime', 'label']]\n",
    "    \n",
    "    # def _build_cls_label(self, dataframe, N=10, ATR_period=14):\n",
    "    #     def calculate_atr(df, period=14):\n",
    "    #         # True Range的计算\n",
    "    #         df['high_low'] = df['high'] - df['low']\n",
    "    #         df['high_cp'] = abs(df['high'] - df['close'].shift())\n",
    "    #         df['low_cp'] = abs(df['low'] - df['close'].shift())\n",
    "    #         tr = df[['high_low', 'high_cp', 'low_cp']].max(axis=1)\n",
    "    #         return tr.rolling(window=period, min_periods=1).mean()\n",
    "    #     df = dataframe.copy()\n",
    "    #     df['ATR'] = calculate_atr(df, ATR_period)\n",
    "    #     # 初始化标签列\n",
    "    #     df['label'] = 0\n",
    "    #     # 迭代每条记录\n",
    "    #     for i in range(len(df)):\n",
    "    #         if i + N >= len(df):\n",
    "    #             continue  # 如果没有足够的未来数据，则跳过\n",
    "    #         buy_price = df.at[i + 1, 'open']  # 明天的开盘价\n",
    "    #         # 初始化未触发止盈止损标志\n",
    "    #         triggered = False\n",
    "    #         # 检查接下来的N天内是否满足条件\n",
    "    #         for j in range(1, N + 1):\n",
    "    #             current_close = df.at[i + j, 'close']\n",
    "    #             atr_value = df.at[i + j, 'ATR']  # 第j天的ATR值\n",
    "    #             take_profit = buy_price + 2 * atr_value\n",
    "    #             stop_loss = buy_price - atr_value\n",
    "    #             if current_close >= take_profit:\n",
    "    #                 df.at[i, 'label'] = 1 # 止盈=1\n",
    "    #                 triggered = True\n",
    "    #                 break\n",
    "    #             elif current_close <= stop_loss:\n",
    "    #                 df.at[i, 'label'] = 2 # 止损=2\n",
    "    #                 triggered = True\n",
    "    #                 break\n",
    "    #         # N天后既没有触发止盈也没有触发止损\n",
    "    #         if not triggered:\n",
    "    #             df.at[i, 'label'] = 0\n",
    "    #     # 删除ATR列，因为它是一个中间计算列\n",
    "    #     df = df[df['high'].shift(-1) != df['low'].shift(-1)]\n",
    "    #     return df[['datetime', 'label', 'ATR']]\n",
    "\n",
    "    def _process_one_stock(self, stock_code, start_date, end_date):\n",
    "        stock_base = self.db_downloader._download_stock_base_info(stock_code) # 获取基础代码\n",
    "        stock_individual = self.db_downloader._download_stock_individual_info(stock_code) # 获取profile信息\n",
    "        stock_history = self.db_downloader._download_stock_history_info(stock_code, start_date, end_date) # 获取历史行情\n",
    "        stock_indicator = self.db_downloader._download_stock_indicator_info(stock_code, start_date, end_date) # 获取指标数据\n",
    "        stock_factor_date = self.db_downloader._download_stock_factor_date_info() # 获取日期特征\n",
    "        stock_factor_qlib = self.db_downloader._download_stock_factor_qlib_info(stock_code, start_date, end_date) # 获取量价特征\n",
    "        stock_label = self._build_reg_label(stock_history) # 构建回归Label\n",
    "        stock_df = stock_base.merge(stock_individual, on=['stock_code']).merge(stock_history, on=['stock_code']).merge(stock_indicator, on=['stock_code', 'datetime']).merge(stock_label, on=['datetime']).merge(stock_factor_date, on=['datetime']).merge(stock_factor_qlib, on=['stock_code', 'datetime']) # 整合数据\n",
    "        stock_df = stock_base \\\n",
    "            .merge(stock_individual, on=['stock_code', 'stock_name']) \\\n",
    "            .merge(stock_history, on=['stock_code']) \\\n",
    "            .merge(stock_indicator, on=['stock_code', 'datetime']) \\\n",
    "            .merge(stock_label, on=['datetime']) \\\n",
    "            .merge(stock_factor_date, on=['datetime']) \\\n",
    "            .merge(stock_factor_qlib, on=['stock_code', 'datetime']) # 整合数据\n",
    "        stock_df = stock_df.dropna()\n",
    "        return stock_df\n",
    "    \n",
    "    def _process_all_stock(self, code_type, start_date, end_date):\n",
    "        # stock_code_list = list(ak.stock_info_a_code_name()['code'].unique()) # 获取A股所有股票列表\n",
    "        stock_code_list = list(ak.index_stock_cons(code_type)['品种代码'].unique()) # 获取沪深300的股票代码列表\n",
    "        stock_df_list = []\n",
    "        for stock_code in tqdm(stock_code_list, desc=f'Process: {code_type} ...'):\n",
    "            stock_df = self._process_one_stock(stock_code, start_date, end_date)\n",
    "            if not stock_df.empty:\n",
    "                stock_df_list.append(stock_df)\n",
    "        return pd.concat(stock_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conn = sqlite3.connect('../database/hh_quant.db')\n",
    "db_downloader = DownloaderBase(db_conn, db_config)\n",
    "proprocessor = PreProcessing(db_downloader=db_downloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 20:09:10.473040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow处理原始数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from model import QuantModel\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_val_data(df, train_start_date, train_end_date, val_start_date, val_end_date):\n",
    "    train_start_date = pd.to_datetime(train_start_date)\n",
    "    train_end_date = pd.to_datetime(train_end_date)\n",
    "    val_start_date = pd.to_datetime(val_start_date)\n",
    "    val_end_date = pd.to_datetime(val_end_date)\n",
    "    train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "    val_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "\n",
    "    print(f\"train_data_size: {train_data.shape}\")\n",
    "    print(f\"validation_data_size: {val_data.shape}\")\n",
    "    return train_data, val_data\n",
    "\n",
    "def transfer_data_type(df, columns, dtype):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "def get_numeric_boundaries(series, num_bins=30):\n",
    "    if series.nunique() < num_bins:\n",
    "        boundaries = sorted(series.unique())\n",
    "    else:\n",
    "        boundaries = sorted(pd.qcut(series, num_bins, retbins=True, duplicates='drop')[1].tolist())\n",
    "    return boundaries\n",
    "\n",
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=True, batch_size=32):\n",
    "    features = dataframe[feature_cols]\n",
    "    labels = dataframe[label_cols]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(features))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds\n",
    "\n",
    "def get_class_weights(label_series, udf_class_weight={}):\n",
    "    class_weight = {}\n",
    "    cnt_list = np.bincount(label_series).tolist()\n",
    "    total_cnt = np.sum(cnt_list)\n",
    "    for label, cnt in enumerate(cnt_list):\n",
    "        class_weight[label] = (1 / cnt) * (total_cnt / 2.0) * udf_class_weight.get(label, 1)\n",
    "    return class_weight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20130101', '20181231', '20190101', '20190630'],\n",
       " ['20130701', '20190630', '20190701', '20191231'],\n",
       " ['20140101', '20191231', '20200101', '20200630'],\n",
       " ['20140701', '20200630', '20200701', '20201231'],\n",
       " ['20150101', '20201231', '20210101', '20210630'],\n",
       " ['20150701', '20210630', '20210701', '20211231'],\n",
       " ['20160101', '20211231', '20220101', '20220630'],\n",
       " ['20160701', '20220630', '20220701', '20221231'],\n",
       " ['20170101', '20221231', '20230101', '20230630'],\n",
       " ['20170701', '20230630', '20230701', '20231231']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_start_date = '20190101'\n",
    "backtest_end_date = '20240101'\n",
    "train_period = 6 # year：训练数据周期长度\n",
    "update_period = 6 # month：模型更新周期长度\n",
    "\n",
    "def get_rolling_date_period(backtest_start_date, backtest_end_date, training_period, update_period):\n",
    "    backtest_start_date = datetime.strptime(backtest_start_date, '%Y%m%d')\n",
    "    backtest_end_date = datetime.strptime(backtest_end_date, '%Y%m%d')\n",
    "    result = []\n",
    "    rolling_flag = True\n",
    "    while rolling_flag:\n",
    "        current_val_start_date = backtest_start_date\n",
    "        current_val_end_date = current_val_start_date + relativedelta(months=update_period) - relativedelta(days=1)\n",
    "        if current_val_start_date < backtest_end_date:\n",
    "            current_train_start_date = current_val_start_date - relativedelta(years=training_period)\n",
    "            current_train_end_date = current_val_start_date - relativedelta(days=1)\n",
    "            result.append([\n",
    "                current_train_start_date.strftime(\"%Y%m%d\"),\n",
    "                current_train_end_date.strftime(\"%Y%m%d\"),\n",
    "                current_val_start_date.strftime(\"%Y%m%d\"),\n",
    "                current_val_end_date.strftime(\"%Y%m%d\")\n",
    "                ])\n",
    "            backtest_start_date += relativedelta(months=update_period) \n",
    "        else:\n",
    "            rolling_flag=False # 结束滚动训练\n",
    "    return result\n",
    "\n",
    "rolling_period = get_rolling_date_period(backtest_start_date, backtest_end_date, train_period, update_period)\n",
    "rolling_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = proprocessor._process_all_stock(code_type='000016', start_date='20130101', end_date='20191231')\n",
    "# dd = proprocessor._process_one_stock('601398', start_date='20130101', end_date='20191231')\n",
    "# dd = proprocessor.db_downloader._download_stock_history_info('601398', start_date='20130101', end_date='20191231')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
    "\n",
    "# 创建一个预处理管道\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('quantile_transformer', QuantileTransformer(output_distribution='normal', n_quantiles=1000)),\n",
    "    # ('minmax_scaler', MinMaxScaler()),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20130101, train_end: 20181231, val_start: 20190101, val_end: 20191231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:07<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (47508, 207)\n",
      "validation_data_size: (8747, 207)\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:   0%|          | 0/1 [02:36<?, ?it/s]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node gradient_tape/quant_model/add/BroadcastGradientArgs defined at (most recent call last):\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/ipykernel_72044/1115789825.py\", line 73, in <module>\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py\", line 598, in minimize\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py\", line 656, in _compute_gradients\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py\", line 532, in _get_gradients\n\nIncompatible shapes: [256,187,1] vs. [256,1]\n\t [[{{node gradient_tape/quant_model/add/BroadcastGradientArgs}}]] [Op:__inference_train_function_24088]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 73\u001b[0m\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     62\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mlegacy\u001b[38;5;241m.\u001b[39mAdam(lr_schedule),\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # 分类任务\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     ]\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 配置模型fit\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# class_weight=class_weights,\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorBoard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs/fit/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistogram_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# 9. 配置保存模型功能\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# model_save_path = f'./models/saved_model/model_of_{val_start_date}'\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# model.save(model_save_path)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# # output_df.to_pickle(f'../../Offline/backtest/backtest_data/{benchmark}/stock_selection_results_{val_start_date}.pkl')\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# output_df.to_pickle(f'../../Offline/backtest/backtest_data/test/stock_selection_results_test.pkl')\u001b[39;00m\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/quant_model/add/BroadcastGradientArgs defined at (most recent call last):\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/Users/alsc/.pyenv/versions/3.10.13/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3106, in _run_cell\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/ipykernel_72044/1115789825.py\", line 73, in <module>\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1154, in train_step\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py\", line 598, in minimize\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py\", line 656, in _compute_gradients\n\n  File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py\", line 532, in _get_gradients\n\nIncompatible shapes: [256,187,1] vs. [256,1]\n\t [[{{node gradient_tape/quant_model/add/BroadcastGradientArgs}}]] [Op:__inference_train_function_24088]"
     ]
    }
   ],
   "source": [
    "feature_config = {\n",
    "    \"target_feature_name\": \"label\",\n",
    "    \"numeric_features\": ['turnover_rate', 'pe_ttm', 'ps_ttm', 'pcf_ncf_ttm', 'pb_mrq', 'KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE0', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME0', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60'],\n",
    "    \"integer_categorical_features\": ['month'],\n",
    "    \"string_categorical_features\": ['industry', 'season'],\n",
    "}\n",
    "full_feature_names = feature_config.get('numeric_features', []) + feature_config.get('integer_categorical_features', []) + feature_config.get('string_categorical_features', [])\n",
    "benchmark = '000016' # [上证50:000016, 沪深300:000300, 中证500:000905]所有股票作为训练数据\n",
    "batch_size = 256\n",
    "\n",
    "sample_period = [['20130101', '20181231', '20190101', '20191231']]\n",
    "for date_period in tqdm(sample_period, desc='Rolling Training...'):\n",
    "# for date_period in tqdm(rolling_period, desc='Rolling Training...'):\n",
    "    train_start_date, train_end_date, val_start_date, val_end_date = date_period\n",
    "    print(f\"train_start: {train_start_date}, train_end: {train_end_date}, val_start: {val_start_date}, val_end: {val_end_date}\")\n",
    "    # 1. 获取所有股票信息\n",
    "    df = proprocessor._process_all_stock(code_type=benchmark, start_date=train_start_date, end_date=val_end_date)\n",
    "    # 2. 拆分训练数据&验证数据\n",
    "    train_data, val_data = extract_train_val_data(df, train_start_date, train_end_date, val_start_date, val_end_date)\n",
    "    # 2.1 计算类别权重（分类任务使用）\n",
    "    # class_weights = get_class_weights(train_data['label'])\n",
    "    # print(f\"class_weights: {class_weights}\")\n",
    "    # 2.2 特征工程\n",
    "    norm_feature_columns = feature_config.get('numeric_features', [])\n",
    "    train_data[norm_feature_columns] = preprocessing_pipeline.fit_transform(train_data[norm_feature_columns])\n",
    "    val_data[norm_feature_columns] = preprocessing_pipeline.transform(val_data[norm_feature_columns])\n",
    "    # # 2.3 目标工程\n",
    "    # target_fp = FeaturePreprocessor()\n",
    "    # norm_target_columns = feature_config.get('target_feature_name', [])\n",
    "    # train_data[norm_target_columns] = feature_fp.fit_transform(train_data[norm_target_columns])\n",
    "    # val_data[norm_target_columns] = feature_fp.transform(val_data[norm_target_columns])\n",
    "    \n",
    "    # 3. 构建训练集和验证集\n",
    "    train_ds = df_to_dataset(train_data, full_feature_names, feature_config.get('target_feature_name', []), shuffle=True, batch_size=batch_size)\n",
    "    val_ds = df_to_dataset(val_data, full_feature_names, feature_config.get('target_feature_name', []), shuffle=False, batch_size=batch_size)\n",
    "    # 4. 配置模型相关参数\n",
    "    model_config = {\n",
    "        \"seed\": 1024,\n",
    "        # \"l2_reg\": 0.001,\n",
    "        \"reduction_ratio\": 3,\n",
    "        \"dnn_hidden_units\": [256,128,64],\n",
    "        \"dnn_activation\": 'relu',\n",
    "        \"dnn_dropout\": 0.2,\n",
    "        \"dnn_use_bn\": True,\n",
    "        \"numeric_features_with_boundaries\": {k: list(get_numeric_boundaries(train_data[k])) for k in feature_config.get('numeric_features', [])},\n",
    "        \"integer_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('integer_categorical_features', [])},\n",
    "        \"string_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('string_categorical_features', [])},\n",
    "        \"feature_embedding_dims\": 4,\n",
    "    }\n",
    "    # 5. 初始化模型\n",
    "    model = QuantModel(model_config)\n",
    "\n",
    "    # 8. 配置optimizer\n",
    "    initial_learning_rate = 5e-4\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=(len(train_data) // batch_size)*5,\n",
    "        decay_rate=1,\n",
    "        staircase=False)\n",
    "    # 配置模型compile\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(lr_schedule),\n",
    "        # loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # 分类任务\n",
    "        loss = tf.keras.losses.MeanAbsoluteError(),\n",
    "        # loss = tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[\n",
    "            # tf.keras.metrics.SparseCategoricalAccuracy() # 分类任务\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "            # tf.keras.metrics.MeanSquaredError(),\n",
    "        ]\n",
    "    )\n",
    "    # 配置模型fit\n",
    "    model.fit(\n",
    "            train_ds, \n",
    "            validation_data=val_ds, \n",
    "            epochs=50,\n",
    "            verbose=2,\n",
    "            # class_weight=class_weights,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min'),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir=\"./logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"), histogram_freq=1)\n",
    "            ],\n",
    "    )\n",
    "    # 9. 配置保存模型功能\n",
    "    # model_save_path = f'./models/saved_model/model_of_{val_start_date}'\n",
    "    # model.save(model_save_path)\n",
    "    # best_model = tf.keras.models.load_model('./best_model')\n",
    "\n",
    "    # 10. 记录预测集合\n",
    "    # model_pred_result = model.predict(val_ds)\n",
    "    # model_pred_result = tf.nn.softmax(model_pred_result)\n",
    "    # model_pred_label = np.argmax(model_pred_result, axis=1)\n",
    "    # output_df = val_data[['stock_code', 'industry', 'stock_name', 'datetime']]\n",
    "    # # 分类结果\n",
    "    # # output_df['true_label'] = val_data['label']\n",
    "    # # output_df['pred_label'] = model_pred_label\n",
    "    # # output_df[['pred_label_0_prob','pred_label_1_prob','pred_label_2_prob']] = model_pred_result\n",
    "    # # 回归结果\n",
    "    # output_df['future_return'] = val_data['future_return']\n",
    "    # output_df['future_return'] = \n",
    "    # # output_df.to_pickle(f'../../Offline/backtest/backtest_data/{benchmark}/stock_selection_results_{val_start_date}.pkl')\n",
    "    # output_df.to_pickle(f'../../Offline/backtest/backtest_data/test/stock_selection_results_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "# model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 7s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "model_pred_result = model.predict(val_ds)\n",
    "output_df = val_data[['stock_code', 'industry', 'stock_name', 'datetime']]\n",
    "# 回归结果\n",
    "output_df['label'] = val_data['label']\n",
    "output_df['label_pred'] = model_pred_result\n",
    "output_df.to_pickle(f'../../Offline/backtest/backtest_data/test/stock_selection_results_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.columns[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_series_dist(train_data['MIN60'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
