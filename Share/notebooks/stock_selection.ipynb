{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import akshare as ak\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from database.downloader.downloader_base import DownloaderBase\n",
    "import database.database_config as db_config\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-01 07:28:51.141015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-01 07:28:52.008558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.13.1\n",
      "TensorFlow GPU version is installed\n",
      "GPU devices available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 只使用CPU进行训练\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# 打印Tensorflow版本\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "\n",
    "# 检查是否有可用的GPU设备\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"TensorFlow GPU version is installed\")\n",
    "else:\n",
    "    print(\"TensorFlow CPU version is installed\")\n",
    "\n",
    "# 检查TensorFlow是否能够访问GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU devices available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU devices found. Running on CPU.\")\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import QuantileTransformer, KBinsDiscretizer, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# 绘图相关函数\n",
    "def plot_series_dist(series):\n",
    "    data = series\n",
    "    # 使用matplotlib画直方图\n",
    "    plt.hist(data, bins=60, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Data')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'mean_absolute_error', 'mean_squared_error']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.ylim([0, plt.ylim()[1]])\n",
    "    plt.legend()\n",
    "\n",
    "def plot_cm(true_labels, pred_labels):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"g\", cmap='Blues')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    recall = recall_score(true_labels, pred_labels)\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "    def __init__(self, db_downloader:DownloaderBase) -> None:\n",
    "        self.db_downloader = db_downloader\n",
    "\n",
    "    def _build_reg_label(self, stock_dataframe):\n",
    "        N = 5 # 最大持仓周期 = N天，第N+1天开盘卖出\n",
    "        df = stock_dataframe.copy()\n",
    "        # 标签构建\n",
    "        df['label'] = df['open'].shift(-N-1) / df['open'].shift(-1) - 1 # 计算第N日收益率\n",
    "        # 极值处理 - quantile处理\n",
    "        df['label'] = np.clip(\n",
    "            df['label'], \n",
    "            np.nanquantile(df['label'], 0.01), \n",
    "            np.nanquantile(df['label'], 0.99),\n",
    "            )\n",
    "        # 过滤第二天一字涨停情况\n",
    "        df = df[df['high'].shift(-1) != df['low'].shift(-1)]\n",
    "        return df[['datetime', 'label']]\n",
    "    \n",
    "    # def _build_reg_label(self, stock_dataframe):\n",
    "    #     N = 10  # 最大持仓周期 = N天\n",
    "    #     df = stock_dataframe.copy()\n",
    "    #     # 计算未来N天内的最高收盘价\n",
    "    #     df['max_open_future'] = df['open'].shift(-N).rolling(window=N).max()\n",
    "    #     # 计算标签：即未来N天内最大收益率\n",
    "    #     df['label'] = df['max_open_future'] / df['open'].shift(-1) - 1\n",
    "    #     # 可选的极值处理 - quantile处理\n",
    "    #     df['label'] = np.clip(\n",
    "    #         df['label'],\n",
    "    #         np.nanquantile(df['label'], 0.01),\n",
    "    #         np.nanquantile(df['label'], 0.99),\n",
    "    #     )\n",
    "    #     # 过滤第二天一字涨停情况\n",
    "    #     df = df[df['high'].shift(-1) != df['low'].shift(-1)]\n",
    "    #     return df[['datetime', 'label']]\n",
    "\n",
    "    def _process_one_stock(self, stock_code, start_date, end_date):\n",
    "        stock_base = self.db_downloader._download_stock_base_info(stock_code) # 获取基础代码\n",
    "        stock_individual = self.db_downloader._download_stock_individual_info(stock_code) # 获取profile信息\n",
    "        stock_history = self.db_downloader._download_stock_history_info(stock_code, start_date, end_date) # 获取历史行情\n",
    "        stock_indicator = self.db_downloader._download_stock_indicator_info(stock_code, start_date, end_date) # 获取指标数据\n",
    "        stock_factor_date = self.db_downloader._download_stock_factor_date_info() # 获取日期特征\n",
    "        stock_factor_qlib = self.db_downloader._download_stock_factor_qlib_info(stock_code, start_date, end_date) # 获取量价特征\n",
    "        stock_label = self._build_reg_label(stock_history) # 构建Label\n",
    "        stock_df = stock_base.merge(stock_individual, on=['stock_code']).merge(stock_history, on=['stock_code']).merge(stock_indicator, on=['stock_code', 'datetime']).merge(stock_label, on=['datetime']).merge(stock_factor_date, on=['datetime']).merge(stock_factor_qlib, on=['stock_code', 'datetime']) # 整合数据\n",
    "        stock_df = stock_base \\\n",
    "            .merge(stock_individual, on=['stock_code', 'stock_name']) \\\n",
    "            .merge(stock_history, on=['stock_code']) \\\n",
    "            .merge(stock_indicator, on=['stock_code', 'datetime']) \\\n",
    "            .merge(stock_label, on=['datetime']) \\\n",
    "            .merge(stock_factor_date, on=['datetime']) \\\n",
    "            .merge(stock_factor_qlib, on=['stock_code', 'datetime']) # 整合数据\n",
    "        stock_df = stock_df.dropna()\n",
    "        return stock_df\n",
    "    \n",
    "    def _process_all_stock(self, code_type, start_date, end_date):\n",
    "        # stock_code_list = list(ak.stock_info_a_code_name()['code'].unique()) # 获取A股所有股票列表\n",
    "        stock_code_list = list(ak.index_stock_cons(code_type)['品种代码'].unique()) # 获取沪深300的股票代码列表\n",
    "        stock_df_list = []\n",
    "        for stock_code in tqdm(stock_code_list, desc=f'Process: {code_type} ...'):\n",
    "            stock_df = self._process_one_stock(stock_code, start_date, end_date)\n",
    "            if not stock_df.empty:\n",
    "                stock_df_list.append(stock_df)\n",
    "        return pd.concat(stock_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'train': ['20120629', '20180629'],\n",
       "  'val': ['20180630', '20181231'],\n",
       "  'test': ['20190101', '20190630']},\n",
       " {'train': ['20121229', '20181229'],\n",
       "  'val': ['20181230', '20190630'],\n",
       "  'test': ['20190701', '20191231']}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rolling_data_period(backtest_start_date, backtest_duration=5, train_period=6, val_period=0.5, test_period=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        backtest_start_date (_type_): _description_\n",
    "        backtest_duration (int, optional): _description_. Defaults to 5.\n",
    "        train_period (int, optional): _description_. Defaults to 6.\n",
    "        val_period (float, optional): _description_. Defaults to 0.5.\n",
    "        test_period (float, optional): _description_. Defaults to 0.5.\n",
    "    Returns:\n",
    "        result: _description_\n",
    "    \"\"\"\n",
    "    backtest_start_date = datetime.strptime('20190101', '%Y%m%d')\n",
    "    backtest_end_date = backtest_start_date + relativedelta(years=backtest_duration) # 回测5年数据\n",
    "    train_period = relativedelta(years=train_period) # 使用6年的训练数据\n",
    "    val_period = relativedelta(months=(12 * val_period)) # 使用半年的验证数据\n",
    "    test_period = relativedelta(months=(12 * test_period)) # 使用半年的测试数据(半年模型一更新)\n",
    "\n",
    "    result = []\n",
    "    rolling_flag = True\n",
    "    bench_date = backtest_start_date\n",
    "    while rolling_flag:\n",
    "        if bench_date < backtest_end_date:\n",
    "            test_start, test_end = bench_date, (bench_date + test_period - relativedelta(days=1))\n",
    "            val_start, val_end = (test_start - relativedelta(days=1) - val_period), (test_start - relativedelta(days=1))\n",
    "            train_start, train_end =(val_start - relativedelta(days=1) - train_period), (val_start - relativedelta(days=1))\n",
    "            result.append({\n",
    "                \"train\": [train_start.strftime(\"%Y%m%d\"), train_end.strftime(\"%Y%m%d\")],\n",
    "                \"val\": [val_start.strftime(\"%Y%m%d\"), val_end.strftime(\"%Y%m%d\")],\n",
    "                \"test\": [test_start.strftime(\"%Y%m%d\"), test_end.strftime(\"%Y%m%d\")]\n",
    "            })\n",
    "            bench_date += test_period\n",
    "        else:\n",
    "            rolling_flag = False \n",
    "    return result\n",
    "\n",
    "rolling_backtest_period = get_rolling_data_period('20200101', backtest_duration=5, train_period=6, val_period=0.5, test_period=0.5)\n",
    "rolling_backtest_period[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_backtest_period = [\n",
    "    {\n",
    "        'train': ['20130101', '20181231'],\n",
    "        'val': ['20190101', '20191231'],\n",
    "        'test': ['20200101', '20231231']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_val_data(df, train_start_date, train_end_date, val_start_date, val_end_date, test_start_date, test_end_date):\n",
    "    train_start_date = pd.to_datetime(train_start_date)\n",
    "    train_end_date = pd.to_datetime(train_end_date)\n",
    "    val_start_date = pd.to_datetime(val_start_date)\n",
    "    val_end_date = pd.to_datetime(val_end_date)\n",
    "    test_start_date = pd.to_datetime(test_start_date)\n",
    "    test_end_date = pd.to_datetime(test_end_date)\n",
    "\n",
    "    train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "    val_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "    test_data = df[(pd.to_datetime(df['datetime']) >= test_start_date) & (pd.to_datetime(df['datetime']) <= test_end_date)]\n",
    "\n",
    "    print(f\"train_data_size: {train_data.shape}\")\n",
    "    print(f\"validation_data_size: {val_data.shape}\")\n",
    "    print(f\"test_data_size: {test_data.shape}\")\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=True, batch_size=32):\n",
    "    features = dataframe[feature_cols]\n",
    "    labels = dataframe[label_cols]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(features), 10000))\n",
    "    ds = ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training_Period...:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['20120629', '20180629'], 'val': ['20180630', '20181231'], 'test': ['20190101', '20190630']}\n",
      "开始加载原始数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000905 ...: 100%|██████████| 430/430 [00:44<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始拆分训练、验证、测试集合...\n",
      "train_data_size: (341307, 207)\n",
      "validation_data_size: (40652, 207)\n",
      "test_data_size: (38692, 207)\n",
      "开始抽取特征数据...\n",
      "开始特征工程处理...\n",
      "开始将DataFrame转换为DataSet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training_Period...:   0%|          | 0/10 [04:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 转换为tensorflow所使用的dataset\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m开始将DataFrame转换为DataSet...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mdf_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m df_to_dataset(val_df, feature_columns, label_columns, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m     54\u001b[0m test_ds \u001b[38;5;241m=\u001b[39m df_to_dataset(test_df, feature_columns, label_columns, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)  \n",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m, in \u001b[0;36mdf_to_dataset\u001b[0;34m(dataframe, feature_cols, label_cols, shuffle, batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m features \u001b[38;5;241m=\u001b[39m dataframe[feature_cols]\n\u001b[1;32m     20\u001b[0m labels \u001b[38;5;241m=\u001b[39m dataframe[label_cols]\n\u001b[0;32m---> 21\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m     23\u001b[0m     ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mshuffle(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(features), \u001b[38;5;241m10000\u001b[39m))\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:831\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py:104\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[43mtype_spec_from_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m   \u001b[38;5;66;03m# TypeError indicates it was not possible to compute a `TypeSpec` for\u001b[39;00m\n\u001b[1;32m    107\u001b[0m   \u001b[38;5;66;03m# the value. As a fallback try converting the value to a tensor.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m   normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    109\u001b[0m       ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m i))\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py:507\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[0;34m(element, use_fallback)\u001b[0m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    504\u001b[0m     logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(element)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[0;32m--> 507\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCould not build a `TypeSpec` for \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m with type \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/core/series.py:1632\u001b[0m, in \u001b[0;36mSeries.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-repr-returned\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m repr_params \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mget_series_repr_params()\n\u001b[0;32m-> 1632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrepr_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/core/series.py:1725\u001b[0m, in \u001b[0;36mSeries.to_string\u001b[0;34m(self, buf, na_rep, float_format, header, index, length, dtype, name, max_rows, min_rows)\u001b[0m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;124;03mRender a string representation of the Series.\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;124;03m    String representation of Series if ``buf=None``, otherwise None.\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m formatter \u001b[38;5;241m=\u001b[39m fmt\u001b[38;5;241m.\u001b[39mSeriesFormatter(\n\u001b[1;32m   1714\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1715\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1723\u001b[0m     max_rows\u001b[38;5;241m=\u001b[39mmax_rows,\n\u001b[1;32m   1724\u001b[0m )\n\u001b[0;32m-> 1725\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;66;03m# catch contract violations\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/io/formats/format.py:393\u001b[0m, in \u001b[0;36mSeriesFormatter.to_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseries)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m([], \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfooter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m fmt_index, have_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_formatted_index()\n\u001b[0;32m--> 393\u001b[0m fmt_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_formatted_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_truncated_vertically:\n\u001b[1;32m    396\u001b[0m     n_header_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/io/formats/format.py:377\u001b[0m, in \u001b[0;36mSeriesFormatter._get_formatted_values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_formatted_values\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformat_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfloat_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleading_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/io/formats/format.py:1330\u001b[0m, in \u001b[0;36mformat_array\u001b[0;34m(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting, fallback_formatter)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     digits \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.precision\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1316\u001b[0m fmt_obj \u001b[38;5;241m=\u001b[39m fmt_klass(\n\u001b[1;32m   1317\u001b[0m     values,\n\u001b[1;32m   1318\u001b[0m     digits\u001b[38;5;241m=\u001b[39mdigits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     fallback_formatter\u001b[38;5;241m=\u001b[39mfallback_formatter,\n\u001b[1;32m   1328\u001b[0m )\n\u001b[0;32m-> 1330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmt_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/io/formats/format.py:1364\u001b[0m, in \u001b[0;36mGenericArrayFormatter.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_result\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1363\u001b[0m     fmt_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_strings()\n\u001b[0;32m-> 1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_make_fixed_width\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmt_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjustify\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/io/formats/format.py:1944\u001b[0m, in \u001b[0;36m_make_fixed_width\u001b[0;34m(strings, justify, minimum, adj)\u001b[0m\n\u001b[1;32m   1941\u001b[0m             x \u001b[38;5;241m=\u001b[39m x[: max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1944\u001b[0m strings \u001b[38;5;241m=\u001b[39m [just(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m strings]\n\u001b[1;32m   1945\u001b[0m result \u001b[38;5;241m=\u001b[39m adjustment\u001b[38;5;241m.\u001b[39mjustify(strings, max_len, mode\u001b[38;5;241m=\u001b[39mjustify)\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/io/formats/format.py:1944\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1941\u001b[0m             x \u001b[38;5;241m=\u001b[39m x[: max_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m-> 1944\u001b[0m strings \u001b[38;5;241m=\u001b[39m [\u001b[43mjust\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m strings]\n\u001b[1;32m   1945\u001b[0m result \u001b[38;5;241m=\u001b[39m adjustment\u001b[38;5;241m.\u001b[39mjustify(strings, max_len, mode\u001b[38;5;241m=\u001b[39mjustify)\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/hy-tmp/miniconda3/envs/quant/lib/python3.8/site-packages/pandas/io/formats/format.py:1938\u001b[0m, in \u001b[0;36m_make_fixed_width.<locals>.just\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conf_max \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_len \u001b[38;5;241m>\u001b[39m conf_max:\n\u001b[1;32m   1936\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m conf_max\n\u001b[0;32m-> 1938\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjust\u001b[39m(x: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m conf_max \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1940\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (conf_max \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m&\u001b[39m (adjustment\u001b[38;5;241m.\u001b[39mlen(x) \u001b[38;5;241m>\u001b[39m max_len):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 数据库初始化\n",
    "db_conn = sqlite3.connect('../database/hh_quant.db')\n",
    "db_downloader = DownloaderBase(db_conn, db_config)\n",
    "proprocessor = PreProcessing(db_downloader=db_downloader)\n",
    "\n",
    "# 相关配置\n",
    "rolling_flag = True\n",
    "benchmark = '000905'\n",
    "feature_config = {\n",
    "    \"target_features\": [\"label\"],\n",
    "    \"numeric_features\": ['turnover_rate', 'pe_ttm', 'ps_ttm', 'pcf_ncf_ttm', 'pb_mrq', 'KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60'],\n",
    "    \"integer_categorical_features\": ['month'],\n",
    "    \"string_categorical_features\": ['industry', 'season'],\n",
    "}\n",
    "batch_size = 512\n",
    "\n",
    "# 是否开启滚动训练&回测\n",
    "if rolling_flag:\n",
    "    backtest_period = rolling_backtest_period\n",
    "else:\n",
    "    backtest_period = sample_backtest_period\n",
    "\n",
    "# 训练执行\n",
    "for date_period_params in tqdm(backtest_period, desc='Training_Period...'):\n",
    "    print(date_period_params)\n",
    "    train_start_date, train_end_date = date_period_params['train']\n",
    "    val_start_date, val_end_date = date_period_params['val']\n",
    "    test_start_date, test_end_date = date_period_params['test']\n",
    "    # 获取全区间数据\n",
    "    print(\"开始加载原始数据...\")\n",
    "    df = proprocessor._process_all_stock(code_type=benchmark, start_date=train_start_date, end_date=test_end_date)\n",
    "    # 抽取训练验证数据\n",
    "    print(\"开始拆分训练、验证、测试集合...\")\n",
    "    train_data, val_data, test_data = extract_train_val_data(df, *[train_start_date, train_end_date, val_start_date, val_end_date, test_start_date, test_end_date])\n",
    "    # 从data中抽取相关特征数据\n",
    "    print(\"开始抽取特征数据...\")\n",
    "    feature_columns = feature_config.get('numeric_features', []) + feature_config.get('integer_categorical_features', []) + feature_config.get('string_categorical_features', [])\n",
    "    label_columns = feature_config.get('target_features', [])\n",
    "    full_feature_columns = feature_columns + label_columns\n",
    "    train_df, val_df, test_df = train_data[full_feature_columns], val_data[full_feature_columns], test_data[full_feature_columns]\n",
    "    # 对相关特征进行特征工程\n",
    "    print(\"开始特征工程处理...\")\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "            ('quantile_transformer', QuantileTransformer(output_distribution='uniform', n_quantiles=1000)),\n",
    "        ])\n",
    "    numeric_feature_columns = feature_config.get('numeric_features', [])\n",
    "    train_df[numeric_feature_columns] = preprocessing_pipeline.fit_transform(train_df[numeric_feature_columns])\n",
    "    val_df[numeric_feature_columns] = preprocessing_pipeline.transform(val_df[numeric_feature_columns])\n",
    "    test_df[numeric_feature_columns] = preprocessing_pipeline.transform(test_df[numeric_feature_columns])\n",
    "    # 转换为tensorflow所使用的dataset\n",
    "    print(\"开始将DataFrame转换为DataSet...\")\n",
    "    train_ds = df_to_dataset(train_df, feature_columns, label_columns, shuffle=True, batch_size=batch_size)\n",
    "    val_ds = df_to_dataset(val_df, feature_columns, label_columns, shuffle=False, batch_size=batch_size)\n",
    "    test_ds = df_to_dataset(test_df, feature_columns, label_columns, shuffle=False, batch_size=batch_size)  \n",
    "    # 准备模型训练\n",
    "    print(\"开始模型初始化 & 训练...\")\n",
    "    # from model.model_base import QuantModel\n",
    "    # from model.model_senet import QuantModel\n",
    "    from model.model_moe import QuantModel\n",
    "    model_config = {\n",
    "            \"seed\": 1024,\n",
    "            \"feature_embedding_dims\": 4,\n",
    "            \"integer_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('integer_categorical_features', [])},\n",
    "            \"string_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('string_categorical_features', [])},\n",
    "        }\n",
    "    model = QuantModel(config=model_config)\n",
    "\n",
    "    initial_learning_rate = 1e-3\n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=(len(train_data) // batch_size)*10,\n",
    "        decay_rate=1,\n",
    "        staircase=False)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "        loss = tf.keras.losses.MeanAbsoluteError(),\n",
    "        metrics=[\n",
    "            tf.keras.metrics.MeanAbsoluteError(),\n",
    "            tf.keras.metrics.MeanSquaredError(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    EPOCHS = 500\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', \n",
    "        verbose=1,\n",
    "        patience=10,\n",
    "        mode='min',\n",
    "        restore_best_weights=True)\n",
    "\n",
    "    baseline_history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks = [early_stopping],\n",
    "    )\n",
    "\n",
    "    plot_metrics(baseline_history)\n",
    "    \n",
    "\n",
    "    # 保存模型\n",
    "    print(\"开始保存模型...\")\n",
    "    model_save_path = f'./models/saved_model/{benchmark}/model_of_reg_{test_start_date}'\n",
    "    model.save(model_save_path)\n",
    "    model = tf.keras.models.load_model(model_save_path)\n",
    "\n",
    "    # 输出回测预测\n",
    "    print(\"开始保存回测预测结果...\")\n",
    "    model_pred_result = model.predict(test_ds)\n",
    "    output_df = test_data[['stock_code', 'industry', 'stock_name', 'datetime']]\n",
    "    output_df['label'] = test_data['label']\n",
    "    output_df['label_pred'] = model_pred_result\n",
    "    output_df.to_pickle(f'../../Offline/backtest/backtest_data/{benchmark}/stock_selection_results_{test_start_date}.pkl')    \n",
    "\n",
    "    # 绘制结果\n",
    "    test_true_label = test_data['label']\n",
    "    test_pred_label = model_pred_result\n",
    "    plot_cm(\n",
    "        true_labels = (np.array(test_true_label) > 0).astype(int), \n",
    "        pred_labels = (np.array(test_pred_label) > 0).astype(int)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
