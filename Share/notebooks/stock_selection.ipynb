{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import akshare as ak\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from database.downloader.downloader_base import DownloaderBase\n",
    "import database.database_config as db_config\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series_dist(series):\n",
    "    data = series\n",
    "    # 使用matplotlib画直方图\n",
    "    plt.hist(data, bins=60, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Data')\n",
    "    plt.show()\n",
    "\n",
    "class PreProcessing:\n",
    "    def __init__(self, db_downloader:DownloaderBase) -> None:\n",
    "        self.db_downloader = db_downloader\n",
    "\n",
    "    def _build_label(self, stock_dataframe):\n",
    "        N = 5 # 最大持仓周期 = N天，第N+1天开盘卖出\n",
    "        df = stock_dataframe.copy()\n",
    "        # 标签构建\n",
    "        df['future_return'] = df['close'].shift(-N) / df['open'].shift(-1) - 1 # 计算第N日收益率\n",
    "        # 极值处理\n",
    "        df['future_return'] = np.clip(\n",
    "            df['future_return'], \n",
    "            np.nanquantile(df['future_return'], 0.01), \n",
    "            np.nanquantile(df['future_return'], 0.99),\n",
    "            )\n",
    "        # 过滤第二天一字涨停情况\n",
    "        df = df[df['high'].shift(-1) != df['low'].shift(-1)]\n",
    "        return df[['datetime', 'future_return']]\n",
    "\n",
    "\n",
    "    def _process_one_stock(self, stock_code, start_date, end_date):\n",
    "        stock_base = self.db_downloader._download_stock_history_info(stock_code, start_date, end_date) # 获取历史行情\n",
    "        stock_factor_date = self.db_downloader._download_stock_factor_date_info() # 获取日期特征\n",
    "        stock_factor_qlib = self.db_downloader._download_stock_factor_qlib_info(stock_code, start_date, end_date) # 获取量价特征\n",
    "        stock_label = self._build_label(stock_base) # 构建label\n",
    "        stock_df = stock_base.merge(stock_label, on=['datetime']).merge(stock_factor_date, on=['datetime']).merge(stock_factor_qlib, on=['stock_code', 'datetime']) # 整合数据\n",
    "        stock_df = stock_df.dropna()\n",
    "        return stock_df\n",
    "    \n",
    "    def _process_all_stock(self, code_type, start_date, end_date):\n",
    "        # stock_code_list = list(ak.stock_info_a_code_name()['code'].unique()) # 获取A股所有股票列表\n",
    "        # stock_code_list = list(ak.index_stock_cons(\"000905\")['品种代码'].unique()) # 获取中证500的股票代码列表\n",
    "        # stock_code_list = list(ak.index_stock_cons(\"000300\")['品种代码'].unique()) # 获取沪深300的股票代码列表\n",
    "        stock_code_list = list(ak.index_stock_cons(code_type)['品种代码'].unique()) # 获取中证50的股票代码列表\n",
    "        stock_df_list = []\n",
    "        for stock_code in tqdm(stock_code_list, desc=f'Process: {code_type} ...'):\n",
    "            stock_df = self._process_one_stock(stock_code, start_date, end_date)\n",
    "            if not stock_df.empty:\n",
    "                stock_df_list.append(stock_df)\n",
    "        return pd.concat(stock_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conn = sqlite3.connect('../database/hh_quant.db')\n",
    "db_downloader = DownloaderBase(db_conn, db_config)\n",
    "\n",
    "proprocessor = PreProcessing(db_downloader=db_downloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow处理原始数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Senet(tf.keras.layers.Layer):\n",
    "    def __init__(self, reduction_ratio=3, seed=1024, **kwargs):\n",
    "        super(Senet, self).__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.seed = seed  \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.field_size = len(input_shape)\n",
    "        self.reduction_size = max(1, self.field_size // self.reduction_ratio)\n",
    "        self.scale_layer = tf.keras.layers.Dense(units=self.reduction_size, activation='relu')\n",
    "        self.expand_layer = tf.keras.layers.Dense(units=self.field_size, activation='relu')\n",
    "        super(Senet, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # print(f\"Senet Is Training Mode: {training}\")\n",
    "        inputs = [tf.expand_dims(i, axis=1) for i in inputs]\n",
    "        inputs = tf.concat(inputs, axis=1) # [B, N, dim]\n",
    "        Z = tf.reduce_mean(inputs, axis=-1) # [B, N]\n",
    "        A_1 = self.scale_layer(Z, training=training) # [B, X]\n",
    "        A_2 = self.expand_layer(A_1, training=training) # [B, N]\n",
    "        scale_inputs = tf.multiply(inputs, tf.expand_dims(A_2, axis=-1))\n",
    "        output = scale_inputs + inputs # skip-connection\n",
    "        return output # [B, N, dim]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Senet, self).get_config()\n",
    "        config.update({\n",
    "            'reduction_ratio': self.reduction_ratio,\n",
    "            'seed': self.seed\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class Dnn(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units=[64,32], activation=\"relu\", dropout_rate=0.2, use_bn=True, seed=1024, **kwargs):\n",
    "        super(Dnn, self).__init__(**kwargs)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        self.seed = seed\n",
    "        self.dense_layers = []\n",
    "        self.dropout_layers = []\n",
    "        self.bn_layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        for units in self.hidden_units:\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(units=units, activation=self.activation))\n",
    "            self.dropout_layers.append(tf.keras.layers.Dropout(rate=self.dropout_rate, seed=self.seed))\n",
    "            if self.use_bn:\n",
    "                self.bn_layers.append(tf.keras.layers.BatchNormalization())\n",
    "        super(Dnn, self).build(input_shape)  # Be sure to call this at the end\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # print(f\"Dnn Is Training Mode: {training}\")\n",
    "        x = inputs\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            x = self.dense_layers[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn_layers[i](x, training=training)\n",
    "            x = self.dropout_layers[i](x, training=training)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Dnn, self).get_config()\n",
    "        config.update({\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'activation': self.activation,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'use_bn': self.use_bn,\n",
    "            'seed': self.seed\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "class QuantModel(tf.keras.Model):\n",
    "\tdef __init__(self, config, **kwargs):\n",
    "\t\tsuper(QuantModel, self).__init__(**kwargs)\n",
    "\t\tself.config = config\n",
    "\n",
    "\t\t# 添加属性来存储预定义的层\n",
    "\t\tself.lookup_layers = {}\n",
    "\t\tself.embedding_layers = {}\n",
    "\n",
    "        # 创建连续特征的离散化层和嵌入层\n",
    "\t\tfor feature_name, boundaries in self.config.get(\"numeric_features_with_boundaries\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.Discretization(bin_boundaries=boundaries, output_mode='int', name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(boundaries) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "        # 创建整数特征的查找层和嵌入层\n",
    "\t\tfor feature_name, vocab in self.config.get(\"integer_categorical_features_with_vocab\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.IntegerLookup(vocabulary=vocab, name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(vocab) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "\t\t# 创建字符串特征的查找层和嵌入层\n",
    "\t\tfor feature_name, vocab in self.config.get(\"string_categorical_features_with_vocab\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.StringLookup(vocabulary=vocab, name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(vocab) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "\n",
    "\t\t# 任务Dnn层\n",
    "\t\tself.task_tower_list = []\n",
    "\t\tfor task_type in self.config['task_type']:\n",
    "\t\t\ttask_tower = tf.keras.Sequential([\n",
    "\t\t\t\tSenet(reduction_ratio=self.config.get('reduction_ratio', 3), seed=self.config.get('seed', 1024)),\n",
    "\t\t\t\ttf.keras.layers.Flatten(),\n",
    "\t\t\t\tDnn(\n",
    "\t\t\t\t\thidden_units=self.config.get('dnn_hidden_units', [64,32]), \n",
    "\t\t\t\t\tactivation=self.config.get('dnn_activation', 'relu'), \n",
    "\t\t\t\t\tdropout_rate=self.config.get('dnn_dropout', 0.2), \n",
    "\t\t\t\t\tuse_bn=self.config.get('dnn_use_bn', True), \n",
    "\t\t\t\t\tseed=self.config.get('seed', 1024),\n",
    "\t\t\t\t),\n",
    "\t\t\t\ttf.keras.layers.Dense(1, activation=None, name=task_type)\n",
    "\t\t\t])\n",
    "\t\t\tself.task_tower_list.append(task_tower)\n",
    "\n",
    "\tdef call(self, inputs, training=False):\n",
    "\t\t# print(f\"QuantModel Is Training Mode: {training}\")\n",
    "\t\t# 确保inputs是一个字典类型，每个键值对应一个特征输入\n",
    "\t\tif not isinstance(inputs, dict): \n",
    "\t\t\traise ValueError('The inputs to the model should be a dictionary where keys are feature names.')\n",
    "\t\tencoded_features = []\n",
    "    \t# 现在使用已经实例化的层来编码输入\n",
    "\t\tfor feature_name, feature_value in inputs.items():\n",
    "        \t# 使用预定义的查找层和嵌入层\n",
    "\t\t\tlookup_layer = self.lookup_layers[feature_name]\n",
    "\t\t\tembedding_layer = self.embedding_layers[feature_name]\n",
    "\t\t\tencoded_feature = embedding_layer(lookup_layer(feature_value))\n",
    "\t\t\tencoded_features.append(encoded_feature)\n",
    "\t\t\n",
    "\t\t# task任务塔\n",
    "\t\tlogits_list = []\n",
    "\t\tfor task_tower in self.task_tower_list:\n",
    "\t\t\ttask_output = task_tower(encoded_features)\n",
    "\t\t\tlogits_list.append(task_output)\n",
    "\t\treturn logits_list\n",
    "\t\n",
    "\tdef get_config(self):\n",
    "\t\t# 调用基类的get_config方法（如果基类实现了get_config）\n",
    "\t\tconfig = super(QuantModel, self).get_config()\n",
    "        # 添加QuantModel特有的配置信息\n",
    "\t\tconfig.update({\n",
    "            # 假设self.config是一个可序列化的字典，如果不是，你可能需要在这里适当地处理它\n",
    "            'config': self.config\n",
    "        })\n",
    "\t\treturn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 2014-01-01 00:00:00, train_end: 2019-12-31 00:00:00, val_start: 2020-01-01 00:00:00, val_end: 2020-06-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:10<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (59264, 199)\n",
      "validation_data_size: (5000, 199)\n",
      "Epoch 1/20\n",
      "463/463 - 46s - loss: 0.6732 - val_loss: 0.1828 - 46s/epoch - 99ms/step\n",
      "Epoch 2/20\n",
      "463/463 - 15s - loss: 0.2790 - val_loss: 0.1053 - 15s/epoch - 32ms/step\n",
      "Epoch 3/20\n",
      "463/463 - 15s - loss: 0.1788 - val_loss: 0.0618 - 15s/epoch - 33ms/step\n",
      "Epoch 4/20\n",
      "463/463 - 14s - loss: 0.1251 - val_loss: 0.0371 - 14s/epoch - 31ms/step\n",
      "Epoch 5/20\n",
      "463/463 - 16s - loss: 0.0938 - val_loss: 0.0233 - 16s/epoch - 35ms/step\n",
      "Epoch 6/20\n",
      "463/463 - 16s - loss: 0.0705 - val_loss: 0.0151 - 16s/epoch - 34ms/step\n",
      "Epoch 7/20\n",
      "463/463 - 17s - loss: 0.0544 - val_loss: 0.0104 - 17s/epoch - 36ms/step\n",
      "Epoch 8/20\n",
      "463/463 - 16s - loss: 0.0426 - val_loss: 0.0074 - 16s/epoch - 34ms/step\n",
      "Epoch 9/20\n",
      "463/463 - 16s - loss: 0.0325 - val_loss: 0.0057 - 16s/epoch - 34ms/step\n",
      "Epoch 10/20\n",
      "463/463 - 15s - loss: 0.0255 - val_loss: 0.0046 - 15s/epoch - 32ms/step\n",
      "Epoch 11/20\n",
      "463/463 - 15s - loss: 0.0194 - val_loss: 0.0038 - 15s/epoch - 33ms/step\n",
      "Epoch 12/20\n",
      "463/463 - 15s - loss: 0.0148 - val_loss: 0.0034 - 15s/epoch - 33ms/step\n",
      "Epoch 13/20\n",
      "463/463 - 16s - loss: 0.0110 - val_loss: 0.0030 - 16s/epoch - 34ms/step\n",
      "Epoch 14/20\n",
      "463/463 - 15s - loss: 0.0084 - val_loss: 0.0028 - 15s/epoch - 32ms/step\n",
      "Epoch 15/20\n",
      "463/463 - 15s - loss: 0.0063 - val_loss: 0.0027 - 15s/epoch - 32ms/step\n",
      "Epoch 16/20\n",
      "463/463 - 16s - loss: 0.0049 - val_loss: 0.0027 - 16s/epoch - 34ms/step\n",
      "Epoch 17/20\n",
      "463/463 - 15s - loss: 0.0038 - val_loss: 0.0026 - 15s/epoch - 33ms/step\n",
      "Epoch 18/20\n",
      "463/463 - 15s - loss: 0.0032 - val_loss: 0.0026 - 15s/epoch - 32ms/step\n",
      "Epoch 19/20\n",
      "463/463 - 15s - loss: 0.0028 - val_loss: 0.0026 - 15s/epoch - 33ms/step\n",
      "Epoch 20/20\n",
      "463/463 - 16s - loss: 0.0025 - val_loss: 0.0026 - 16s/epoch - 34ms/step\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20200101/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20200101/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 17ms/step\n",
      "train_start: 2014-04-01 00:00:00, train_end: 2020-03-31 00:00:00, val_start: 2020-04-01 00:00:00, val_end: 2020-09-30 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:09<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (59648, 199)\n",
      "validation_data_size: (5410, 199)\n",
      "Epoch 1/20\n",
      "466/466 - 47s - loss: 0.9482 - val_loss: 0.6543 - 47s/epoch - 100ms/step\n",
      "Epoch 2/20\n",
      "466/466 - 16s - loss: 0.4165 - val_loss: 0.1440 - 16s/epoch - 33ms/step\n",
      "Epoch 3/20\n",
      "466/466 - 15s - loss: 0.2756 - val_loss: 0.0821 - 15s/epoch - 31ms/step\n",
      "Epoch 4/20\n",
      "466/466 - 16s - loss: 0.2010 - val_loss: 0.0508 - 16s/epoch - 33ms/step\n",
      "Epoch 5/20\n",
      "466/466 - 16s - loss: 0.1535 - val_loss: 0.0329 - 16s/epoch - 34ms/step\n",
      "Epoch 6/20\n",
      "466/466 - 15s - loss: 0.1224 - val_loss: 0.0219 - 15s/epoch - 32ms/step\n",
      "Epoch 7/20\n",
      "466/466 - 14s - loss: 0.0961 - val_loss: 0.0151 - 14s/epoch - 31ms/step\n",
      "Epoch 8/20\n",
      "466/466 - 15s - loss: 0.0761 - val_loss: 0.0107 - 15s/epoch - 31ms/step\n",
      "Epoch 9/20\n",
      "466/466 - 13s - loss: 0.0607 - val_loss: 0.0079 - 13s/epoch - 29ms/step\n",
      "Epoch 10/20\n",
      "466/466 - 14s - loss: 0.0475 - val_loss: 0.0062 - 14s/epoch - 30ms/step\n",
      "Epoch 11/20\n",
      "466/466 - 15s - loss: 0.0374 - val_loss: 0.0049 - 15s/epoch - 33ms/step\n",
      "Epoch 12/20\n",
      "466/466 - 14s - loss: 0.0293 - val_loss: 0.0041 - 14s/epoch - 30ms/step\n",
      "Epoch 13/20\n",
      "466/466 - 13s - loss: 0.0226 - val_loss: 0.0035 - 13s/epoch - 29ms/step\n",
      "Epoch 14/20\n",
      "466/466 - 14s - loss: 0.0170 - val_loss: 0.0031 - 14s/epoch - 30ms/step\n",
      "Epoch 15/20\n",
      "466/466 - 15s - loss: 0.0131 - val_loss: 0.0029 - 15s/epoch - 33ms/step\n",
      "Epoch 16/20\n",
      "466/466 - 15s - loss: 0.0097 - val_loss: 0.0028 - 15s/epoch - 32ms/step\n",
      "Epoch 17/20\n",
      "466/466 - 14s - loss: 0.0073 - val_loss: 0.0027 - 14s/epoch - 31ms/step\n",
      "Epoch 18/20\n",
      "466/466 - 14s - loss: 0.0055 - val_loss: 0.0026 - 14s/epoch - 30ms/step\n",
      "Epoch 19/20\n",
      "466/466 - 15s - loss: 0.0042 - val_loss: 0.0026 - 15s/epoch - 32ms/step\n",
      "Epoch 20/20\n",
      "466/466 - 15s - loss: 0.0035 - val_loss: 0.0026 - 15s/epoch - 32ms/step\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20200401/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20200401/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 3s 16ms/step\n",
      "train_start: 2014-07-01 00:00:00, train_end: 2020-06-30 00:00:00, val_start: 2020-07-01 00:00:00, val_end: 2020-12-31 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:09<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (59964, 199)\n",
      "validation_data_size: (5567, 199)\n",
      "Epoch 1/20\n",
      "469/469 - 45s - loss: 0.7954 - val_loss: 0.1419 - 45s/epoch - 95ms/step\n",
      "Epoch 2/20\n",
      "469/469 - 16s - loss: 0.3044 - val_loss: 0.0778 - 16s/epoch - 34ms/step\n",
      "Epoch 3/20\n",
      "469/469 - 17s - loss: 0.1961 - val_loss: 0.0396 - 17s/epoch - 35ms/step\n",
      "Epoch 4/20\n",
      "469/469 - 16s - loss: 0.1475 - val_loss: 0.0219 - 16s/epoch - 35ms/step\n",
      "Epoch 5/20\n",
      "469/469 - 17s - loss: 0.1143 - val_loss: 0.0137 - 17s/epoch - 36ms/step\n",
      "Epoch 6/20\n",
      "469/469 - 16s - loss: 0.0963 - val_loss: 0.0094 - 16s/epoch - 34ms/step\n",
      "Epoch 7/20\n",
      "469/469 - 17s - loss: 0.0777 - val_loss: 0.0069 - 17s/epoch - 37ms/step\n",
      "Epoch 8/20\n",
      "469/469 - 15s - loss: 0.0633 - val_loss: 0.0055 - 15s/epoch - 31ms/step\n",
      "Epoch 9/20\n",
      "469/469 - 16s - loss: 0.0527 - val_loss: 0.0046 - 16s/epoch - 34ms/step\n",
      "Epoch 10/20\n",
      "469/469 - 16s - loss: 0.0433 - val_loss: 0.0040 - 16s/epoch - 34ms/step\n",
      "Epoch 11/20\n",
      "469/469 - 17s - loss: 0.0341 - val_loss: 0.0037 - 17s/epoch - 36ms/step\n",
      "Epoch 12/20\n",
      "469/469 - 18s - loss: 0.0273 - val_loss: 0.0034 - 18s/epoch - 39ms/step\n",
      "Epoch 13/20\n",
      "469/469 - 17s - loss: 0.0214 - val_loss: 0.0033 - 17s/epoch - 35ms/step\n",
      "Epoch 14/20\n",
      "469/469 - 18s - loss: 0.0163 - val_loss: 0.0032 - 18s/epoch - 38ms/step\n",
      "Epoch 15/20\n",
      "469/469 - 15s - loss: 0.0130 - val_loss: 0.0031 - 15s/epoch - 33ms/step\n",
      "Epoch 16/20\n",
      "469/469 - 15s - loss: 0.0101 - val_loss: 0.0031 - 15s/epoch - 32ms/step\n",
      "Epoch 17/20\n",
      "469/469 - 15s - loss: 0.0077 - val_loss: 0.0030 - 15s/epoch - 32ms/step\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 104\u001b[0m\n\u001b[1;32m    102\u001b[0m loss \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError()]\n\u001b[1;32m    103\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39mloss)\n\u001b[0;32m--> 104\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/saved_model/model_of_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_start_date\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    113\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_save_path)\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def extract_train_val_data(df, train_start_date, train_end_date, val_start_date, val_end_date):\n",
    "    # print(f\"train_start: {train_start_date}, train_end: {train_end_date}, val_start: {val_start_date}, val_end: {val_end_date}\")\n",
    "    train_start_date = pd.to_datetime(train_start_date)\n",
    "    train_end_date = pd.to_datetime(train_end_date)\n",
    "    val_start_date = pd.to_datetime(val_start_date)\n",
    "    val_end_date = pd.to_datetime(val_end_date)\n",
    "\n",
    "    train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "    val_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "\n",
    "    print(f\"train_data_size: {train_data.shape}\")\n",
    "    print(f\"validation_data_size: {val_data.shape}\")\n",
    "    return train_data, val_data\n",
    "\n",
    "def transfer_data_type(df, columns, dtype):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "def get_numeric_boundaries(series, num_bins=30):\n",
    "    if series.nunique() < num_bins:\n",
    "        boundaries = sorted(series.unique())\n",
    "    else:\n",
    "        boundaries = pd.qcut(series, num_bins, retbins=True, duplicates='drop')[1].tolist()\n",
    "    return boundaries\n",
    "\n",
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=True, batch_size=32):\n",
    "    features = dataframe[feature_cols]\n",
    "    labels = [dataframe[label_col] for label_col in label_cols]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(features), tuple(labels)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(features))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds\n",
    "\n",
    "backtest_start_date = datetime.strptime('20200101', '%Y%m%d')\n",
    "backtest_end_date = datetime.strptime('20240101', '%Y%m%d')\n",
    "val_start_date = backtest_start_date\n",
    "train_period = 6 # year\n",
    "update_period = 6 # month\n",
    "feature_config = {\n",
    "    \"target_feature_name\": [\"future_return\"],\n",
    "    \"numeric_features\": ['KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE0', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME0', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60'],\n",
    "    \"integer_categorical_features\": ['weekday', 'day_of_month', 'month'],\n",
    "    \"string_categorical_features\": ['day_of_week', 'season']\n",
    "}\n",
    "full_feature_names = feature_config.get('numeric_features', []) + feature_config.get('integer_categorical_features', []) + feature_config.get('string_categorical_features', [])\n",
    "\n",
    "rolling_training_flag = True\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "epoch_date_list = []\n",
    "\n",
    "while rolling_training_flag:\n",
    "    val_end_date = val_start_date + relativedelta(months=update_period) - relativedelta(days=1)\n",
    "    if val_start_date < backtest_end_date:\n",
    "        train_start_date = val_start_date - relativedelta(years=train_period)\n",
    "        train_end_date = val_start_date - relativedelta(days=1)\n",
    "        current_date_period = [train_start_date, train_end_date, val_start_date, val_end_date]\n",
    "        epoch_date_list.append([train_start_date, train_end_date, val_start_date, val_end_date])\n",
    "        print(f\"train_start: {train_start_date}, train_end: {train_end_date}, val_start: {val_start_date}, val_end: {val_end_date}\")\n",
    "         # 在这里处理滚动训练流程 ...\n",
    "        df = proprocessor._process_all_stock(code_type='000016', start_date=train_start_date.strftime(\"%Y%m%d\"), end_date=val_end_date.strftime(\"%Y%m%d\"))\n",
    "        train_data, val_data = extract_train_val_data(df, *current_date_period)\n",
    "\n",
    "        train_ds = df_to_dataset(train_data, full_feature_names, feature_config.get('target_feature_name', []), shuffle=True, batch_size=batch_size)\n",
    "        val_ds = df_to_dataset(val_data, full_feature_names, feature_config.get('target_feature_name', []), shuffle=False, batch_size=batch_size)\n",
    "\n",
    "        model_config = {\n",
    "             \"seed\": 1024,\n",
    "             \"reduction_ratio\": 3,\n",
    "            \"dnn_hidden_units\": [64,32],\n",
    "            \"dnn_activation\": 'relu',\n",
    "            \"dnn_dropout\": 0.1,\n",
    "            \"dnn_use_bn\": True,\n",
    "            \"numeric_features_with_boundaries\": {k: list(get_numeric_boundaries(train_data[k])) for k in feature_config.get('numeric_features', [])},\n",
    "            \"integer_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('integer_categorical_features', [])},\n",
    "            \"string_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('string_categorical_features', [])},\n",
    "            \"feature_embedding_dims\": 6,\n",
    "            \"task_type\": ['reg'],\n",
    "        }\n",
    "        model = QuantModel(model_config)\n",
    "\n",
    "        monitor_indicator = 'val_loss'\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=monitor_indicator,\n",
    "            verbose=2,\n",
    "            patience=10,\n",
    "            mode='min',\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "\n",
    "        log_dir = \"./logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "        # 配置optimizer\n",
    "        # optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4) # for Mac M1/M2\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # for intel\n",
    "        loss = [tf.keras.losses.MeanSquaredError()]\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        model.fit(\n",
    "                train_ds, \n",
    "                validation_data=val_ds, \n",
    "                epochs=20,\n",
    "                verbose=2,\n",
    "                callbacks=[tensorboard_callback, early_stopping]\n",
    "        )\n",
    "\n",
    "        model_save_path = f'./models/saved_model/model_of_{val_start_date.strftime(\"%Y%m%d\")}'\n",
    "        model.save(model_save_path)\n",
    "        # best_model = tf.keras.models.load_model('./best_model')\n",
    "\n",
    "        model_red_result = model.predict(val_ds)\n",
    "        output_df = val_data[['stock_code', 'stock_name', 'datetime']]\n",
    "        output_df['future_return'] = val_data['future_return']\n",
    "        output_df['future_return_pred'] = model_red_result[0]\n",
    "        output_file_path = f'./predicts/stock_selection_results_{val_start_date.strftime(\"%Y%m%d\")}.pkl'\n",
    "        output_df.to_pickle(output_file_path)\n",
    "\n",
    "        # 更新周期\n",
    "        val_start_date += relativedelta(months=3) \n",
    "    else:\n",
    "        rolling_training_flag=False # 结束滚动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ./logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=float64, numpy=-0.008333333333333304>,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
