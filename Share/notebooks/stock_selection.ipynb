{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import akshare as ak\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime, date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "from database.downloader.downloader_base import DownloaderBase\n",
    "import database.database_config as db_config\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_series_dist(series):\n",
    "    data = series\n",
    "    # 使用matplotlib画直方图\n",
    "    plt.hist(data, bins=60, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Data')\n",
    "    plt.show()\n",
    "\n",
    "class PreProcessing:\n",
    "    def __init__(self, db_downloader:DownloaderBase) -> None:\n",
    "        self.db_downloader = db_downloader\n",
    "\n",
    "    def _build_label(self, stock_dataframe):\n",
    "        N = 5 # 最大持仓周期 = N天，第N+1天开盘卖出\n",
    "        df = stock_dataframe.copy()\n",
    "        # 标签构建\n",
    "        df['future_return'] = df['close'].shift(-N) / df['open'].shift(-1) - 1 # 计算第N日收益率\n",
    "        # 极值处理\n",
    "        df['future_return'] = np.clip(\n",
    "            df['future_return'], \n",
    "            np.nanquantile(df['future_return'], 0.01), \n",
    "            np.nanquantile(df['future_return'], 0.99),\n",
    "            )\n",
    "        # 过滤第二天一字涨停情况\n",
    "        df = df[df['high'].shift(-1) != df['low'].shift(-1)]\n",
    "        return df[['datetime', 'future_return']]\n",
    "\n",
    "\n",
    "    def _process_one_stock(self, stock_code, start_date, end_date):\n",
    "        stock_base = self.db_downloader._download_stock_history_info(stock_code, start_date, end_date) # 获取历史行情\n",
    "        stock_factor_date = self.db_downloader._download_stock_factor_date_info() # 获取日期特征\n",
    "        stock_factor_qlib = self.db_downloader._download_stock_factor_qlib_info(stock_code, start_date, end_date) # 获取量价特征\n",
    "        stock_label = self._build_label(stock_base) # 构建label\n",
    "        stock_df = stock_base.merge(stock_label, on=['datetime']).merge(stock_factor_date, on=['datetime']).merge(stock_factor_qlib, on=['stock_code', 'datetime']) # 整合数据\n",
    "        stock_df = stock_df.dropna()\n",
    "        return stock_df\n",
    "    \n",
    "    def _process_all_stock(self, code_type, start_date, end_date):\n",
    "        # stock_code_list = list(ak.stock_info_a_code_name()['code'].unique()) # 获取A股所有股票列表\n",
    "        # stock_code_list = list(ak.index_stock_cons(\"000905\")['品种代码'].unique()) # 获取中证500的股票代码列表\n",
    "        # stock_code_list = list(ak.index_stock_cons(\"000300\")['品种代码'].unique()) # 获取沪深300的股票代码列表\n",
    "        stock_code_list = list(ak.index_stock_cons(code_type)['品种代码'].unique()) # 获取中证50的股票代码列表\n",
    "        stock_df_list = []\n",
    "        for stock_code in tqdm(stock_code_list, desc=f'Process: {code_type} ...'):\n",
    "            stock_df = self._process_one_stock(stock_code, start_date, end_date)\n",
    "            if not stock_df.empty:\n",
    "                stock_df_list.append(stock_df)\n",
    "        return pd.concat(stock_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conn = sqlite3.connect('../database/hh_quant.db')\n",
    "db_downloader = DownloaderBase(db_conn, db_config)\n",
    "\n",
    "proprocessor = PreProcessing(db_downloader=db_downloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow处理原始数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Senet(tf.keras.layers.Layer):\n",
    "    def __init__(self, reduction_ratio=3, seed=1024, **kwargs):\n",
    "        super(Senet, self).__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.seed = seed  \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.field_size = len(input_shape)\n",
    "        self.reduction_size = max(1, self.field_size // self.reduction_ratio)\n",
    "        self.scale_layer = tf.keras.layers.Dense(units=self.reduction_size, activation='relu')\n",
    "        self.expand_layer = tf.keras.layers.Dense(units=self.field_size, activation='relu')\n",
    "        super(Senet, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # print(f\"Senet Is Training Mode: {training}\")\n",
    "        inputs = [tf.expand_dims(i, axis=1) for i in inputs]\n",
    "        inputs = tf.concat(inputs, axis=1) # [B, N, dim]\n",
    "        Z = tf.reduce_mean(inputs, axis=-1) # [B, N]\n",
    "        A_1 = self.scale_layer(Z, training=training) # [B, X]\n",
    "        A_2 = self.expand_layer(A_1, training=training) # [B, N]\n",
    "        scale_inputs = tf.multiply(inputs, tf.expand_dims(A_2, axis=-1))\n",
    "        output = scale_inputs + inputs # skip-connection\n",
    "        return output # [B, N, dim]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Senet, self).get_config()\n",
    "        config.update({\n",
    "            'reduction_ratio': self.reduction_ratio,\n",
    "            'seed': self.seed\n",
    "        })\n",
    "        return config\n",
    "\n",
    "class Dnn(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units=[64,32], activation=\"relu\", dropout_rate=0.2, use_bn=True, seed=1024, **kwargs):\n",
    "        super(Dnn, self).__init__(**kwargs)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        self.seed = seed\n",
    "        self.dense_layers = []\n",
    "        self.dropout_layers = []\n",
    "        self.bn_layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        for units in self.hidden_units:\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(units=units, activation=self.activation))\n",
    "            self.dropout_layers.append(tf.keras.layers.Dropout(rate=self.dropout_rate, seed=self.seed))\n",
    "            if self.use_bn:\n",
    "                self.bn_layers.append(tf.keras.layers.BatchNormalization())\n",
    "        super(Dnn, self).build(input_shape)  # Be sure to call this at the end\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # print(f\"Dnn Is Training Mode: {training}\")\n",
    "        x = inputs\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            x = self.dense_layers[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn_layers[i](x, training=training)\n",
    "            x = self.dropout_layers[i](x, training=training)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Dnn, self).get_config()\n",
    "        config.update({\n",
    "            'hidden_units': self.hidden_units,\n",
    "            'activation': self.activation,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'use_bn': self.use_bn,\n",
    "            'seed': self.seed\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "class QuantModel(tf.keras.Model):\n",
    "\tdef __init__(self, config, **kwargs):\n",
    "\t\tsuper(QuantModel, self).__init__(**kwargs)\n",
    "\t\tself.config = config\n",
    "\n",
    "\t\t# 添加属性来存储预定义的层\n",
    "\t\tself.lookup_layers = {}\n",
    "\t\tself.embedding_layers = {}\n",
    "\n",
    "        # 创建连续特征的离散化层和嵌入层\n",
    "\t\tfor feature_name, boundaries in self.config.get(\"numeric_features_with_boundaries\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.Discretization(bin_boundaries=boundaries, output_mode='int', name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(boundaries) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "        # 创建整数特征的查找层和嵌入层\n",
    "\t\tfor feature_name, vocab in self.config.get(\"integer_categorical_features_with_vocab\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.IntegerLookup(vocabulary=vocab, name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(vocab) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "\t\t# 创建字符串特征的查找层和嵌入层\n",
    "\t\tfor feature_name, vocab in self.config.get(\"string_categorical_features_with_vocab\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.StringLookup(vocabulary=vocab, name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(vocab) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "\n",
    "\t\t# 任务Dnn层\n",
    "\t\tself.task_tower_list = []\n",
    "\t\tfor task_type in self.config['task_type']:\n",
    "\t\t\ttask_tower = tf.keras.Sequential([\n",
    "\t\t\t\tSenet(reduction_ratio=self.config.get('reduction_ratio', 3), seed=self.config.get('seed', 1024)),\n",
    "\t\t\t\ttf.keras.layers.Flatten(),\n",
    "\t\t\t\tDnn(\n",
    "\t\t\t\t\thidden_units=self.config.get('dnn_hidden_units', [64,32]), \n",
    "\t\t\t\t\tactivation=self.config.get('dnn_activation', 'relu'), \n",
    "\t\t\t\t\tdropout_rate=self.config.get('dnn_dropout', 0.2), \n",
    "\t\t\t\t\tuse_bn=self.config.get('dnn_use_bn', True), \n",
    "\t\t\t\t\tseed=self.config.get('seed', 1024),\n",
    "\t\t\t\t),\n",
    "\t\t\t\ttf.keras.layers.Dense(1, activation=None, name=task_type)\n",
    "\t\t\t])\n",
    "\t\t\tself.task_tower_list.append(task_tower)\n",
    "\n",
    "\tdef call(self, inputs, training=False):\n",
    "\t\t# print(f\"QuantModel Is Training Mode: {training}\")\n",
    "\t\t# 确保inputs是一个字典类型，每个键值对应一个特征输入\n",
    "\t\tif not isinstance(inputs, dict): \n",
    "\t\t\traise ValueError('The inputs to the model should be a dictionary where keys are feature names.')\n",
    "\t\tencoded_features = []\n",
    "    \t# 现在使用已经实例化的层来编码输入\n",
    "\t\tfor feature_name, feature_value in inputs.items():\n",
    "        \t# 使用预定义的查找层和嵌入层\n",
    "\t\t\tlookup_layer = self.lookup_layers[feature_name]\n",
    "\t\t\tembedding_layer = self.embedding_layers[feature_name]\n",
    "\t\t\tencoded_feature = embedding_layer(lookup_layer(feature_value))\n",
    "\t\t\tencoded_features.append(encoded_feature)\n",
    "\t\t\n",
    "\t\t# task任务塔\n",
    "\t\tlogits_list = []\n",
    "\t\tfor task_tower in self.task_tower_list:\n",
    "\t\t\ttask_output = task_tower(encoded_features)\n",
    "\t\t\tlogits_list.append(task_output)\n",
    "\t\treturn logits_list\n",
    "\t\n",
    "\tdef get_config(self):\n",
    "\t\t# 调用基类的get_config方法（如果基类实现了get_config）\n",
    "\t\tconfig = super(QuantModel, self).get_config()\n",
    "        # 添加QuantModel特有的配置信息\n",
    "\t\tconfig.update({\n",
    "            # 假设self.config是一个可序列化的字典，如果不是，你可能需要在这里适当地处理它\n",
    "            'config': self.config\n",
    "        })\n",
    "\t\treturn config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_val_data(df, train_start_date, train_end_date, val_start_date, val_end_date):\n",
    "    train_start_date = pd.to_datetime(train_start_date)\n",
    "    train_end_date = pd.to_datetime(train_end_date)\n",
    "    val_start_date = pd.to_datetime(val_start_date)\n",
    "    val_end_date = pd.to_datetime(val_end_date)\n",
    "\n",
    "    train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "    val_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "\n",
    "    print(f\"train_data_size: {train_data.shape}\")\n",
    "    print(f\"validation_data_size: {val_data.shape}\")\n",
    "    return train_data, val_data\n",
    "\n",
    "def transfer_data_type(df, columns, dtype):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "def get_numeric_boundaries(series, num_bins=30):\n",
    "    if series.nunique() < num_bins:\n",
    "        boundaries = sorted(series.unique())\n",
    "    else:\n",
    "        boundaries = pd.qcut(series, num_bins, retbins=True, duplicates='drop')[1].tolist()\n",
    "    return boundaries\n",
    "\n",
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=True, batch_size=32):\n",
    "    features = dataframe[feature_cols]\n",
    "    labels = [dataframe[label_col] for label_col in label_cols]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(features), tuple(labels)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(features))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['20140101', '20191231', '20200101', '20200630'],\n",
       " ['20140701', '20200630', '20200701', '20201231'],\n",
       " ['20150101', '20201231', '20210101', '20210630'],\n",
       " ['20150701', '20210630', '20210701', '20211231'],\n",
       " ['20160101', '20211231', '20220101', '20220630'],\n",
       " ['20160701', '20220630', '20220701', '20221231'],\n",
       " ['20170101', '20221231', '20230101', '20230630'],\n",
       " ['20170701', '20230630', '20230701', '20231231']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_start_date = '20200101'\n",
    "backtest_end_date = '20240101'\n",
    "train_period = 6 # year：训练数据周期长度\n",
    "update_period = 6 # month：模型更新周期长度\n",
    "\n",
    "def get_rolling_date_period(backtest_start_date, backtest_end_date, training_period, update_period):\n",
    "    backtest_start_date = datetime.strptime(backtest_start_date, '%Y%m%d')\n",
    "    backtest_end_date = datetime.strptime(backtest_end_date, '%Y%m%d')\n",
    "    result = []\n",
    "    rolling_flag = True\n",
    "    while rolling_flag:\n",
    "        current_val_start_date = backtest_start_date\n",
    "        current_val_end_date = current_val_start_date + relativedelta(months=update_period) - relativedelta(days=1)\n",
    "        if current_val_start_date < backtest_end_date:\n",
    "            current_train_start_date = current_val_start_date - relativedelta(years=training_period)\n",
    "            current_train_end_date = current_val_start_date - relativedelta(days=1)\n",
    "            result.append([\n",
    "                current_train_start_date.strftime(\"%Y%m%d\"),\n",
    "                current_train_end_date.strftime(\"%Y%m%d\"),\n",
    "                current_val_start_date.strftime(\"%Y%m%d\"),\n",
    "                current_val_end_date.strftime(\"%Y%m%d\")\n",
    "                ])\n",
    "            backtest_start_date += relativedelta(months=update_period) \n",
    "        else:\n",
    "            rolling_flag=False # 结束滚动训练\n",
    "    return result\n",
    "\n",
    "rolling_period = get_rolling_date_period(backtest_start_date, backtest_end_date, train_period, update_period)\n",
    "rolling_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20140101, train_end: 20191231, val_start: 20200101, val_end: 20200630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:02<00:00, 24.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (59264, 199)\n",
      "validation_data_size: (5000, 199)\n",
      "Epoch 1/50\n",
      "463/463 - 12s - loss: 0.7604 - val_loss: 0.1998 - 12s/epoch - 27ms/step\n",
      "Epoch 2/50\n",
      "463/463 - 6s - loss: 0.3056 - val_loss: 0.1021 - 6s/epoch - 12ms/step\n",
      "Epoch 3/50\n",
      "463/463 - 6s - loss: 0.1898 - val_loss: 0.0552 - 6s/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "463/463 - 6s - loss: 0.1365 - val_loss: 0.0308 - 6s/epoch - 12ms/step\n",
      "Epoch 5/50\n",
      "463/463 - 6s - loss: 0.1020 - val_loss: 0.0188 - 6s/epoch - 13ms/step\n",
      "Epoch 6/50\n",
      "463/463 - 5s - loss: 0.0807 - val_loss: 0.0122 - 5s/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "463/463 - 6s - loss: 0.0634 - val_loss: 0.0085 - 6s/epoch - 13ms/step\n",
      "Epoch 8/50\n",
      "463/463 - 6s - loss: 0.0513 - val_loss: 0.0063 - 6s/epoch - 13ms/step\n",
      "Epoch 9/50\n",
      "463/463 - 6s - loss: 0.0406 - val_loss: 0.0049 - 6s/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "463/463 - 6s - loss: 0.0325 - val_loss: 0.0041 - 6s/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "463/463 - 6s - loss: 0.0256 - val_loss: 0.0036 - 6s/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "463/463 - 5s - loss: 0.0205 - val_loss: 0.0032 - 5s/epoch - 12ms/step\n",
      "Epoch 13/50\n",
      "463/463 - 5s - loss: 0.0159 - val_loss: 0.0029 - 5s/epoch - 12ms/step\n",
      "Epoch 14/50\n",
      "463/463 - 5s - loss: 0.0125 - val_loss: 0.0028 - 5s/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "463/463 - 5s - loss: 0.0097 - val_loss: 0.0027 - 5s/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "463/463 - 5s - loss: 0.0075 - val_loss: 0.0026 - 5s/epoch - 12ms/step\n",
      "Epoch 17/50\n",
      "463/463 - 5s - loss: 0.0059 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 18/50\n",
      "463/463 - 5s - loss: 0.0047 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 19/50\n",
      "463/463 - 5s - loss: 0.0038 - val_loss: 0.0026 - 5s/epoch - 12ms/step\n",
      "Epoch 20/50\n",
      "463/463 - 5s - loss: 0.0032 - val_loss: 0.0026 - 5s/epoch - 12ms/step\n",
      "Epoch 21/50\n",
      "463/463 - 5s - loss: 0.0028 - val_loss: 0.0026 - 5s/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "463/463 - 6s - loss: 0.0026 - val_loss: 0.0026 - 6s/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "463/463 - 5s - loss: 0.0024 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 24/50\n",
      "463/463 - 5s - loss: 0.0024 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 25/50\n",
      "463/463 - 5s - loss: 0.0024 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 26/50\n",
      "463/463 - 5s - loss: 0.0023 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "463/463 - 5s - loss: 0.0023 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 28/50\n",
      "463/463 - 5s - loss: 0.0023 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 29/50\n",
      "463/463 - 5s - loss: 0.0023 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 30/50\n",
      "463/463 - 5s - loss: 0.0023 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "463/463 - 5s - loss: 0.0023 - val_loss: 0.0026 - 5s/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "463/463 - 6s - loss: 0.0023 - val_loss: 0.0026 - 6s/epoch - 13ms/step\n",
      "Epoch 33/50\n",
      "463/463 - 5s - loss: 0.0022 - val_loss: 0.0027 - 5s/epoch - 12ms/step\n",
      "Epoch 34/50\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "463/463 - 5s - loss: 0.0022 - val_loss: 0.0027 - 5s/epoch - 12ms/step\n",
      "Epoch 34: early stopping\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20200101/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20200101/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:  12%|█▎        | 1/8 [04:25<30:58, 265.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20140701, train_end: 20200630, val_start: 20200701, val_end: 20201231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:02<00:00, 24.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (59964, 199)\n",
      "validation_data_size: (5567, 199)\n",
      "Epoch 1/50\n",
      "469/469 - 12s - loss: 0.6161 - val_loss: 0.3057 - 12s/epoch - 27ms/step\n",
      "Epoch 2/50\n",
      "469/469 - 5s - loss: 0.2459 - val_loss: 0.0925 - 5s/epoch - 12ms/step\n",
      "Epoch 3/50\n",
      "469/469 - 5s - loss: 0.1557 - val_loss: 0.0496 - 5s/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "469/469 - 5s - loss: 0.1089 - val_loss: 0.0269 - 5s/epoch - 12ms/step\n",
      "Epoch 5/50\n",
      "469/469 - 5s - loss: 0.0802 - val_loss: 0.0158 - 5s/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "469/469 - 5s - loss: 0.0613 - val_loss: 0.0102 - 5s/epoch - 11ms/step\n",
      "Epoch 7/50\n",
      "469/469 - 5s - loss: 0.0471 - val_loss: 0.0070 - 5s/epoch - 11ms/step\n",
      "Epoch 8/50\n",
      "469/469 - 5s - loss: 0.0368 - val_loss: 0.0053 - 5s/epoch - 11ms/step\n",
      "Epoch 9/50\n",
      "469/469 - 6s - loss: 0.0290 - val_loss: 0.0045 - 6s/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "469/469 - 5s - loss: 0.0226 - val_loss: 0.0039 - 5s/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "469/469 - 5s - loss: 0.0176 - val_loss: 0.0035 - 5s/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "469/469 - 5s - loss: 0.0135 - val_loss: 0.0033 - 5s/epoch - 12ms/step\n",
      "Epoch 13/50\n",
      "469/469 - 6s - loss: 0.0101 - val_loss: 0.0031 - 6s/epoch - 13ms/step\n",
      "Epoch 14/50\n",
      "469/469 - 5s - loss: 0.0076 - val_loss: 0.0030 - 5s/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "469/469 - 5s - loss: 0.0058 - val_loss: 0.0030 - 5s/epoch - 11ms/step\n",
      "Epoch 16/50\n",
      "469/469 - 6s - loss: 0.0045 - val_loss: 0.0029 - 6s/epoch - 12ms/step\n",
      "Epoch 17/50\n",
      "469/469 - 5s - loss: 0.0037 - val_loss: 0.0029 - 5s/epoch - 11ms/step\n",
      "Epoch 18/50\n",
      "469/469 - 5s - loss: 0.0031 - val_loss: 0.0029 - 5s/epoch - 11ms/step\n",
      "Epoch 19/50\n",
      "469/469 - 5s - loss: 0.0028 - val_loss: 0.0029 - 5s/epoch - 12ms/step\n",
      "Epoch 20/50\n",
      "469/469 - 6s - loss: 0.0026 - val_loss: 0.0029 - 6s/epoch - 12ms/step\n",
      "Epoch 21/50\n",
      "469/469 - 6s - loss: 0.0025 - val_loss: 0.0029 - 6s/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "469/469 - 5s - loss: 0.0025 - val_loss: 0.0029 - 5s/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "469/469 - 6s - loss: 0.0025 - val_loss: 0.0029 - 6s/epoch - 12ms/step\n",
      "Epoch 24/50\n",
      "469/469 - 6s - loss: 0.0025 - val_loss: 0.0029 - 6s/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "469/469 - 5s - loss: 0.0025 - val_loss: 0.0029 - 5s/epoch - 11ms/step\n",
      "Epoch 26/50\n",
      "469/469 - 5s - loss: 0.0025 - val_loss: 0.0029 - 5s/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "469/469 - 5s - loss: 0.0025 - val_loss: 0.0029 - 5s/epoch - 11ms/step\n",
      "Epoch 28/50\n",
      "469/469 - 5s - loss: 0.0025 - val_loss: 0.0029 - 5s/epoch - 11ms/step\n",
      "Epoch 29/50\n",
      "469/469 - 5s - loss: 0.0024 - val_loss: 0.0029 - 5s/epoch - 11ms/step\n",
      "Epoch 30/50\n",
      "469/469 - 5s - loss: 0.0024 - val_loss: 0.0029 - 5s/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "469/469 - 5s - loss: 0.0024 - val_loss: 0.0029 - 5s/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "469/469 - 5s - loss: 0.0023 - val_loss: 0.0030 - 5s/epoch - 11ms/step\n",
      "Epoch 33/50\n",
      "469/469 - 5s - loss: 0.0022 - val_loss: 0.0030 - 5s/epoch - 11ms/step\n",
      "Epoch 34/50\n",
      "469/469 - 5s - loss: 0.0021 - val_loss: 0.0031 - 5s/epoch - 11ms/step\n",
      "Epoch 35/50\n",
      "469/469 - 5s - loss: 0.0020 - val_loss: 0.0031 - 5s/epoch - 11ms/step\n",
      "Epoch 36/50\n",
      "469/469 - 5s - loss: 0.0019 - val_loss: 0.0032 - 5s/epoch - 11ms/step\n",
      "Epoch 37/50\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "469/469 - 5s - loss: 0.0018 - val_loss: 0.0032 - 5s/epoch - 11ms/step\n",
      "Epoch 37: early stopping\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20200701/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20200701/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:  25%|██▌       | 2/8 [09:06<27:27, 274.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20150101, train_end: 20201231, val_start: 20210101, val_end: 20210630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:02<00:00, 24.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (60799, 199)\n",
      "validation_data_size: (5307, 199)\n",
      "Epoch 1/50\n",
      "475/475 - 12s - loss: 0.7925 - val_loss: 0.0484 - 12s/epoch - 25ms/step\n",
      "Epoch 2/50\n",
      "475/475 - 5s - loss: 0.3231 - val_loss: 0.1080 - 5s/epoch - 11ms/step\n",
      "Epoch 3/50\n",
      "475/475 - 5s - loss: 0.2067 - val_loss: 0.0557 - 5s/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "475/475 - 5s - loss: 0.1469 - val_loss: 0.0301 - 5s/epoch - 11ms/step\n",
      "Epoch 5/50\n",
      "475/475 - 5s - loss: 0.1124 - val_loss: 0.0180 - 5s/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "475/475 - 5s - loss: 0.0875 - val_loss: 0.0119 - 5s/epoch - 11ms/step\n",
      "Epoch 7/50\n",
      "475/475 - 5s - loss: 0.0708 - val_loss: 0.0084 - 5s/epoch - 11ms/step\n",
      "Epoch 8/50\n",
      "475/475 - 5s - loss: 0.0569 - val_loss: 0.0065 - 5s/epoch - 11ms/step\n",
      "Epoch 9/50\n",
      "475/475 - 5s - loss: 0.0455 - val_loss: 0.0053 - 5s/epoch - 11ms/step\n",
      "Epoch 10/50\n",
      "475/475 - 5s - loss: 0.0354 - val_loss: 0.0046 - 5s/epoch - 11ms/step\n",
      "Epoch 11/50\n",
      "475/475 - 5s - loss: 0.0280 - val_loss: 0.0042 - 5s/epoch - 11ms/step\n",
      "Epoch 12/50\n",
      "475/475 - 5s - loss: 0.0220 - val_loss: 0.0039 - 5s/epoch - 11ms/step\n",
      "Epoch 13/50\n",
      "475/475 - 6s - loss: 0.0172 - val_loss: 0.0037 - 6s/epoch - 12ms/step\n",
      "Epoch 14/50\n",
      "475/475 - 5s - loss: 0.0131 - val_loss: 0.0036 - 5s/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "475/475 - 6s - loss: 0.0103 - val_loss: 0.0035 - 6s/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "475/475 - 5s - loss: 0.0077 - val_loss: 0.0035 - 5s/epoch - 11ms/step\n",
      "Epoch 17/50\n",
      "475/475 - 5s - loss: 0.0059 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 18/50\n",
      "475/475 - 5s - loss: 0.0046 - val_loss: 0.0035 - 5s/epoch - 11ms/step\n",
      "Epoch 19/50\n",
      "475/475 - 5s - loss: 0.0038 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 20/50\n",
      "475/475 - 5s - loss: 0.0032 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 21/50\n",
      "475/475 - 5s - loss: 0.0029 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 22/50\n",
      "475/475 - 5s - loss: 0.0027 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 23/50\n",
      "475/475 - 5s - loss: 0.0026 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 24/50\n",
      "475/475 - 5s - loss: 0.0025 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 25/50\n",
      "475/475 - 5s - loss: 0.0025 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 26/50\n",
      "475/475 - 5s - loss: 0.0025 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "475/475 - 6s - loss: 0.0025 - val_loss: 0.0034 - 6s/epoch - 12ms/step\n",
      "Epoch 28/50\n",
      "475/475 - 5s - loss: 0.0025 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 29/50\n",
      "475/475 - 5s - loss: 0.0025 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 30/50\n",
      "475/475 - 5s - loss: 0.0025 - val_loss: 0.0034 - 5s/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "475/475 - 5s - loss: 0.0025 - val_loss: 0.0035 - 5s/epoch - 11ms/step\n",
      "Epoch 32/50\n",
      "475/475 - 5s - loss: 0.0024 - val_loss: 0.0035 - 5s/epoch - 11ms/step\n",
      "Epoch 33/50\n",
      "475/475 - 5s - loss: 0.0024 - val_loss: 0.0035 - 5s/epoch - 11ms/step\n",
      "Epoch 34/50\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "475/475 - 5s - loss: 0.0023 - val_loss: 0.0036 - 5s/epoch - 11ms/step\n",
      "Epoch 34: early stopping\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20210101/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20210101/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:  38%|███▊      | 3/8 [13:30<22:28, 269.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20150701, train_end: 20210630, val_start: 20210701, val_end: 20211231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:02<00:00, 24.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (61763, 199)\n",
      "validation_data_size: (5723, 199)\n",
      "Epoch 1/50\n",
      "483/483 - 13s - loss: 0.5469 - val_loss: 0.0405 - 13s/epoch - 26ms/step\n",
      "Epoch 2/50\n",
      "483/483 - 6s - loss: 0.2170 - val_loss: 0.0683 - 6s/epoch - 12ms/step\n",
      "Epoch 3/50\n",
      "483/483 - 6s - loss: 0.1359 - val_loss: 0.0364 - 6s/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "483/483 - 6s - loss: 0.0969 - val_loss: 0.0213 - 6s/epoch - 12ms/step\n",
      "Epoch 5/50\n",
      "483/483 - 6s - loss: 0.0724 - val_loss: 0.0136 - 6s/epoch - 12ms/step\n",
      "Epoch 6/50\n",
      "483/483 - 6s - loss: 0.0565 - val_loss: 0.0095 - 6s/epoch - 12ms/step\n",
      "Epoch 7/50\n",
      "483/483 - 6s - loss: 0.0441 - val_loss: 0.0072 - 6s/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "483/483 - 6s - loss: 0.0341 - val_loss: 0.0057 - 6s/epoch - 12ms/step\n",
      "Epoch 9/50\n",
      "483/483 - 6s - loss: 0.0259 - val_loss: 0.0049 - 6s/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "483/483 - 6s - loss: 0.0196 - val_loss: 0.0043 - 6s/epoch - 12ms/step\n",
      "Epoch 11/50\n",
      "483/483 - 6s - loss: 0.0149 - val_loss: 0.0039 - 6s/epoch - 12ms/step\n",
      "Epoch 12/50\n",
      "483/483 - 6s - loss: 0.0111 - val_loss: 0.0036 - 6s/epoch - 12ms/step\n",
      "Epoch 13/50\n",
      "483/483 - 6s - loss: 0.0082 - val_loss: 0.0035 - 6s/epoch - 12ms/step\n",
      "Epoch 14/50\n",
      "483/483 - 6s - loss: 0.0062 - val_loss: 0.0033 - 6s/epoch - 12ms/step\n",
      "Epoch 15/50\n",
      "483/483 - 6s - loss: 0.0047 - val_loss: 0.0033 - 6s/epoch - 12ms/step\n",
      "Epoch 16/50\n",
      "483/483 - 6s - loss: 0.0037 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 17/50\n",
      "483/483 - 6s - loss: 0.0031 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 18/50\n",
      "483/483 - 6s - loss: 0.0027 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 19/50\n",
      "483/483 - 6s - loss: 0.0025 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 20/50\n",
      "483/483 - 6s - loss: 0.0024 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 21/50\n",
      "483/483 - 6s - loss: 0.0024 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "483/483 - 6s - loss: 0.0024 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "483/483 - 6s - loss: 0.0024 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 24/50\n",
      "483/483 - 6s - loss: 0.0024 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "483/483 - 6s - loss: 0.0024 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 26/50\n",
      "483/483 - 6s - loss: 0.0024 - val_loss: 0.0032 - 6s/epoch - 13ms/step\n",
      "Epoch 27/50\n",
      "483/483 - 6s - loss: 0.0024 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 28/50\n",
      "483/483 - 6s - loss: 0.0023 - val_loss: 0.0032 - 6s/epoch - 13ms/step\n",
      "Epoch 29/50\n",
      "483/483 - 6s - loss: 0.0023 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 30/50\n",
      "483/483 - 6s - loss: 0.0023 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "483/483 - 6s - loss: 0.0023 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "Restoring model weights from the end of the best epoch: 22.\n",
      "483/483 - 6s - loss: 0.0022 - val_loss: 0.0032 - 6s/epoch - 12ms/step\n",
      "Epoch 32: early stopping\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20210701/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20210701/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:  50%|█████     | 4/8 [17:55<17:52, 268.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20160101, train_end: 20211231, val_start: 20220101, val_end: 20220630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:02<00:00, 23.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (62931, 199)\n",
      "validation_data_size: (5482, 199)\n",
      "Epoch 1/50\n",
      "492/492 - 13s - loss: 0.7202 - val_loss: 0.1032 - 13s/epoch - 26ms/step\n",
      "Epoch 2/50\n",
      "492/492 - 5s - loss: 0.2818 - val_loss: 0.0827 - 5s/epoch - 11ms/step\n",
      "Epoch 3/50\n",
      "492/492 - 5s - loss: 0.1722 - val_loss: 0.0369 - 5s/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "492/492 - 5s - loss: 0.1234 - val_loss: 0.0188 - 5s/epoch - 11ms/step\n",
      "Epoch 5/50\n",
      "492/492 - 5s - loss: 0.0948 - val_loss: 0.0111 - 5s/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "492/492 - 5s - loss: 0.0753 - val_loss: 0.0075 - 5s/epoch - 11ms/step\n",
      "Epoch 7/50\n",
      "492/492 - 5s - loss: 0.0596 - val_loss: 0.0056 - 5s/epoch - 11ms/step\n",
      "Epoch 8/50\n",
      "492/492 - 5s - loss: 0.0480 - val_loss: 0.0044 - 5s/epoch - 11ms/step\n",
      "Epoch 9/50\n",
      "492/492 - 5s - loss: 0.0374 - val_loss: 0.0037 - 5s/epoch - 11ms/step\n",
      "Epoch 10/50\n",
      "492/492 - 5s - loss: 0.0294 - val_loss: 0.0033 - 5s/epoch - 11ms/step\n",
      "Epoch 11/50\n",
      "492/492 - 5s - loss: 0.0227 - val_loss: 0.0030 - 5s/epoch - 11ms/step\n",
      "Epoch 12/50\n",
      "492/492 - 5s - loss: 0.0178 - val_loss: 0.0027 - 5s/epoch - 11ms/step\n",
      "Epoch 13/50\n",
      "492/492 - 5s - loss: 0.0133 - val_loss: 0.0027 - 5s/epoch - 11ms/step\n",
      "Epoch 14/50\n",
      "492/492 - 5s - loss: 0.0098 - val_loss: 0.0026 - 5s/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "492/492 - 6s - loss: 0.0075 - val_loss: 0.0025 - 6s/epoch - 11ms/step\n",
      "Epoch 16/50\n",
      "492/492 - 6s - loss: 0.0056 - val_loss: 0.0025 - 6s/epoch - 12ms/step\n",
      "Epoch 17/50\n",
      "492/492 - 5s - loss: 0.0042 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 18/50\n",
      "492/492 - 5s - loss: 0.0034 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 19/50\n",
      "492/492 - 5s - loss: 0.0028 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 20/50\n",
      "492/492 - 5s - loss: 0.0025 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 21/50\n",
      "492/492 - 5s - loss: 0.0023 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 22/50\n",
      "492/492 - 5s - loss: 0.0023 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 23/50\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 24/50\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 25/50\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 26/50\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 28/50\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 29/50\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 30/50\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "Restoring model weights from the end of the best epoch: 21.\n",
      "492/492 - 5s - loss: 0.0022 - val_loss: 0.0025 - 5s/epoch - 11ms/step\n",
      "Epoch 31: early stopping\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20220101/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20220101/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:  62%|██████▎   | 5/8 [22:11<13:10, 263.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20160701, train_end: 20220630, val_start: 20220701, val_end: 20221231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:02<00:00, 23.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (63834, 199)\n",
      "validation_data_size: (5909, 199)\n",
      "Epoch 1/50\n",
      "499/499 - 12s - loss: 0.8581 - val_loss: 0.2136 - 12s/epoch - 24ms/step\n",
      "Epoch 2/50\n",
      "499/499 - 6s - loss: 0.3462 - val_loss: 0.1097 - 6s/epoch - 11ms/step\n",
      "Epoch 3/50\n",
      "499/499 - 6s - loss: 0.2199 - val_loss: 0.0546 - 6s/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "499/499 - 6s - loss: 0.1573 - val_loss: 0.0302 - 6s/epoch - 11ms/step\n",
      "Epoch 5/50\n",
      "499/499 - 6s - loss: 0.1203 - val_loss: 0.0183 - 6s/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "499/499 - 5s - loss: 0.0932 - val_loss: 0.0119 - 5s/epoch - 11ms/step\n",
      "Epoch 7/50\n",
      "499/499 - 6s - loss: 0.0728 - val_loss: 0.0079 - 6s/epoch - 11ms/step\n",
      "Epoch 8/50\n",
      "499/499 - 6s - loss: 0.0574 - val_loss: 0.0057 - 6s/epoch - 11ms/step\n",
      "Epoch 9/50\n",
      "499/499 - 6s - loss: 0.0442 - val_loss: 0.0043 - 6s/epoch - 11ms/step\n",
      "Epoch 10/50\n",
      "499/499 - 6s - loss: 0.0338 - val_loss: 0.0033 - 6s/epoch - 11ms/step\n",
      "Epoch 11/50\n",
      "499/499 - 6s - loss: 0.0260 - val_loss: 0.0028 - 6s/epoch - 11ms/step\n",
      "Epoch 12/50\n",
      "499/499 - 6s - loss: 0.0195 - val_loss: 0.0024 - 6s/epoch - 11ms/step\n",
      "Epoch 13/50\n",
      "499/499 - 6s - loss: 0.0142 - val_loss: 0.0022 - 6s/epoch - 12ms/step\n",
      "Epoch 14/50\n",
      "499/499 - 6s - loss: 0.0105 - val_loss: 0.0020 - 6s/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "499/499 - 6s - loss: 0.0075 - val_loss: 0.0019 - 6s/epoch - 11ms/step\n",
      "Epoch 16/50\n",
      "499/499 - 6s - loss: 0.0055 - val_loss: 0.0019 - 6s/epoch - 11ms/step\n",
      "Epoch 17/50\n",
      "499/499 - 6s - loss: 0.0041 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 18/50\n",
      "499/499 - 6s - loss: 0.0032 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 19/50\n",
      "499/499 - 6s - loss: 0.0027 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 20/50\n",
      "499/499 - 6s - loss: 0.0024 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 21/50\n",
      "499/499 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 24/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 25/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 26/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 28/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 29/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 30/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 32/50\n",
      "499/499 - 6s - loss: 0.0022 - val_loss: 0.0019 - 6s/epoch - 11ms/step\n",
      "Epoch 33/50\n",
      "Restoring model weights from the end of the best epoch: 23.\n",
      "499/499 - 6s - loss: 0.0021 - val_loss: 0.0019 - 6s/epoch - 12ms/step\n",
      "Epoch 33: early stopping\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20220701/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20220701/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:  75%|███████▌  | 6/8 [26:46<08:54, 267.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20170101, train_end: 20221231, val_start: 20230101, val_end: 20230630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:02<00:00, 20.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (64971, 199)\n",
      "validation_data_size: (5649, 199)\n",
      "Epoch 1/50\n",
      "508/508 - 14s - loss: 0.8346 - val_loss: 0.1498 - 14s/epoch - 28ms/step\n",
      "Epoch 2/50\n",
      "508/508 - 7s - loss: 0.3116 - val_loss: 0.0946 - 7s/epoch - 14ms/step\n",
      "Epoch 3/50\n",
      "508/508 - 6s - loss: 0.1976 - val_loss: 0.0458 - 6s/epoch - 12ms/step\n",
      "Epoch 4/50\n",
      "508/508 - 6s - loss: 0.1416 - val_loss: 0.0251 - 6s/epoch - 11ms/step\n",
      "Epoch 5/50\n",
      "508/508 - 6s - loss: 0.1089 - val_loss: 0.0155 - 6s/epoch - 12ms/step\n",
      "Epoch 6/50\n",
      "508/508 - 6s - loss: 0.0858 - val_loss: 0.0103 - 6s/epoch - 11ms/step\n",
      "Epoch 7/50\n",
      "508/508 - 6s - loss: 0.0676 - val_loss: 0.0074 - 6s/epoch - 11ms/step\n",
      "Epoch 8/50\n",
      "508/508 - 6s - loss: 0.0527 - val_loss: 0.0055 - 6s/epoch - 11ms/step\n",
      "Epoch 9/50\n",
      "508/508 - 6s - loss: 0.0417 - val_loss: 0.0043 - 6s/epoch - 11ms/step\n",
      "Epoch 10/50\n",
      "508/508 - 6s - loss: 0.0323 - val_loss: 0.0035 - 6s/epoch - 11ms/step\n",
      "Epoch 11/50\n",
      "508/508 - 6s - loss: 0.0242 - val_loss: 0.0029 - 6s/epoch - 11ms/step\n",
      "Epoch 12/50\n",
      "508/508 - 6s - loss: 0.0180 - val_loss: 0.0025 - 6s/epoch - 11ms/step\n",
      "Epoch 13/50\n",
      "508/508 - 6s - loss: 0.0134 - val_loss: 0.0022 - 6s/epoch - 11ms/step\n",
      "Epoch 14/50\n",
      "508/508 - 6s - loss: 0.0099 - val_loss: 0.0021 - 6s/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "508/508 - 6s - loss: 0.0072 - val_loss: 0.0020 - 6s/epoch - 11ms/step\n",
      "Epoch 16/50\n",
      "508/508 - 6s - loss: 0.0052 - val_loss: 0.0019 - 6s/epoch - 11ms/step\n",
      "Epoch 17/50\n",
      "508/508 - 6s - loss: 0.0040 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 18/50\n",
      "508/508 - 6s - loss: 0.0032 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 19/50\n",
      "508/508 - 6s - loss: 0.0027 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 20/50\n",
      "508/508 - 6s - loss: 0.0024 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 21/50\n",
      "508/508 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 22/50\n",
      "508/508 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 23/50\n",
      "508/508 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 24/50\n",
      "508/508 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 25/50\n",
      "508/508 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 26/50\n",
      "508/508 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "508/508 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 28/50\n",
      "508/508 - 6s - loss: 0.0023 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 29/50\n",
      "508/508 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 12ms/step\n",
      "Epoch 30/50\n",
      "508/508 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 31/50\n",
      "508/508 - 6s - loss: 0.0022 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 32/50\n",
      "508/508 - 6s - loss: 0.0022 - val_loss: 0.0019 - 6s/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "508/508 - 6s - loss: 0.0021 - val_loss: 0.0019 - 6s/epoch - 11ms/step\n",
      "Epoch 34/50\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "508/508 - 6s - loss: 0.0020 - val_loss: 0.0020 - 6s/epoch - 11ms/step\n",
      "Epoch 34: early stopping\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20230101/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20230101/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...:  88%|████████▊ | 7/8 [31:36<04:34, 274.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_start: 20170701, train_end: 20230630, val_start: 20230701, val_end: 20231231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:02<00:00, 22.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (66185, 199)\n",
      "validation_data_size: (5950, 199)\n",
      "Epoch 1/50\n",
      "518/518 - 12s - loss: 0.8566 - val_loss: 0.1103 - 12s/epoch - 23ms/step\n",
      "Epoch 2/50\n",
      "518/518 - 6s - loss: 0.3121 - val_loss: 0.0882 - 6s/epoch - 11ms/step\n",
      "Epoch 3/50\n",
      "518/518 - 6s - loss: 0.2020 - val_loss: 0.0435 - 6s/epoch - 11ms/step\n",
      "Epoch 4/50\n",
      "518/518 - 6s - loss: 0.1456 - val_loss: 0.0238 - 6s/epoch - 11ms/step\n",
      "Epoch 5/50\n",
      "518/518 - 6s - loss: 0.1120 - val_loss: 0.0137 - 6s/epoch - 11ms/step\n",
      "Epoch 6/50\n",
      "518/518 - 6s - loss: 0.0868 - val_loss: 0.0085 - 6s/epoch - 11ms/step\n",
      "Epoch 7/50\n",
      "518/518 - 6s - loss: 0.0676 - val_loss: 0.0056 - 6s/epoch - 12ms/step\n",
      "Epoch 8/50\n",
      "518/518 - 6s - loss: 0.0543 - val_loss: 0.0038 - 6s/epoch - 11ms/step\n",
      "Epoch 9/50\n",
      "518/518 - 6s - loss: 0.0409 - val_loss: 0.0028 - 6s/epoch - 12ms/step\n",
      "Epoch 10/50\n",
      "518/518 - 6s - loss: 0.0320 - val_loss: 0.0022 - 6s/epoch - 11ms/step\n",
      "Epoch 11/50\n",
      "518/518 - 6s - loss: 0.0244 - val_loss: 0.0018 - 6s/epoch - 11ms/step\n",
      "Epoch 12/50\n",
      "518/518 - 6s - loss: 0.0184 - val_loss: 0.0016 - 6s/epoch - 11ms/step\n",
      "Epoch 13/50\n",
      "518/518 - 6s - loss: 0.0134 - val_loss: 0.0014 - 6s/epoch - 11ms/step\n",
      "Epoch 14/50\n",
      "518/518 - 6s - loss: 0.0098 - val_loss: 0.0013 - 6s/epoch - 11ms/step\n",
      "Epoch 15/50\n",
      "518/518 - 6s - loss: 0.0072 - val_loss: 0.0013 - 6s/epoch - 11ms/step\n",
      "Epoch 16/50\n",
      "518/518 - 6s - loss: 0.0053 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 17/50\n",
      "518/518 - 6s - loss: 0.0040 - val_loss: 0.0012 - 6s/epoch - 12ms/step\n",
      "Epoch 18/50\n",
      "518/518 - 6s - loss: 0.0032 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 19/50\n",
      "518/518 - 6s - loss: 0.0027 - val_loss: 0.0012 - 6s/epoch - 12ms/step\n",
      "Epoch 20/50\n",
      "518/518 - 6s - loss: 0.0025 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 21/50\n",
      "518/518 - 6s - loss: 0.0024 - val_loss: 0.0012 - 6s/epoch - 12ms/step\n",
      "Epoch 22/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 12ms/step\n",
      "Epoch 23/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 24/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 25/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 26/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 27/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 28/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 29/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 30/50\n",
      "518/518 - 6s - loss: 0.0023 - val_loss: 0.0012 - 6s/epoch - 12ms/step\n",
      "Epoch 31/50\n",
      "518/518 - 6s - loss: 0.0022 - val_loss: 0.0012 - 6s/epoch - 11ms/step\n",
      "Epoch 32/50\n",
      "518/518 - 6s - loss: 0.0022 - val_loss: 0.0012 - 6s/epoch - 12ms/step\n",
      "Epoch 33/50\n",
      "518/518 - 6s - loss: 0.0021 - val_loss: 0.0013 - 6s/epoch - 11ms/step\n",
      "Epoch 34/50\n",
      "518/518 - 6s - loss: 0.0021 - val_loss: 0.0013 - 6s/epoch - 11ms/step\n",
      "Epoch 35/50\n",
      "518/518 - 6s - loss: 0.0020 - val_loss: 0.0013 - 6s/epoch - 11ms/step\n",
      "Epoch 36/50\n",
      "518/518 - 6s - loss: 0.0019 - val_loss: 0.0014 - 6s/epoch - 11ms/step\n",
      "Epoch 37/50\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "518/518 - 6s - loss: 0.0018 - val_loss: 0.0014 - 6s/epoch - 12ms/step\n",
      "Epoch 37: early stopping\n",
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20230701/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/saved_model/model_of_20230701/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 1s 7ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling Training...: 100%|██████████| 8/8 [36:41<00:00, 275.22s/it]\n"
     ]
    }
   ],
   "source": [
    "feature_config = {\n",
    "    \"target_feature_name\": [\"future_return\"],\n",
    "    \"numeric_features\": ['KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE0', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME0', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60'],\n",
    "    \"integer_categorical_features\": ['weekday', 'day_of_month', 'month'],\n",
    "    \"string_categorical_features\": ['day_of_week', 'season']\n",
    "}\n",
    "full_feature_names = feature_config.get('numeric_features', []) + feature_config.get('integer_categorical_features', []) + feature_config.get('string_categorical_features', [])\n",
    "stock_pool = '000016' # 上证50所有股票作为训练数据\n",
    "batch_size = 128\n",
    "\n",
    "for date_period in tqdm(rolling_period, desc='Rolling Training...'):\n",
    "    train_start_date, train_end_date, val_start_date, val_end_date = date_period\n",
    "    print(f\"train_start: {train_start_date}, train_end: {train_end_date}, val_start: {val_start_date}, val_end: {val_end_date}\")\n",
    "    # 1. 获取所有股票信息\n",
    "    df = proprocessor._process_all_stock(code_type=stock_pool, start_date=train_start_date, end_date=val_end_date)\n",
    "    # 2. 拆分训练数据&验证数据\n",
    "    train_data, val_data = extract_train_val_data(df, train_start_date, train_end_date, val_start_date, val_end_date)\n",
    "    # 3. 构建训练集和验证集\n",
    "    train_ds = df_to_dataset(train_data, full_feature_names, feature_config.get('target_feature_name', []), shuffle=True, batch_size=batch_size)\n",
    "    val_ds = df_to_dataset(val_data, full_feature_names, feature_config.get('target_feature_name', []), shuffle=False, batch_size=batch_size)\n",
    "    # 4. 配置模型相关参数\n",
    "    model_config = {\n",
    "        \"seed\": 1024,\n",
    "        \"reduction_ratio\": 3,\n",
    "        \"dnn_hidden_units\": [64,32],\n",
    "        \"dnn_activation\": 'relu',\n",
    "        \"dnn_dropout\": 0.1,\n",
    "        \"dnn_use_bn\": True,\n",
    "        \"numeric_features_with_boundaries\": {k: list(get_numeric_boundaries(train_data[k])) for k in feature_config.get('numeric_features', [])},\n",
    "        \"integer_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('integer_categorical_features', [])},\n",
    "        \"string_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('string_categorical_features', [])},\n",
    "        \"feature_embedding_dims\": 6,\n",
    "        \"task_type\": ['reg'],\n",
    "    }\n",
    "    # 5. 初始化模型\n",
    "    model = QuantModel(model_config)\n",
    "    # 6. 配置训练相关参数\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        verbose=2,\n",
    "        patience=10,\n",
    "        mode='min',\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    # 7. 配置Tensorboard记录功能\n",
    "    log_dir = \"./logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # 8. 配置optimizer\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4) # for Mac M1/M2\n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4) # for intel\n",
    "    loss = [tf.keras.losses.MeanSquaredError()]\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    model.fit(\n",
    "            train_ds, \n",
    "            validation_data=val_ds, \n",
    "            epochs=50,\n",
    "            verbose=2,\n",
    "            callbacks=[tensorboard_callback, early_stopping]\n",
    "    )\n",
    "    # 9. 配置保存模型功能\n",
    "    # model_save_path = f'./models/saved_model/model_of_{val_start_date}'\n",
    "    # model.save(model_save_path)\n",
    "    # best_model = tf.keras.models.load_model('./best_model')\n",
    "\n",
    "    # 10. 记录预测集合\n",
    "    model_red_result = model.predict(val_ds)\n",
    "    output_df = val_data[['stock_code', 'stock_name', 'datetime']]\n",
    "    output_df['future_return'] = val_data['future_return']\n",
    "    output_df['future_return_pred'] = model_red_result[0]\n",
    "    output_file_path = f'../../Offline/backtest/backtest_data/stock_selection_results_{val_start_date}.pkl'\n",
    "    output_df.to_pickle(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ./logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
