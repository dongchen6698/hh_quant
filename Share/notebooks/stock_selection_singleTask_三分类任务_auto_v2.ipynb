{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "import baostock as bs\n",
    "import json\n",
    "bs.login()\n",
    "\n",
    "from database_auto.db_factor_prebuilder.utils.expression_excutor import AlphaExpressionExcutor\n",
    "from database_auto.db_data_downloader.downloader_base import DownloaderBase\n",
    "import database_auto.database_config as db_config\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 03:43:25.538140: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-20 03:43:26.414906: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置全局初始化信息...避免GPU并行差异\n",
      "Tensorflow Version: 2.13.1\n",
      "TensorFlow GPU version is installed\n",
      "GPU devices available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def set_global_determinism(seed=1024):\n",
    "    print(\"配置全局初始化信息...避免GPU并行差异\")\n",
    "    # 配置随机种子\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 配置GPU并行参数\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1' \n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1' \n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "set_global_determinism()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 只显示error日志\n",
    "\n",
    "# 只使用CPU进行训练\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# 打印Tensorflow版本\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "\n",
    "# 检查是否有可用的GPU设备\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"TensorFlow GPU version is installed\")\n",
    "else:\n",
    "    print(\"TensorFlow CPU version is installed\")\n",
    "\n",
    "# 检查TensorFlow是否能够访问GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU devices available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU devices found. Running on CPU.\")\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# 绘图相关函数\n",
    "def plot_series_dist(series):\n",
    "    data = series\n",
    "    plt.figure(figsize=(5,5))\n",
    "    # 使用matplotlib画直方图\n",
    "    plt.hist(data, bins=60, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Data')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'mean_squared_error']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.ylim([0, plt.ylim()[1]])\n",
    "    plt.legend()\n",
    "\n",
    "def plot_cm(true_labels, pred_labels):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"g\", cmap='Blues')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    recall = recall_score(true_labels, pred_labels)\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "class QuantileClipTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_quantile=0.01, upper_quantile=0.99):\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # 计算给定分位数的分界值\n",
    "        self.lower_bound_ = np.nanquantile(X, self.lower_quantile, axis=0)\n",
    "        self.upper_bound_ = np.nanquantile(X, self.upper_quantile, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # 对整个数组应用剪辑操作\n",
    "        return np.clip(X, self.lower_bound_, self.upper_bound_)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alpha_factor_sync(stock_history, alpha_factor_path, exp_excutor):\n",
    "    stock_history['vwap'] = stock_history[['open', 'high', 'low', 'close']].mean(axis=1)\n",
    "    stock_history['returns'] = stock_history['close'].pct_change()\n",
    "    alpha_factor_dict = json.loads(open(alpha_factor_path, \"r\").read())\n",
    "    dataframe = stock_history[[\"code\", \"datetime\"]]\n",
    "    for alpha_name, alpha_expression in tqdm(alpha_factor_dict.items(), desc='Alpha...'):\n",
    "        try:\n",
    "            dataframe[alpha_name] = exp_excutor.excute(stock_history, alpha_expression)\n",
    "        except Exception as e:\n",
    "            dataframe[alpha_name] = np.NaN\n",
    "    return dataframe\n",
    "\n",
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=False, batch_size=32):\n",
    "    features = dataframe[feature_cols]\n",
    "    if label_cols is not None:\n",
    "        labels = dataframe[label_cols]\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(features), tf.expand_dims(labels, axis=-1)))\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(features)))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(features), 10000), seed=1024)\n",
    "    ds = ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化数据库\n",
    "db_conn = sqlite3.connect('../database_auto/hh_quant_auto.db')\n",
    "db_downloader = DownloaderBase(db_conn, db_config)\n",
    "exp_excutor = AlphaExpressionExcutor()\n",
    "\n",
    "# 初始化基础信息\n",
    "benchmark = '000016'  # 对比指数 = 上证50\n",
    "train_val_test_period = {\n",
    "    'train': ['2009-01-01', '2016-12-31'],\n",
    "    'val': ['2017-01-01', '2018-12-31'],\n",
    "    'test': ['2019-01-01', '2024-12-31']\n",
    "}\n",
    "\n",
    "train_start_date, train_end_date = train_val_test_period['train']\n",
    "val_start_date, val_end_date = train_val_test_period['val']\n",
    "test_start_date, test_end_date = train_val_test_period['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取benchmark对应的成分股\n",
    "# stock_pool = bs.query_sz50_stocks().get_data()['code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 预计算基础数据\n",
    "# stock_df_list = []\n",
    "# for stock_code in tqdm(stock_pool, desc='LoadingStockData'):\n",
    "#     # 1. 获取基础信息\n",
    "#     stock_history = db_downloader._download_history_base_info(stock_code, train_start_date, test_end_date)\n",
    "#     stock_profile = db_downloader._download_all_stock_info(stock_code)\n",
    "#     stock_indicator = db_downloader._download_history_indicator_info(stock_code, train_start_date, test_end_date)\n",
    "#     stock_indicator = stock_indicator.replace(\"\", np.NaN).ffill()\n",
    "#     stock_date_factor = db_downloader._download_history_date_factor_info(train_start_date, test_end_date)\n",
    "#     stock_alpha_184_factor = build_alpha_factor_sync(\n",
    "#         stock_history, \n",
    "#         '../database_auto/db_factor_prebuilder/factor_lib/alpha_184.json',\n",
    "#         exp_excutor)\n",
    "#     stock_alpha_101_factor = build_alpha_factor_sync(\n",
    "#         stock_history, \n",
    "#         '../database_auto/db_factor_prebuilder/factor_lib/alpha_101.json',\n",
    "#         exp_excutor)\n",
    "#     stock_alpha_191_factor = build_alpha_factor_sync(\n",
    "#         stock_history, \n",
    "#         '../database_auto/db_factor_prebuilder/factor_lib/alpha_191.json',\n",
    "#         exp_excutor)\n",
    "#     stock_df = stock_history.merge(stock_profile, on=['code']) \\\n",
    "#             .merge(stock_indicator, on=['code', 'datetime']) \\\n",
    "#             .merge(stock_date_factor, on=['datetime']) \\\n",
    "#             .merge(stock_alpha_184_factor, on=['code', 'datetime']) \\\n",
    "#             .merge(stock_alpha_101_factor, on=['code', 'datetime']) \\\n",
    "#             .merge(stock_alpha_191_factor, on=['code', 'datetime'])\n",
    "#     stock_df_list.append(stock_df)\n",
    "\n",
    "# # 保存原始数据\n",
    "# whole_stock_df = pd.concat(stock_df_list)\n",
    "# whole_stock_df.to_pickle(f'{benchmark}_{train_start_date}_{test_end_date}_base.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 预计算label数据\n",
    "# def _build_reg_label(stock_dataframe, N=15):\n",
    "#     stock_df = stock_dataframe.copy()\n",
    "#     # 计算未来N日内的最高收益率\n",
    "#     stock_df['max_return'] = stock_df['close'].rolling(window=N).max().shift(-N) / stock_df['close'] - 1\n",
    "#     # 计算未来N日内的最低收益率\n",
    "#     stock_df['min_return'] = stock_df['close'].rolling(window=N).min().shift(-N) / stock_df['close'] - 1\n",
    "#     # 计算未来N日内的收益率和（期望最高收益率越高越好，最低收益率也越高越好，由于最低收益率是负数，因此使用最高+最低来作为综合收益指标）\n",
    "#     stock_df['label'] = stock_df['max_return'] + stock_df['min_return']\n",
    "#     # 过滤第二天一字涨停股票\n",
    "#     stock_df = stock_df[stock_df['high'].shift(-1) != stock_df['low'].shift(-1)]\n",
    "#     return stock_df[['code', 'datetime', 'label']]\n",
    "\n",
    "# stock_label_list = []\n",
    "# for stock_code in tqdm(stock_pool, desc='LoadingStockData'):\n",
    "#     # 1. 获取基础信息\n",
    "#     stock_history = db_downloader._download_history_base_info(stock_code, train_start_date, test_end_date)\n",
    "#     stock_label = _build_reg_label(stock_history)\n",
    "#     stock_label_list.append(stock_label)\n",
    "\n",
    "# # 保存标签数据\n",
    "# whole_label_df = pd.concat(stock_label_list)\n",
    "# whole_label_df.to_pickle(f'{benchmark}_{train_start_date}_{test_end_date}_label.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_alpha_184_columns = [f\"alpha_184_{i+1}\" for i in range(184)]\n",
    "feature_config = {\n",
    "    \"numeric_features\": ['turnover_rate', 'pe_ttm', 'ps_ttm', 'pcf_ncf_ttm', 'pb_mrq'] + factor_alpha_184_columns,\n",
    "    \"integer_categorical_features\": ['month'],\n",
    "    \"string_categorical_features\": ['industry', 'season'],\n",
    "}\n",
    "full_feature_columns = ['code', 'code_name', 'datetime', 'high', 'low', 'close'] + [i for feas in feature_config.values() for i in feas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行基本的数据清理\n",
    "def base_dataframe_cleaner(dataframe):\n",
    "    stock_df = dataframe.copy()\n",
    "    stock_df = stock_df.replace(\"\", np.NaN).ffill()\n",
    "    stock_df = stock_df.replace([np.inf, -np.inf], np.nan)\n",
    "    stock_df = stock_df.dropna(axis=1, how='all') # 过滤特征列全为空的column\n",
    "    stock_df = stock_df.dropna() # 剔除包含空值的行\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170869, 199)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从本地加载Base数据\n",
    "stock_base_df_raw = pd.read_pickle('./000016_2006-01-01_2024-12-31_base.pickle')\n",
    "stock_base_df = base_dataframe_cleaner(stock_base_df_raw[full_feature_columns])\n",
    "# 从本地加载Label数据\n",
    "stock_label_df = pd.read_pickle('./000016_2006-01-01_2024-12-31_label.pickle')\n",
    "stock_merge_df = stock_base_df.merge(stock_label_df, on=['code', 'datetime'])\n",
    "stock_merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_uptrend_label(group, rate):\n",
    "    group['uptrend'] = 0  # 初始化新列为0\n",
    "    # 计算 True Range（TR）\n",
    "    group['h-l'] = group['high'] - group['low']\n",
    "    group['h-yc'] = abs(group['high'] - group['close'].shift(1))\n",
    "    group['l-yc'] = abs(group['low'] - group['close'].shift(1))\n",
    "    group['TR'] = group[['h-l', 'h-yc', 'l-yc']].max(axis=1)\n",
    "    # 设定 ATR 的计算周期\n",
    "    n = 14\n",
    "    # 计算 ATR\n",
    "    group['ATR'] = group['TR'].rolling(window=n, min_periods=1).mean()\n",
    "\n",
    "    group['high_loss'] = group['close']\n",
    "    group['low_loss'] = group['close']\n",
    "    for i in range(len(group)):\n",
    "        price1 = group.at[group.index[i], 'close']\n",
    "        atr = group.at[group.index[i], 'ATR']\n",
    "        low = price1 - rate * atr\n",
    "        high = price1 + rate * atr\n",
    "        future_prices = group['close'].iloc[i+1:i+15] # 获取当前日期之后14天的价格\n",
    "        group.at[group.index[i], 'high_loss'] = high\n",
    "        group.at[group.index[i], 'low_loss'] = low\n",
    "        if len(future_prices) > 0:  # 确保14天内有数据\n",
    "            if (future_prices < low).any():  # 如果14天内存在价格小于止损价\n",
    "                a = future_prices[future_prices < low].index[0]  # 找到第一个价格小于止损价的日期\n",
    "                if (group['close'].loc[group.index[i]:a-1] < high).all():  # 检查之前到当天的价格是否都小于最小止盈价\n",
    "                    group.at[group.index[i], 'uptrend'] = 1  # 满足条件时设置为1\n",
    "            if (future_prices > high).any():  # 如果14天内存在价格小于止损价\n",
    "                a = future_prices[future_prices > high].index[0]  # 找到第一个价格小于止损价的日期\n",
    "                if (group['close'].loc[group.index[i]:a-1] > low).all():  # 检查之前到当天的价格是否都小于最小止盈价\n",
    "                    group.at[group.index[i], 'uptrend'] = 2  # 满足条件时设置为2\n",
    "    return group\n",
    "stock_merge_label_df = stock_merge_df.sort_values(by='datetime').groupby('code', group_keys=False).apply(calculate_uptrend_label, rate=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_merge_label_df.groupby('uptrend').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始拆分训练、验证、测试集合...\n",
      "train_data_size: (71297, 207)\n",
      "val_data_size: (19998, 207)\n",
      "test_data_size: (61755, 207)\n"
     ]
    }
   ],
   "source": [
    "print(\"开始拆分训练、验证、测试集合...\")\n",
    "def split_data_by_date(dataframe, start_date, end_date):\n",
    "    return dataframe[(dataframe['datetime'] >= start_date) & (dataframe['datetime'] <= end_date)]\n",
    "\n",
    "train_data = split_data_by_date(stock_merge_label_df, train_start_date, train_end_date)\n",
    "val_data = split_data_by_date(stock_merge_label_df, val_start_date, val_end_date)\n",
    "test_data = split_data_by_date(stock_merge_label_df, test_start_date, test_end_date)\n",
    "\n",
    "print(f\"train_data_size: {train_data.shape}\")\n",
    "print(f\"val_data_size: {val_data.shape}\")\n",
    "print(f\"test_data_size: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始对特征进行预处理...\n"
     ]
    }
   ],
   "source": [
    "print(\"开始对特征进行预处理...\")\n",
    "feature_preprocess_pipeline = Pipeline(steps=[\n",
    "    ('robust_scaler', RobustScaler()),\n",
    "    ('minmax_scaler', MinMaxScaler()),\n",
    "])\n",
    "preprocess_feature_columns = feature_config.get('numeric_features', [])\n",
    "train_data[preprocess_feature_columns] = feature_preprocess_pipeline.fit_transform(train_data[preprocess_feature_columns])\n",
    "val_data[preprocess_feature_columns] = feature_preprocess_pipeline.transform(val_data[preprocess_feature_columns])\n",
    "test_data[preprocess_feature_columns] = feature_preprocess_pipeline.transform(test_data[preprocess_feature_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始将DataFrame转换为DataSet...\n"
     ]
    }
   ],
   "source": [
    "# 转换为tensorflow所使用的dataset\n",
    "print(\"开始将DataFrame转换为DataSet...\")\n",
    "batch_size = 1024\n",
    "feature_columns = feature_config.get('numeric_features', []) + feature_config.get('integer_categorical_features', []) + feature_config.get('string_categorical_features', [])\n",
    "label_columns = ['uptrend']\n",
    "# 将data转换为dataset\n",
    "train_ds = df_to_dataset(train_data, feature_columns, label_columns, shuffle=True, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val_data, feature_columns, label_columns, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test_data, feature_columns, None, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型初始化 & 训练...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-20 03:51:30.004135: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556a4042db50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-20 03:51:30.004198: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-06-20 03:51:30.009232: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-20 03:51:30.035300: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 59s 379ms/step - loss: 1.2993 - accuracy: 0.3648 - val_loss: 1.0829 - val_accuracy: 0.3758\n",
      "Epoch 2/500\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 1.2041 - accuracy: 0.3731 - val_loss: 1.0750 - val_accuracy: 0.3867\n",
      "Epoch 3/500\n",
      "70/70 [==============================] - 14s 199ms/step - loss: 1.1672 - accuracy: 0.3788 - val_loss: 1.0696 - val_accuracy: 0.4013\n",
      "Epoch 4/500\n",
      "70/70 [==============================] - 14s 199ms/step - loss: 1.1383 - accuracy: 0.3824 - val_loss: 1.0655 - val_accuracy: 0.4087\n",
      "Epoch 5/500\n",
      "70/70 [==============================] - 14s 198ms/step - loss: 1.1200 - accuracy: 0.3870 - val_loss: 1.0630 - val_accuracy: 0.4139\n",
      "Epoch 6/500\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 1.1038 - accuracy: 0.3928 - val_loss: 1.0610 - val_accuracy: 0.4198\n",
      "Epoch 7/500\n",
      "70/70 [==============================] - 14s 204ms/step - loss: 1.0972 - accuracy: 0.3925 - val_loss: 1.0593 - val_accuracy: 0.4229\n",
      "Epoch 8/500\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 1.0887 - accuracy: 0.3975 - val_loss: 1.0581 - val_accuracy: 0.4260\n",
      "Epoch 9/500\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 1.0827 - accuracy: 0.3981 - val_loss: 1.0579 - val_accuracy: 0.4288\n",
      "Epoch 10/500\n",
      "70/70 [==============================] - 14s 197ms/step - loss: 1.0761 - accuracy: 0.4026 - val_loss: 1.0566 - val_accuracy: 0.4297\n",
      "Epoch 11/500\n",
      "70/70 [==============================] - 14s 196ms/step - loss: 1.0699 - accuracy: 0.4114 - val_loss: 1.0556 - val_accuracy: 0.4302\n",
      "Epoch 12/500\n",
      "70/70 [==============================] - 14s 196ms/step - loss: 1.0661 - accuracy: 0.4104 - val_loss: 1.0548 - val_accuracy: 0.4302\n",
      "Epoch 13/500\n",
      "70/70 [==============================] - 13s 191ms/step - loss: 1.0638 - accuracy: 0.4118 - val_loss: 1.0544 - val_accuracy: 0.4306\n",
      "Epoch 14/500\n",
      "70/70 [==============================] - 13s 189ms/step - loss: 1.0596 - accuracy: 0.4161 - val_loss: 1.0536 - val_accuracy: 0.4317\n",
      "Epoch 15/500\n",
      "70/70 [==============================] - 13s 191ms/step - loss: 1.0559 - accuracy: 0.4210 - val_loss: 1.0532 - val_accuracy: 0.4300\n",
      "Epoch 16/500\n",
      "70/70 [==============================] - 14s 195ms/step - loss: 1.0520 - accuracy: 0.4210 - val_loss: 1.0526 - val_accuracy: 0.4314\n",
      "Epoch 17/500\n",
      "70/70 [==============================] - 13s 190ms/step - loss: 1.0480 - accuracy: 0.4248 - val_loss: 1.0524 - val_accuracy: 0.4329\n",
      "Epoch 18/500\n",
      "70/70 [==============================] - 13s 186ms/step - loss: 1.0436 - accuracy: 0.4298 - val_loss: 1.0527 - val_accuracy: 0.4297\n",
      "Epoch 19/500\n",
      "70/70 [==============================] - 13s 189ms/step - loss: 1.0391 - accuracy: 0.4353 - val_loss: 1.0536 - val_accuracy: 0.4266\n",
      "Epoch 20/500\n",
      "70/70 [==============================] - 13s 190ms/step - loss: 1.0333 - accuracy: 0.4445 - val_loss: 1.0549 - val_accuracy: 0.4212\n",
      "Epoch 21/500\n",
      "70/70 [==============================] - 13s 187ms/step - loss: 1.0273 - accuracy: 0.4507 - val_loss: 1.0567 - val_accuracy: 0.4181\n",
      "Epoch 22/500\n",
      "70/70 [==============================] - 14s 194ms/step - loss: 1.0215 - accuracy: 0.4540 - val_loss: 1.0580 - val_accuracy: 0.4159\n",
      "Epoch 23/500\n",
      "70/70 [==============================] - 13s 192ms/step - loss: 1.0141 - accuracy: 0.4619 - val_loss: 1.0614 - val_accuracy: 0.4108\n",
      "Epoch 24/500\n",
      "70/70 [==============================] - 14s 202ms/step - loss: 1.0049 - accuracy: 0.4723 - val_loss: 1.0655 - val_accuracy: 0.4053\n",
      "Epoch 25/500\n",
      "70/70 [==============================] - 14s 201ms/step - loss: 0.9943 - accuracy: 0.4812 - val_loss: 1.0714 - val_accuracy: 0.4028\n",
      "Epoch 26/500\n",
      "70/70 [==============================] - 14s 196ms/step - loss: 0.9824 - accuracy: 0.4931 - val_loss: 1.0757 - val_accuracy: 0.3992\n",
      "Epoch 27/500\n",
      "70/70 [==============================] - ETA: 0s - loss: 0.9697 - accuracy: 0.5045Restoring model weights from the end of the best epoch: 17.\n",
      "70/70 [==============================] - 14s 200ms/step - loss: 0.9697 - accuracy: 0.5045 - val_loss: 1.0843 - val_accuracy: 0.3943\n",
      "Epoch 27: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 准备模型训练\n",
    "print(\"开始模型初始化 & 训练...\")\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from models.single_task.model_moe_multi_classification import QuantModel\n",
    "\n",
    "model_config = {\n",
    "        \"seed\": 1024,\n",
    "        \"feature_use_embedding\": True,\n",
    "        \"feature_embedding_dims\": 4,\n",
    "        \"output_dim\":3,\n",
    "        \"numeric_features_with_boundaries\": {k: pd.qcut(train_data[k], q=20, retbins=True, duplicates='drop')[1].tolist() for k in feature_config.get('numeric_features', [])},\n",
    "        \"integer_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('integer_categorical_features', [])},\n",
    "        \"string_categorical_features_with_vocab\": {k: list(train_data[k].unique()) for k in feature_config.get('string_categorical_features', [])},\n",
    "    }\n",
    "model = QuantModel(config=model_config)\n",
    "\n",
    "initial_learning_rate = 5e-4\n",
    "# t = 0.45\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(initial_learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='min',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "baseline_history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始保存回测预测结果...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61/61 [==============================] - 8s 112ms/step\n"
     ]
    }
   ],
   "source": [
    "print(\"开始保存回测预测结果...\")\n",
    "model_pred_result = model.predict(test_ds)\n",
    "output_df = test_data[['code', 'code_name', 'datetime']]\n",
    "output_df['label_pred_0'] = model_pred_result[:,0]\n",
    "output_df['label_pred_1'] = model_pred_result[:,1]\n",
    "output_df['label_pred_2'] = model_pred_result[:,2]\n",
    "output_df = output_df.rename(columns={\n",
    "    'code': 'stock_code',\n",
    "    'code_name': 'stock_name'\n",
    "})\n",
    "output_df.to_pickle(f'../../Offline/backtest/backtest_data/test/{benchmark}_{test_start_date}_三分类任务_v6.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_code</th>\n",
       "      <th>stock_name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>label_pred_0</th>\n",
       "      <th>label_pred_1</th>\n",
       "      <th>label_pred_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130163</th>\n",
       "      <td>sh.601633</td>\n",
       "      <td>长城汽车</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.236453</td>\n",
       "      <td>0.392881</td>\n",
       "      <td>0.370666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158325</th>\n",
       "      <td>sh.603259</td>\n",
       "      <td>药明康德</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.286431</td>\n",
       "      <td>0.330983</td>\n",
       "      <td>0.382585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144809</th>\n",
       "      <td>sh.601888</td>\n",
       "      <td>中国中免</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.241540</td>\n",
       "      <td>0.355941</td>\n",
       "      <td>0.402519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162340</th>\n",
       "      <td>sh.603501</td>\n",
       "      <td>韦尔股份</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.252594</td>\n",
       "      <td>0.366358</td>\n",
       "      <td>0.381048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100853</th>\n",
       "      <td>sh.601166</td>\n",
       "      <td>兴业银行</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>0.192256</td>\n",
       "      <td>0.368884</td>\n",
       "      <td>0.438860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       stock_code stock_name    datetime  label_pred_0  label_pred_1  \\\n",
       "130163  sh.601633       长城汽车  2019-01-02      0.236453      0.392881   \n",
       "158325  sh.603259       药明康德  2019-01-02      0.286431      0.330983   \n",
       "144809  sh.601888       中国中免  2019-01-02      0.241540      0.355941   \n",
       "162340  sh.603501       韦尔股份  2019-01-02      0.252594      0.366358   \n",
       "100853  sh.601166       兴业银行  2019-01-02      0.192256      0.368884   \n",
       "\n",
       "        label_pred_2  \n",
       "130163      0.370666  \n",
       "158325      0.382585  \n",
       "144809      0.402519  \n",
       "162340      0.381048  \n",
       "100853      0.438860  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型、预处理等模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_json_file(data, path):\n",
    "#     import json\n",
    "#     with open(path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# def save_sklearn_pipeline(pipeline, path):\n",
    "#     import joblib\n",
    "#     joblib.dump(pipeline, path)\n",
    "\n",
    "# # 保存特征json\n",
    "# save_json_file(feature_config, '../../Online/enhance_sz50/feature_config.json')\n",
    "# # 保存特征预处理pipeline\n",
    "# save_sklearn_pipeline(feature_preprocess_pipeline, '../../Online/enhance_sz50/feature_preprocess_pipeline.joblib')\n",
    "# # 保存模型\n",
    "# model.save(f'../../Online/enhance_sz50/tf_models/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
