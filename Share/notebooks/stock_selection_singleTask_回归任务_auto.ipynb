{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import akshare as ak\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "from database_auto.db_data_downloader.downloader_base import DownloaderBase\n",
    "import database_auto.database_config as db_config\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None\n",
    "\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 2.13.1\n",
      "TensorFlow GPU version is installed\n",
      "GPU devices available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 只使用CPU进行训练\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# 打印Tensorflow版本\n",
    "print(f\"Tensorflow Version: {tf.__version__}\")\n",
    "\n",
    "# 检查是否有可用的GPU设备\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"TensorFlow GPU version is installed\")\n",
    "else:\n",
    "    print(\"TensorFlow CPU version is installed\")\n",
    "\n",
    "# 检查TensorFlow是否能够访问GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU devices available:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU devices found. Running on CPU.\")\n",
    "\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "# 绘图相关函数\n",
    "def plot_series_dist(series):\n",
    "    data = series\n",
    "    plt.figure(figsize=(5,5))\n",
    "    # 使用matplotlib画直方图\n",
    "    plt.hist(data, bins=60, edgecolor='k', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Data')\n",
    "    plt.show()\n",
    "\n",
    "def plot_metrics(history):\n",
    "  metrics = ['loss', 'mean_squared_error']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch, history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    plt.ylim([0, plt.ylim()[1]])\n",
    "    plt.legend()\n",
    "\n",
    "def plot_cm(true_labels, pred_labels):\n",
    "    cm = confusion_matrix(true_labels, pred_labels)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"g\", cmap='Blues')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels)\n",
    "    recall = recall_score(true_labels, pred_labels)\n",
    "    print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing:\n",
    "    def __init__(self, db_downloader:DownloaderBase) -> None:\n",
    "        self.db_downloader = db_downloader\n",
    "\n",
    "    def _build_reg_label(self, stock_dataframe, N=15):\n",
    "        stock_df = stock_dataframe.copy()\n",
    "        # 计算未来N日内的最高收益率\n",
    "        stock_df['max_return'] = stock_df['close'].rolling(window=N).max().shift(-N) / stock_df['close'] - 1\n",
    "        # 计算未来N日内的最低收益率\n",
    "        stock_df['min_return'] = stock_df['close'].rolling(window=N).min().shift(-N) / stock_df['close'] - 1\n",
    "        # 计算未来N日内的收益率和（期望最高收益率越高越好，最低收益率也越高越好，由于最低收益率是负数，因此使用最高+最低来作为综合收益指标）\n",
    "        stock_df['return_sum'] = stock_df['max_return'] + stock_df['min_return']\n",
    "        stock_df['label'] = stock_df['return_sum']\n",
    "        # 过滤第二天一字涨停股票\n",
    "        stock_df = stock_df[stock_df['high'].shift(-1) != stock_df['low'].shift(-1)]\n",
    "        return stock_df[['datetime', 'label']]\n",
    "\n",
    "    def _process_one_stock(self, stock_code, start_date, end_date):\n",
    "        stock_history = self.db_downloader._download_history_base_info(stock_code, start_date, end_date)\n",
    "        stock_profile = self.db_downloader._download_all_stock_info(stock_code)\n",
    "        stock_indicator = self.db_downloader._download_history_indicator_info(stock_code, start_date, end_date)\n",
    "        stock_indicator = stock_indicator.replace(\"\", np.NaN).ffill()\n",
    "        stock_factor_date = self.db_downloader._download_history_date_factor_info(start_date, end_date)\n",
    "        stock_factor_alpha184 = self.db_downloader._download_history_alpha184_factor_info_old_version(stock_code, start_date, end_date)\n",
    "        stock_label = self._build_reg_label(stock_history)\n",
    "\n",
    "        stock_df = stock_history.merge(stock_profile, on=['code']) \\\n",
    "                .merge(stock_indicator, on=['code', 'datetime']) \\\n",
    "                .merge(stock_factor_alpha184, on=['code', 'datetime']) \\\n",
    "                .merge(stock_factor_date, on=['datetime']) \\\n",
    "                .merge(stock_label, on=['datetime'])\n",
    "        stock_df = stock_df.dropna()\n",
    "        return stock_df\n",
    "\n",
    "    # def _get_index_hist_cons(self, benchmark, start_date, end_date):\n",
    "    #     import baostock as bs\n",
    "    #     bs.login()\n",
    "    #     index_stock_cons_set = set()\n",
    "    #     for cur_date in tqdm(pd.date_range(start=start_date, end=end_date, freq='B')):\n",
    "    #         cur_date = datetime.strftime(cur_date, '%Y-%m-%d')\n",
    "    #         if benchmark == '000016':\n",
    "    #             # print(\"开始处理上证50...\")\n",
    "    #             dataframe = bs.query_sz50_stocks(date=cur_date).get_data()\n",
    "    #         elif benchmark == '000300':\n",
    "    #             # print(\"开始处理沪深300...\")\n",
    "    #             dataframe = bs.query_hs300_stocks(date=cur_date).get_data()\n",
    "    #         elif benchmark == '000905':\n",
    "    #             # print(\"开始处理中证500...\")\n",
    "    #             dataframe = bs.query_zz500_stocks(date=cur_date).get_data()\n",
    "    #         if not dataframe.empty:\n",
    "    #             index_stock_cons_set.update(\n",
    "    #                 dataframe['code'].unique()\n",
    "    #             )\n",
    "    #     bs.logout()\n",
    "    #     return index_stock_cons_set\n",
    "    \n",
    "    def _get_index_latest_cons(self, benchmark):\n",
    "        import baostock as bs\n",
    "        bs.login()\n",
    "        if benchmark == '000016':\n",
    "            dataframe = bs.query_sz50_stocks().get_data()\n",
    "        elif benchmark == '000300':\n",
    "            dataframe = bs.query_hs300_stocks().get_data()\n",
    "        return dataframe['code'].unique()\n",
    "\n",
    "    def _process_all_stock(self, benchmark, start_date, end_date):\n",
    "        # 获取区间内benchmark的所有成份股\n",
    "        # stock_code_list = self._get_index_hist_cons(benchmark, start_date, end_date)\n",
    "        stock_code_list = self._get_index_latest_cons(benchmark)\n",
    "        stock_df_list = []\n",
    "        for stock_code in tqdm(stock_code_list, desc=f'Process: {benchmark} ...'):\n",
    "            stock_df = self._process_one_stock(stock_code, start_date, end_date)\n",
    "            if not stock_df.empty:\n",
    "                stock_df_list.append(stock_df)\n",
    "        return pd.concat(stock_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_data_period(backtest_start_date, backtest_duration=5, train_period=6, val_period=0.5, test_period=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        backtest_start_date (_type_): _description_\n",
    "        backtest_duration (int, optional): _description_. Defaults to 5.\n",
    "        train_period (int, optional): _description_. Defaults to 6.\n",
    "        val_period (float, optional): _description_. Defaults to 0.5.\n",
    "        test_period (float, optional): _description_. Defaults to 0.5.\n",
    "    Returns:\n",
    "        result: _description_\n",
    "    \"\"\"\n",
    "    backtest_start_date = datetime.strptime(backtest_start_date, '%Y%m%d')\n",
    "    backtest_end_date = backtest_start_date + relativedelta(years=backtest_duration) # 回测5年数据\n",
    "    train_period = relativedelta(years=train_period) # 使用6年的训练数据\n",
    "    val_period = relativedelta(months=(12 * val_period)) # 使用半年的验证数据\n",
    "    test_period = relativedelta(months=(12 * test_period)) # 使用半年的测试数据(半年模型一更新)\n",
    "\n",
    "    result = []\n",
    "    rolling_flag = True\n",
    "    bench_date = backtest_start_date\n",
    "    while rolling_flag:\n",
    "        if bench_date < backtest_end_date:\n",
    "            test_start, test_end = bench_date, (bench_date + test_period - relativedelta(days=1))\n",
    "            val_start, val_end = (test_start - relativedelta(days=1) - val_period), (test_start - relativedelta(days=1))\n",
    "            train_start, train_end =(val_start - relativedelta(days=1) - train_period), (val_start - relativedelta(days=1))\n",
    "            result.append({\n",
    "                \"train\": [train_start.strftime(\"%Y%m%d\"), train_end.strftime(\"%Y%m%d\")],\n",
    "                \"val\": [val_start.strftime(\"%Y%m%d\"), val_end.strftime(\"%Y%m%d\")],\n",
    "                \"test\": [test_start.strftime(\"%Y%m%d\"), test_end.strftime(\"%Y%m%d\")]\n",
    "            })\n",
    "            bench_date += test_period\n",
    "        else:\n",
    "            rolling_flag = False \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_train_val_data(df, train_start_date, train_end_date, val_start_date, val_end_date, test_start_date, test_end_date):\n",
    "    train_start_date = pd.to_datetime(train_start_date)\n",
    "    train_end_date = pd.to_datetime(train_end_date)\n",
    "    val_start_date = pd.to_datetime(val_start_date)\n",
    "    val_end_date = pd.to_datetime(val_end_date)\n",
    "    test_start_date = pd.to_datetime(test_start_date)\n",
    "    test_end_date = pd.to_datetime(test_end_date)\n",
    "\n",
    "    train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "    val_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "    test_data = df[(pd.to_datetime(df['datetime']) >= test_start_date) & (pd.to_datetime(df['datetime']) <= test_end_date)]\n",
    "\n",
    "    print(f\"train_data_size: {train_data.shape}\")\n",
    "    print(f\"validation_data_size: {val_data.shape}\")\n",
    "    print(f\"test_data_size: {test_data.shape}\")\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=False, batch_size=32):\n",
    "    features = dataframe[feature_cols]\n",
    "    labels = dataframe[label_cols]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=min(len(features), 10000), seed=1024)\n",
    "    ds = ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer\n",
    "\n",
    "\n",
    "class QuantileClipTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_quantile=0.01, upper_quantile=0.99):\n",
    "        self.lower_quantile = lower_quantile\n",
    "        self.upper_quantile = upper_quantile\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # 计算给定分位数的分界值\n",
    "        self.lower_bound_ = np.nanquantile(X, self.lower_quantile, axis=0)\n",
    "        self.upper_bound_ = np.nanquantile(X, self.upper_quantile, axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # 对整个数组应用剪辑操作\n",
    "        return np.clip(X, self.lower_bound_, self.upper_bound_)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关闭滚动回测...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train': ['2009-01-01', '2016-12-31'],\n",
       "  'val': ['2017-01-01', '2018-12-31'],\n",
       "  'test': ['2019-01-01', '2024-12-31']}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据库初始化\n",
    "db_conn = sqlite3.connect('../database_auto/hh_quant_auto.db')\n",
    "db_downloader = DownloaderBase(db_conn, db_config)\n",
    "proprocessor = PreProcessing(db_downloader=db_downloader)\n",
    "\n",
    "# 相关配置\n",
    "rolling_flag = False\n",
    "# benchmark = '000905'\n",
    "benchmark = '000016'  # 上证50\n",
    "# benchmark = '000300' # 沪深300\n",
    "\n",
    "feature_config = {\n",
    "    \"target_features\": ['label'],\n",
    "    \"numeric_features\": ['turnover_rate', 'pe_ttm', 'ps_ttm', 'pcf_ncf_ttm', 'pb_mrq', 'KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60'],\n",
    "    \"integer_categorical_features\": ['month'],\n",
    "    \"string_categorical_features\": ['industry', 'season'],\n",
    "}\n",
    "batch_size = 1024\n",
    "\n",
    "# 是否开启滚动训练&回测\n",
    "if rolling_flag:\n",
    "    print(\"开启滚动回测...\")\n",
    "    backtest_period = get_rolling_data_period(\n",
    "        backtest_start_date='20200101', # 回测开始日期\n",
    "        backtest_duration=4, # 一共回测多久的数据（单位：年）\n",
    "        train_period=6, # 使用过去多久的时间进行训练（单位：年）\n",
    "        val_period=1, # 验证数据周期（单位：年）\n",
    "        test_period=1, # 测试数据周期（单位：年）\n",
    "    )\n",
    "else:\n",
    "    print(\"关闭滚动回测...\")\n",
    "    backtest_period = [\n",
    "        {\n",
    "            'train': ['2009-01-01', '2016-12-31'], # 8年训练\n",
    "            'val': ['2017-01-01', '2018-12-31'], # 2年验证\n",
    "            'test': ['2019-01-01', '2024-12-31'] # 5年+预测\n",
    "        }\n",
    "    ]\n",
    "\n",
    "backtest_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:09<00:00,  5.50it/s]\n"
     ]
    }
   ],
   "source": [
    "df = proprocessor._process_all_stock('000016', '2009-01-01', '2024-12-31')\n",
    "# df[(df['code'] == 'sh.688981') & (df['datetime'] == '2024-04-17')].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['2009-01-01', '2016-12-31'], 'val': ['2017-01-01', '2018-12-31'], 'test': ['2019-01-01', '2024-12-31']}\n",
      "开始加载原始数据...\n",
      "login success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process: 000016 ...: 100%|██████████| 50/50 [00:08<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始拆分训练、验证、测试集合...\n",
      "train_data_size: (50924, 204)\n",
      "validation_data_size: (18461, 204)\n",
      "test_data_size: (59602, 204)\n",
      "开始抽取特征数据...\n",
      "开始对特征进行预处理...\n",
      "开始对标签进行预处理...\n"
     ]
    }
   ],
   "source": [
    "date_period_params = backtest_period[0]\n",
    "print(date_period_params)\n",
    "train_start_date, train_end_date = date_period_params['train']\n",
    "val_start_date, val_end_date = date_period_params['val']\n",
    "test_start_date, test_end_date = date_period_params['test']\n",
    "# 获取全区间数据\n",
    "print(\"开始加载原始数据...\")\n",
    "df = proprocessor._process_all_stock(benchmark=benchmark, start_date=train_start_date, end_date=test_end_date)\n",
    "# 抽取训练验证数据\n",
    "print(\"开始拆分训练、验证、测试集合...\")\n",
    "train_data, val_data, test_data = extract_train_val_data(df, *[train_start_date, train_end_date, val_start_date, val_end_date, test_start_date, test_end_date])\n",
    "# 从data中抽取相关特征数据\n",
    "print(\"开始抽取特征数据...\")\n",
    "feature_columns = feature_config.get('numeric_features', []) + feature_config.get('integer_categorical_features', []) + feature_config.get('string_categorical_features', [])\n",
    "label_columns = feature_config.get('target_features', [])\n",
    "full_feature_columns = feature_columns + label_columns\n",
    "train_df, val_df, test_df = train_data[full_feature_columns], val_data[full_feature_columns], test_data[full_feature_columns]\n",
    "# 对相关特征进行特征工程\n",
    "print(\"开始对特征进行预处理...\")\n",
    "feature_preprocess_pipeline = Pipeline(steps=[\n",
    "    ('robust_scaler', RobustScaler()),\n",
    "    # ('quantile_transformer', QuantileTransformer(n_quantiles=5000, output_distribution='uniform', random_state=42)),\n",
    "    ('minmax_scaler', MinMaxScaler()),\n",
    "])\n",
    "preprocess_feature_columns = feature_config.get('numeric_features', [])\n",
    "train_df[preprocess_feature_columns] = feature_preprocess_pipeline.fit_transform(train_df[preprocess_feature_columns])\n",
    "val_df[preprocess_feature_columns] = feature_preprocess_pipeline.transform(val_df[preprocess_feature_columns])\n",
    "test_df[preprocess_feature_columns] = feature_preprocess_pipeline.transform(test_df[preprocess_feature_columns])\n",
    "\n",
    "print(\"开始对标签进行预处理...\")\n",
    "label_preprocess_pipeline = Pipeline(steps=[\n",
    "    ('quantile_clipper', QuantileClipTransformer()),\n",
    "    ('robust_scaler', RobustScaler()),\n",
    "    # ('quantile_transformer', QuantileTransformer(n_quantiles=1000, output_distribution='normal', random_state=42)),\n",
    "    ('minmax_scaler', MinMaxScaler()),\n",
    "])\n",
    "preprocess_target_columns = feature_config.get('target_features', [])\n",
    "train_df[preprocess_target_columns] = label_preprocess_pipeline.fit_transform(train_df[preprocess_target_columns])\n",
    "val_df[preprocess_target_columns] = label_preprocess_pipeline.transform(val_df[preprocess_target_columns])\n",
    "test_df[preprocess_target_columns] = label_preprocess_pipeline.transform(test_df[preprocess_target_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>turnover_rate</th>\n",
       "      <th>pe_ttm</th>\n",
       "      <th>ps_ttm</th>\n",
       "      <th>pcf_ncf_ttm</th>\n",
       "      <th>pb_mrq</th>\n",
       "      <th>KMID</th>\n",
       "      <th>KLEN</th>\n",
       "      <th>KMID2</th>\n",
       "      <th>KUP</th>\n",
       "      <th>KUP2</th>\n",
       "      <th>KLOW</th>\n",
       "      <th>KLOW2</th>\n",
       "      <th>KSFT</th>\n",
       "      <th>KSFT2</th>\n",
       "      <th>OPEN0</th>\n",
       "      <th>OPEN1</th>\n",
       "      <th>OPEN2</th>\n",
       "      <th>OPEN3</th>\n",
       "      <th>OPEN4</th>\n",
       "      <th>HIGH0</th>\n",
       "      <th>HIGH1</th>\n",
       "      <th>HIGH2</th>\n",
       "      <th>HIGH3</th>\n",
       "      <th>HIGH4</th>\n",
       "      <th>LOW0</th>\n",
       "      <th>LOW1</th>\n",
       "      <th>LOW2</th>\n",
       "      <th>LOW3</th>\n",
       "      <th>LOW4</th>\n",
       "      <th>CLOSE1</th>\n",
       "      <th>CLOSE2</th>\n",
       "      <th>CLOSE3</th>\n",
       "      <th>CLOSE4</th>\n",
       "      <th>VOLUME1</th>\n",
       "      <th>VOLUME2</th>\n",
       "      <th>VOLUME3</th>\n",
       "      <th>VOLUME4</th>\n",
       "      <th>ROC5</th>\n",
       "      <th>ROC10</th>\n",
       "      <th>ROC20</th>\n",
       "      <th>ROC30</th>\n",
       "      <th>ROC60</th>\n",
       "      <th>MAX5</th>\n",
       "      <th>MAX10</th>\n",
       "      <th>MAX20</th>\n",
       "      <th>MAX30</th>\n",
       "      <th>MAX60</th>\n",
       "      <th>MIN5</th>\n",
       "      <th>MIN10</th>\n",
       "      <th>MIN20</th>\n",
       "      <th>MIN30</th>\n",
       "      <th>MIN60</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA10</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA30</th>\n",
       "      <th>MA60</th>\n",
       "      <th>STD5</th>\n",
       "      <th>STD10</th>\n",
       "      <th>STD20</th>\n",
       "      <th>STD30</th>\n",
       "      <th>STD60</th>\n",
       "      <th>BETA5</th>\n",
       "      <th>BETA10</th>\n",
       "      <th>BETA20</th>\n",
       "      <th>BETA30</th>\n",
       "      <th>BETA60</th>\n",
       "      <th>RSQR5</th>\n",
       "      <th>RSQR10</th>\n",
       "      <th>RSQR20</th>\n",
       "      <th>RSQR30</th>\n",
       "      <th>RSQR60</th>\n",
       "      <th>RESI5</th>\n",
       "      <th>RESI10</th>\n",
       "      <th>RESI20</th>\n",
       "      <th>RESI30</th>\n",
       "      <th>RESI60</th>\n",
       "      <th>QTLU5</th>\n",
       "      <th>QTLU10</th>\n",
       "      <th>QTLU20</th>\n",
       "      <th>QTLU30</th>\n",
       "      <th>QTLU60</th>\n",
       "      <th>QTLD5</th>\n",
       "      <th>QTLD10</th>\n",
       "      <th>QTLD20</th>\n",
       "      <th>QTLD30</th>\n",
       "      <th>QTLD60</th>\n",
       "      <th>TSRANK5</th>\n",
       "      <th>TSRANK10</th>\n",
       "      <th>TSRANK20</th>\n",
       "      <th>TSRANK30</th>\n",
       "      <th>TSRANK60</th>\n",
       "      <th>RSV5</th>\n",
       "      <th>RSV10</th>\n",
       "      <th>RSV20</th>\n",
       "      <th>RSV30</th>\n",
       "      <th>RSV60</th>\n",
       "      <th>IMAX5</th>\n",
       "      <th>IMAX10</th>\n",
       "      <th>IMAX20</th>\n",
       "      <th>IMAX30</th>\n",
       "      <th>IMAX60</th>\n",
       "      <th>IMIN5</th>\n",
       "      <th>IMIN10</th>\n",
       "      <th>IMIN20</th>\n",
       "      <th>IMIN30</th>\n",
       "      <th>IMIN60</th>\n",
       "      <th>IMXD5</th>\n",
       "      <th>IMXD10</th>\n",
       "      <th>IMXD20</th>\n",
       "      <th>IMXD30</th>\n",
       "      <th>IMXD60</th>\n",
       "      <th>CORR5</th>\n",
       "      <th>CORR10</th>\n",
       "      <th>CORR20</th>\n",
       "      <th>CORR30</th>\n",
       "      <th>CORR60</th>\n",
       "      <th>CORD5</th>\n",
       "      <th>CORD10</th>\n",
       "      <th>CORD20</th>\n",
       "      <th>CORD30</th>\n",
       "      <th>CORD60</th>\n",
       "      <th>CNTP5</th>\n",
       "      <th>CNTP10</th>\n",
       "      <th>CNTP20</th>\n",
       "      <th>CNTP30</th>\n",
       "      <th>CNTP60</th>\n",
       "      <th>CNTN5</th>\n",
       "      <th>CNTN10</th>\n",
       "      <th>CNTN20</th>\n",
       "      <th>CNTN30</th>\n",
       "      <th>CNTN60</th>\n",
       "      <th>CNTD5</th>\n",
       "      <th>CNTD10</th>\n",
       "      <th>CNTD20</th>\n",
       "      <th>CNTD30</th>\n",
       "      <th>CNTD60</th>\n",
       "      <th>SUMP5</th>\n",
       "      <th>SUMP10</th>\n",
       "      <th>SUMP20</th>\n",
       "      <th>SUMP30</th>\n",
       "      <th>SUMP60</th>\n",
       "      <th>SUMN5</th>\n",
       "      <th>SUMN10</th>\n",
       "      <th>SUMN20</th>\n",
       "      <th>SUMN30</th>\n",
       "      <th>SUMN60</th>\n",
       "      <th>SUMD5</th>\n",
       "      <th>SUMD10</th>\n",
       "      <th>SUMD20</th>\n",
       "      <th>SUMD30</th>\n",
       "      <th>SUMD60</th>\n",
       "      <th>VMA5</th>\n",
       "      <th>VMA10</th>\n",
       "      <th>VMA20</th>\n",
       "      <th>VMA30</th>\n",
       "      <th>VMA60</th>\n",
       "      <th>VSTD5</th>\n",
       "      <th>VSTD10</th>\n",
       "      <th>VSTD20</th>\n",
       "      <th>VSTD30</th>\n",
       "      <th>VSTD60</th>\n",
       "      <th>WVMA5</th>\n",
       "      <th>WVMA10</th>\n",
       "      <th>WVMA20</th>\n",
       "      <th>WVMA30</th>\n",
       "      <th>WVMA60</th>\n",
       "      <th>VSUMP5</th>\n",
       "      <th>VSUMP10</th>\n",
       "      <th>VSUMP20</th>\n",
       "      <th>VSUMP30</th>\n",
       "      <th>VSUMP60</th>\n",
       "      <th>VSUMN5</th>\n",
       "      <th>VSUMN10</th>\n",
       "      <th>VSUMN20</th>\n",
       "      <th>VSUMN30</th>\n",
       "      <th>VSUMN60</th>\n",
       "      <th>VSUMD5</th>\n",
       "      <th>VSUMD10</th>\n",
       "      <th>VSUMD20</th>\n",
       "      <th>VSUMD30</th>\n",
       "      <th>VSUMD60</th>\n",
       "      <th>month</th>\n",
       "      <th>industry</th>\n",
       "      <th>season</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.841578</td>\n",
       "      <td>0.012192</td>\n",
       "      <td>0.877479</td>\n",
       "      <td>0.041680</td>\n",
       "      <td>0.482108</td>\n",
       "      <td>0.095335</td>\n",
       "      <td>0.800172</td>\n",
       "      <td>0.025570</td>\n",
       "      <td>0.133028</td>\n",
       "      <td>0.046211</td>\n",
       "      <td>0.266628</td>\n",
       "      <td>0.489105</td>\n",
       "      <td>8.669725e-01</td>\n",
       "      <td>0.418434</td>\n",
       "      <td>0.387175</td>\n",
       "      <td>0.373768</td>\n",
       "      <td>0.358275</td>\n",
       "      <td>0.325918</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>0.212298</td>\n",
       "      <td>0.262395</td>\n",
       "      <td>0.303919</td>\n",
       "      <td>0.260756</td>\n",
       "      <td>0.900455</td>\n",
       "      <td>0.605394</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>0.397019</td>\n",
       "      <td>0.359173</td>\n",
       "      <td>0.335271</td>\n",
       "      <td>0.384364</td>\n",
       "      <td>0.380911</td>\n",
       "      <td>0.353711</td>\n",
       "      <td>0.080716</td>\n",
       "      <td>0.084997</td>\n",
       "      <td>0.117099</td>\n",
       "      <td>0.050809</td>\n",
       "      <td>0.317632</td>\n",
       "      <td>0.329123</td>\n",
       "      <td>0.379469</td>\n",
       "      <td>0.322539</td>\n",
       "      <td>0.489385</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.063316</td>\n",
       "      <td>0.193464</td>\n",
       "      <td>0.229166</td>\n",
       "      <td>0.261840</td>\n",
       "      <td>0.597133</td>\n",
       "      <td>0.981827</td>\n",
       "      <td>0.981740</td>\n",
       "      <td>0.981651</td>\n",
       "      <td>0.981381</td>\n",
       "      <td>0.365780</td>\n",
       "      <td>0.336235</td>\n",
       "      <td>0.340839</td>\n",
       "      <td>0.333201</td>\n",
       "      <td>0.388849</td>\n",
       "      <td>0.042353</td>\n",
       "      <td>0.098343</td>\n",
       "      <td>0.159695</td>\n",
       "      <td>0.146417</td>\n",
       "      <td>0.207034</td>\n",
       "      <td>0.636301</td>\n",
       "      <td>0.648305</td>\n",
       "      <td>0.570547</td>\n",
       "      <td>0.577404</td>\n",
       "      <td>0.521416</td>\n",
       "      <td>0.236434</td>\n",
       "      <td>0.622743</td>\n",
       "      <td>0.915534</td>\n",
       "      <td>0.607031</td>\n",
       "      <td>0.100373</td>\n",
       "      <td>0.558938</td>\n",
       "      <td>0.508894</td>\n",
       "      <td>0.602363</td>\n",
       "      <td>0.555440</td>\n",
       "      <td>0.739108</td>\n",
       "      <td>0.133720</td>\n",
       "      <td>0.176910</td>\n",
       "      <td>0.244363</td>\n",
       "      <td>0.261329</td>\n",
       "      <td>0.333339</td>\n",
       "      <td>0.688320</td>\n",
       "      <td>0.650492</td>\n",
       "      <td>0.500004</td>\n",
       "      <td>0.406848</td>\n",
       "      <td>0.434894</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.993142</td>\n",
       "      <td>0.084408</td>\n",
       "      <td>0.012823</td>\n",
       "      <td>0.006939</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.966102</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.459666</td>\n",
       "      <td>0.945243</td>\n",
       "      <td>0.874062</td>\n",
       "      <td>0.804459</td>\n",
       "      <td>0.597788</td>\n",
       "      <td>0.782145</td>\n",
       "      <td>0.641823</td>\n",
       "      <td>0.686787</td>\n",
       "      <td>0.730910</td>\n",
       "      <td>0.674873</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.647103</td>\n",
       "      <td>0.191657</td>\n",
       "      <td>0.191953</td>\n",
       "      <td>0.313155</td>\n",
       "      <td>0.279049</td>\n",
       "      <td>0.352897</td>\n",
       "      <td>0.808343</td>\n",
       "      <td>0.808047</td>\n",
       "      <td>0.686845</td>\n",
       "      <td>0.720951</td>\n",
       "      <td>0.647103</td>\n",
       "      <td>0.191657</td>\n",
       "      <td>0.191953</td>\n",
       "      <td>0.313155</td>\n",
       "      <td>0.279049</td>\n",
       "      <td>0.089666</td>\n",
       "      <td>0.130229</td>\n",
       "      <td>0.143430</td>\n",
       "      <td>0.117604</td>\n",
       "      <td>0.145318</td>\n",
       "      <td>0.015632</td>\n",
       "      <td>0.120173</td>\n",
       "      <td>0.191937</td>\n",
       "      <td>0.139051</td>\n",
       "      <td>0.074666</td>\n",
       "      <td>0.317027</td>\n",
       "      <td>0.373854</td>\n",
       "      <td>0.230806</td>\n",
       "      <td>0.190433</td>\n",
       "      <td>0.114214</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.477762</td>\n",
       "      <td>0.402884</td>\n",
       "      <td>0.440317</td>\n",
       "      <td>0.481866</td>\n",
       "      <td>0.709000</td>\n",
       "      <td>0.522238</td>\n",
       "      <td>0.597116</td>\n",
       "      <td>0.559683</td>\n",
       "      <td>0.518134</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.477762</td>\n",
       "      <td>0.402884</td>\n",
       "      <td>0.440317</td>\n",
       "      <td>0.481866</td>\n",
       "      <td>1</td>\n",
       "      <td>化工</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0.542538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.841601</td>\n",
       "      <td>0.012667</td>\n",
       "      <td>0.877592</td>\n",
       "      <td>0.043481</td>\n",
       "      <td>0.540810</td>\n",
       "      <td>0.201930</td>\n",
       "      <td>0.906317</td>\n",
       "      <td>0.038087</td>\n",
       "      <td>0.093548</td>\n",
       "      <td>0.034441</td>\n",
       "      <td>0.093817</td>\n",
       "      <td>0.540840</td>\n",
       "      <td>9.064516e-01</td>\n",
       "      <td>0.362528</td>\n",
       "      <td>0.348473</td>\n",
       "      <td>0.331699</td>\n",
       "      <td>0.339676</td>\n",
       "      <td>0.282332</td>\n",
       "      <td>0.018179</td>\n",
       "      <td>0.142945</td>\n",
       "      <td>0.218419</td>\n",
       "      <td>0.271066</td>\n",
       "      <td>0.242634</td>\n",
       "      <td>0.784590</td>\n",
       "      <td>0.567675</td>\n",
       "      <td>0.465807</td>\n",
       "      <td>0.395165</td>\n",
       "      <td>0.310664</td>\n",
       "      <td>0.305667</td>\n",
       "      <td>0.296482</td>\n",
       "      <td>0.327165</td>\n",
       "      <td>0.327114</td>\n",
       "      <td>0.038346</td>\n",
       "      <td>0.030523</td>\n",
       "      <td>0.054908</td>\n",
       "      <td>0.028203</td>\n",
       "      <td>0.284172</td>\n",
       "      <td>0.312786</td>\n",
       "      <td>0.357217</td>\n",
       "      <td>0.308196</td>\n",
       "      <td>0.448410</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.051854</td>\n",
       "      <td>0.183595</td>\n",
       "      <td>0.219733</td>\n",
       "      <td>0.252808</td>\n",
       "      <td>0.579334</td>\n",
       "      <td>0.960675</td>\n",
       "      <td>0.960486</td>\n",
       "      <td>0.960295</td>\n",
       "      <td>0.959710</td>\n",
       "      <td>0.318609</td>\n",
       "      <td>0.293998</td>\n",
       "      <td>0.310850</td>\n",
       "      <td>0.309907</td>\n",
       "      <td>0.364895</td>\n",
       "      <td>0.098950</td>\n",
       "      <td>0.063478</td>\n",
       "      <td>0.143334</td>\n",
       "      <td>0.140919</td>\n",
       "      <td>0.189089</td>\n",
       "      <td>0.670610</td>\n",
       "      <td>0.686082</td>\n",
       "      <td>0.586621</td>\n",
       "      <td>0.581267</td>\n",
       "      <td>0.531282</td>\n",
       "      <td>0.578460</td>\n",
       "      <td>0.157109</td>\n",
       "      <td>0.835817</td>\n",
       "      <td>0.596974</td>\n",
       "      <td>0.075304</td>\n",
       "      <td>0.589188</td>\n",
       "      <td>0.519562</td>\n",
       "      <td>0.633186</td>\n",
       "      <td>0.593862</td>\n",
       "      <td>0.757134</td>\n",
       "      <td>0.104862</td>\n",
       "      <td>0.141381</td>\n",
       "      <td>0.205611</td>\n",
       "      <td>0.234259</td>\n",
       "      <td>0.314904</td>\n",
       "      <td>0.611737</td>\n",
       "      <td>0.605687</td>\n",
       "      <td>0.471065</td>\n",
       "      <td>0.385084</td>\n",
       "      <td>0.415047</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.990467</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>0.015524</td>\n",
       "      <td>0.006520</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.901394</td>\n",
       "      <td>0.908289</td>\n",
       "      <td>0.879009</td>\n",
       "      <td>0.794301</td>\n",
       "      <td>0.617869</td>\n",
       "      <td>0.882316</td>\n",
       "      <td>0.780526</td>\n",
       "      <td>0.748364</td>\n",
       "      <td>0.762580</td>\n",
       "      <td>0.690030</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.846179</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.263531</td>\n",
       "      <td>0.350212</td>\n",
       "      <td>0.313790</td>\n",
       "      <td>0.153821</td>\n",
       "      <td>0.687935</td>\n",
       "      <td>0.736469</td>\n",
       "      <td>0.649788</td>\n",
       "      <td>0.686210</td>\n",
       "      <td>0.846179</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.263531</td>\n",
       "      <td>0.350212</td>\n",
       "      <td>0.313790</td>\n",
       "      <td>0.039874</td>\n",
       "      <td>0.049824</td>\n",
       "      <td>0.065197</td>\n",
       "      <td>0.053897</td>\n",
       "      <td>0.068265</td>\n",
       "      <td>0.023229</td>\n",
       "      <td>0.035044</td>\n",
       "      <td>0.083730</td>\n",
       "      <td>0.059222</td>\n",
       "      <td>0.031176</td>\n",
       "      <td>0.480583</td>\n",
       "      <td>0.299247</td>\n",
       "      <td>0.212613</td>\n",
       "      <td>0.173642</td>\n",
       "      <td>0.113180</td>\n",
       "      <td>0.807726</td>\n",
       "      <td>0.357322</td>\n",
       "      <td>0.529292</td>\n",
       "      <td>0.514755</td>\n",
       "      <td>0.506248</td>\n",
       "      <td>0.192274</td>\n",
       "      <td>0.642678</td>\n",
       "      <td>0.470708</td>\n",
       "      <td>0.485245</td>\n",
       "      <td>0.493752</td>\n",
       "      <td>0.807726</td>\n",
       "      <td>0.357322</td>\n",
       "      <td>0.529292</td>\n",
       "      <td>0.514755</td>\n",
       "      <td>0.506248</td>\n",
       "      <td>1</td>\n",
       "      <td>化工</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0.445070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.841584</td>\n",
       "      <td>0.012322</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.042171</td>\n",
       "      <td>0.400420</td>\n",
       "      <td>0.103691</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>0.024639</td>\n",
       "      <td>0.117855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393707</td>\n",
       "      <td>2.522316e-12</td>\n",
       "      <td>0.500728</td>\n",
       "      <td>0.394183</td>\n",
       "      <td>0.363874</td>\n",
       "      <td>0.358553</td>\n",
       "      <td>0.316499</td>\n",
       "      <td>0.105554</td>\n",
       "      <td>0.261358</td>\n",
       "      <td>0.248118</td>\n",
       "      <td>0.297203</td>\n",
       "      <td>0.266207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.647487</td>\n",
       "      <td>0.530368</td>\n",
       "      <td>0.405536</td>\n",
       "      <td>0.365846</td>\n",
       "      <td>0.559337</td>\n",
       "      <td>0.404980</td>\n",
       "      <td>0.349991</td>\n",
       "      <td>0.347343</td>\n",
       "      <td>0.153743</td>\n",
       "      <td>0.058941</td>\n",
       "      <td>0.079684</td>\n",
       "      <td>0.054173</td>\n",
       "      <td>0.311092</td>\n",
       "      <td>0.306436</td>\n",
       "      <td>0.382636</td>\n",
       "      <td>0.296352</td>\n",
       "      <td>0.465814</td>\n",
       "      <td>0.022352</td>\n",
       "      <td>0.060124</td>\n",
       "      <td>0.190716</td>\n",
       "      <td>0.226539</td>\n",
       "      <td>0.259325</td>\n",
       "      <td>0.592176</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.380070</td>\n",
       "      <td>0.311688</td>\n",
       "      <td>0.319491</td>\n",
       "      <td>0.322339</td>\n",
       "      <td>0.375840</td>\n",
       "      <td>0.101485</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.125853</td>\n",
       "      <td>0.146970</td>\n",
       "      <td>0.180058</td>\n",
       "      <td>0.672309</td>\n",
       "      <td>0.708749</td>\n",
       "      <td>0.600571</td>\n",
       "      <td>0.574536</td>\n",
       "      <td>0.539064</td>\n",
       "      <td>0.590325</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>0.793042</td>\n",
       "      <td>0.642385</td>\n",
       "      <td>0.052533</td>\n",
       "      <td>0.434842</td>\n",
       "      <td>0.421152</td>\n",
       "      <td>0.598299</td>\n",
       "      <td>0.579175</td>\n",
       "      <td>0.739230</td>\n",
       "      <td>0.164052</td>\n",
       "      <td>0.150041</td>\n",
       "      <td>0.215525</td>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.327554</td>\n",
       "      <td>0.671547</td>\n",
       "      <td>0.638014</td>\n",
       "      <td>0.491945</td>\n",
       "      <td>0.400787</td>\n",
       "      <td>0.429367</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.945497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.871443</td>\n",
       "      <td>0.875968</td>\n",
       "      <td>0.839218</td>\n",
       "      <td>0.807003</td>\n",
       "      <td>0.646536</td>\n",
       "      <td>0.928379</td>\n",
       "      <td>0.771765</td>\n",
       "      <td>0.714326</td>\n",
       "      <td>0.756236</td>\n",
       "      <td>0.692480</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.619028</td>\n",
       "      <td>0.335872</td>\n",
       "      <td>0.193651</td>\n",
       "      <td>0.375421</td>\n",
       "      <td>0.300170</td>\n",
       "      <td>0.380972</td>\n",
       "      <td>0.664128</td>\n",
       "      <td>0.806349</td>\n",
       "      <td>0.624579</td>\n",
       "      <td>0.699830</td>\n",
       "      <td>0.619028</td>\n",
       "      <td>0.335872</td>\n",
       "      <td>0.193651</td>\n",
       "      <td>0.375421</td>\n",
       "      <td>0.300170</td>\n",
       "      <td>0.092726</td>\n",
       "      <td>0.094754</td>\n",
       "      <td>0.113668</td>\n",
       "      <td>0.104676</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.045649</td>\n",
       "      <td>0.065781</td>\n",
       "      <td>0.130095</td>\n",
       "      <td>0.124264</td>\n",
       "      <td>0.066294</td>\n",
       "      <td>0.398962</td>\n",
       "      <td>0.335257</td>\n",
       "      <td>0.232749</td>\n",
       "      <td>0.182593</td>\n",
       "      <td>0.111906</td>\n",
       "      <td>0.462346</td>\n",
       "      <td>0.382043</td>\n",
       "      <td>0.331650</td>\n",
       "      <td>0.509985</td>\n",
       "      <td>0.477874</td>\n",
       "      <td>0.537654</td>\n",
       "      <td>0.617957</td>\n",
       "      <td>0.668350</td>\n",
       "      <td>0.490015</td>\n",
       "      <td>0.522126</td>\n",
       "      <td>0.462346</td>\n",
       "      <td>0.382043</td>\n",
       "      <td>0.331650</td>\n",
       "      <td>0.509985</td>\n",
       "      <td>0.477874</td>\n",
       "      <td>1</td>\n",
       "      <td>化工</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0.559156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009326</td>\n",
       "      <td>0.841573</td>\n",
       "      <td>0.012085</td>\n",
       "      <td>0.877453</td>\n",
       "      <td>0.041271</td>\n",
       "      <td>0.450668</td>\n",
       "      <td>0.126438</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.025537</td>\n",
       "      <td>0.100172</td>\n",
       "      <td>0.206836</td>\n",
       "      <td>0.899828</td>\n",
       "      <td>0.506209</td>\n",
       "      <td>8.998280e-01</td>\n",
       "      <td>0.449466</td>\n",
       "      <td>0.475964</td>\n",
       "      <td>0.390654</td>\n",
       "      <td>0.383425</td>\n",
       "      <td>0.325853</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.290327</td>\n",
       "      <td>0.318227</td>\n",
       "      <td>0.314148</td>\n",
       "      <td>0.281150</td>\n",
       "      <td>0.861238</td>\n",
       "      <td>0.738237</td>\n",
       "      <td>0.572493</td>\n",
       "      <td>0.448954</td>\n",
       "      <td>0.366964</td>\n",
       "      <td>0.526650</td>\n",
       "      <td>0.517693</td>\n",
       "      <td>0.412374</td>\n",
       "      <td>0.356886</td>\n",
       "      <td>0.053338</td>\n",
       "      <td>0.080969</td>\n",
       "      <td>0.052151</td>\n",
       "      <td>0.026451</td>\n",
       "      <td>0.321595</td>\n",
       "      <td>0.290880</td>\n",
       "      <td>0.361752</td>\n",
       "      <td>0.299346</td>\n",
       "      <td>0.465204</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.066022</td>\n",
       "      <td>0.195795</td>\n",
       "      <td>0.231393</td>\n",
       "      <td>0.263973</td>\n",
       "      <td>0.601336</td>\n",
       "      <td>0.974668</td>\n",
       "      <td>0.974546</td>\n",
       "      <td>0.974423</td>\n",
       "      <td>0.974046</td>\n",
       "      <td>0.421671</td>\n",
       "      <td>0.325667</td>\n",
       "      <td>0.324738</td>\n",
       "      <td>0.330374</td>\n",
       "      <td>0.382508</td>\n",
       "      <td>0.095919</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.117798</td>\n",
       "      <td>0.153186</td>\n",
       "      <td>0.170441</td>\n",
       "      <td>0.643052</td>\n",
       "      <td>0.715957</td>\n",
       "      <td>0.608033</td>\n",
       "      <td>0.566585</td>\n",
       "      <td>0.546266</td>\n",
       "      <td>0.108763</td>\n",
       "      <td>0.171149</td>\n",
       "      <td>0.749807</td>\n",
       "      <td>0.701584</td>\n",
       "      <td>0.034118</td>\n",
       "      <td>0.405685</td>\n",
       "      <td>0.369511</td>\n",
       "      <td>0.578527</td>\n",
       "      <td>0.574170</td>\n",
       "      <td>0.726385</td>\n",
       "      <td>0.196569</td>\n",
       "      <td>0.156647</td>\n",
       "      <td>0.222068</td>\n",
       "      <td>0.256781</td>\n",
       "      <td>0.333821</td>\n",
       "      <td>0.736512</td>\n",
       "      <td>0.661071</td>\n",
       "      <td>0.506837</td>\n",
       "      <td>0.410142</td>\n",
       "      <td>0.439581</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.992992</td>\n",
       "      <td>0.109719</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>0.009554</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.618644</td>\n",
       "      <td>0.827619</td>\n",
       "      <td>0.807214</td>\n",
       "      <td>0.793769</td>\n",
       "      <td>0.809557</td>\n",
       "      <td>0.665175</td>\n",
       "      <td>0.801138</td>\n",
       "      <td>0.839039</td>\n",
       "      <td>0.683228</td>\n",
       "      <td>0.743335</td>\n",
       "      <td>0.687432</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.549297</td>\n",
       "      <td>0.403661</td>\n",
       "      <td>0.224506</td>\n",
       "      <td>0.370855</td>\n",
       "      <td>0.302611</td>\n",
       "      <td>0.450703</td>\n",
       "      <td>0.596339</td>\n",
       "      <td>0.775494</td>\n",
       "      <td>0.629145</td>\n",
       "      <td>0.697389</td>\n",
       "      <td>0.549297</td>\n",
       "      <td>0.403661</td>\n",
       "      <td>0.224506</td>\n",
       "      <td>0.370855</td>\n",
       "      <td>0.302611</td>\n",
       "      <td>0.057481</td>\n",
       "      <td>0.052237</td>\n",
       "      <td>0.068149</td>\n",
       "      <td>0.067968</td>\n",
       "      <td>0.085043</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>0.068606</td>\n",
       "      <td>0.076131</td>\n",
       "      <td>0.040626</td>\n",
       "      <td>0.297384</td>\n",
       "      <td>0.200706</td>\n",
       "      <td>0.203776</td>\n",
       "      <td>0.177890</td>\n",
       "      <td>0.111822</td>\n",
       "      <td>0.567205</td>\n",
       "      <td>0.389870</td>\n",
       "      <td>0.429412</td>\n",
       "      <td>0.535544</td>\n",
       "      <td>0.489263</td>\n",
       "      <td>0.432795</td>\n",
       "      <td>0.610130</td>\n",
       "      <td>0.570588</td>\n",
       "      <td>0.464456</td>\n",
       "      <td>0.510737</td>\n",
       "      <td>0.567205</td>\n",
       "      <td>0.389870</td>\n",
       "      <td>0.429412</td>\n",
       "      <td>0.535544</td>\n",
       "      <td>0.489263</td>\n",
       "      <td>1</td>\n",
       "      <td>化工</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0.611113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.841573</td>\n",
       "      <td>0.012085</td>\n",
       "      <td>0.877453</td>\n",
       "      <td>0.041271</td>\n",
       "      <td>0.471657</td>\n",
       "      <td>0.063754</td>\n",
       "      <td>0.799656</td>\n",
       "      <td>0.025753</td>\n",
       "      <td>0.200344</td>\n",
       "      <td>0.023221</td>\n",
       "      <td>0.200344</td>\n",
       "      <td>0.471657</td>\n",
       "      <td>7.996561e-01</td>\n",
       "      <td>0.428663</td>\n",
       "      <td>0.416640</td>\n",
       "      <td>0.433074</td>\n",
       "      <td>0.391888</td>\n",
       "      <td>0.331376</td>\n",
       "      <td>0.012632</td>\n",
       "      <td>0.209280</td>\n",
       "      <td>0.314105</td>\n",
       "      <td>0.353286</td>\n",
       "      <td>0.279789</td>\n",
       "      <td>0.938343</td>\n",
       "      <td>0.627648</td>\n",
       "      <td>0.605244</td>\n",
       "      <td>0.461051</td>\n",
       "      <td>0.384675</td>\n",
       "      <td>0.451155</td>\n",
       "      <td>0.462874</td>\n",
       "      <td>0.462003</td>\n",
       "      <td>0.385203</td>\n",
       "      <td>0.141906</td>\n",
       "      <td>0.074702</td>\n",
       "      <td>0.193854</td>\n",
       "      <td>0.047488</td>\n",
       "      <td>0.314841</td>\n",
       "      <td>0.279950</td>\n",
       "      <td>0.362434</td>\n",
       "      <td>0.307436</td>\n",
       "      <td>0.427033</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.066022</td>\n",
       "      <td>0.195795</td>\n",
       "      <td>0.231393</td>\n",
       "      <td>0.263973</td>\n",
       "      <td>0.601336</td>\n",
       "      <td>0.988744</td>\n",
       "      <td>0.988690</td>\n",
       "      <td>0.988635</td>\n",
       "      <td>0.988468</td>\n",
       "      <td>0.429783</td>\n",
       "      <td>0.324712</td>\n",
       "      <td>0.317994</td>\n",
       "      <td>0.327559</td>\n",
       "      <td>0.379635</td>\n",
       "      <td>0.074691</td>\n",
       "      <td>0.034982</td>\n",
       "      <td>0.103195</td>\n",
       "      <td>0.155882</td>\n",
       "      <td>0.164720</td>\n",
       "      <td>0.598372</td>\n",
       "      <td>0.717776</td>\n",
       "      <td>0.619442</td>\n",
       "      <td>0.562569</td>\n",
       "      <td>0.550645</td>\n",
       "      <td>0.271135</td>\n",
       "      <td>0.235801</td>\n",
       "      <td>0.703258</td>\n",
       "      <td>0.734654</td>\n",
       "      <td>0.024098</td>\n",
       "      <td>0.496537</td>\n",
       "      <td>0.367399</td>\n",
       "      <td>0.574549</td>\n",
       "      <td>0.584399</td>\n",
       "      <td>0.726032</td>\n",
       "      <td>0.196569</td>\n",
       "      <td>0.155599</td>\n",
       "      <td>0.219225</td>\n",
       "      <td>0.256781</td>\n",
       "      <td>0.330818</td>\n",
       "      <td>0.746068</td>\n",
       "      <td>0.661071</td>\n",
       "      <td>0.506837</td>\n",
       "      <td>0.406036</td>\n",
       "      <td>0.439581</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.189655</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.992992</td>\n",
       "      <td>0.051918</td>\n",
       "      <td>0.007887</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.807913</td>\n",
       "      <td>0.790156</td>\n",
       "      <td>0.743989</td>\n",
       "      <td>0.832225</td>\n",
       "      <td>0.668057</td>\n",
       "      <td>0.769431</td>\n",
       "      <td>0.780645</td>\n",
       "      <td>0.667273</td>\n",
       "      <td>0.738520</td>\n",
       "      <td>0.707992</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.468070</td>\n",
       "      <td>0.221205</td>\n",
       "      <td>0.347337</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.409121</td>\n",
       "      <td>0.531930</td>\n",
       "      <td>0.778795</td>\n",
       "      <td>0.652663</td>\n",
       "      <td>0.664996</td>\n",
       "      <td>0.590879</td>\n",
       "      <td>0.468070</td>\n",
       "      <td>0.221205</td>\n",
       "      <td>0.347337</td>\n",
       "      <td>0.335004</td>\n",
       "      <td>0.116574</td>\n",
       "      <td>0.096104</td>\n",
       "      <td>0.112468</td>\n",
       "      <td>0.120240</td>\n",
       "      <td>0.147839</td>\n",
       "      <td>0.056120</td>\n",
       "      <td>0.041191</td>\n",
       "      <td>0.111575</td>\n",
       "      <td>0.142633</td>\n",
       "      <td>0.076917</td>\n",
       "      <td>0.347976</td>\n",
       "      <td>0.265330</td>\n",
       "      <td>0.207570</td>\n",
       "      <td>0.184720</td>\n",
       "      <td>0.120504</td>\n",
       "      <td>0.498840</td>\n",
       "      <td>0.467695</td>\n",
       "      <td>0.404988</td>\n",
       "      <td>0.511253</td>\n",
       "      <td>0.453605</td>\n",
       "      <td>0.501160</td>\n",
       "      <td>0.532305</td>\n",
       "      <td>0.595012</td>\n",
       "      <td>0.488747</td>\n",
       "      <td>0.546395</td>\n",
       "      <td>0.498840</td>\n",
       "      <td>0.467695</td>\n",
       "      <td>0.404988</td>\n",
       "      <td>0.511253</td>\n",
       "      <td>0.453605</td>\n",
       "      <td>1</td>\n",
       "      <td>化工</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0.703479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   turnover_rate    pe_ttm    ps_ttm  pcf_ncf_ttm    pb_mrq      KMID  \\\n",
       "0       0.005568  0.841578  0.012192     0.877479  0.041680  0.482108   \n",
       "1       0.011445  0.841601  0.012667     0.877592  0.043481  0.540810   \n",
       "2       0.006184  0.841584  0.012322     0.877510  0.042171  0.400420   \n",
       "3       0.009326  0.841573  0.012085     0.877453  0.041271  0.450668   \n",
       "4       0.005450  0.841573  0.012085     0.877453  0.041271  0.471657   \n",
       "\n",
       "       KLEN     KMID2       KUP      KUP2      KLOW     KLOW2      KSFT  \\\n",
       "0  0.095335  0.800172  0.025570  0.133028  0.046211  0.266628  0.489105   \n",
       "1  0.201930  0.906317  0.038087  0.093548  0.034441  0.093817  0.540840   \n",
       "2  0.103691  0.058928  0.024639  0.117855  0.000000  0.000000  0.393707   \n",
       "3  0.126438  0.500000  0.025537  0.100172  0.206836  0.899828  0.506209   \n",
       "4  0.063754  0.799656  0.025753  0.200344  0.023221  0.200344  0.471657   \n",
       "\n",
       "          KSFT2     OPEN0     OPEN1     OPEN2     OPEN3     OPEN4     HIGH0  \\\n",
       "0  8.669725e-01  0.418434  0.387175  0.373768  0.358275  0.325918  0.012490   \n",
       "1  9.064516e-01  0.362528  0.348473  0.331699  0.339676  0.282332  0.018179   \n",
       "2  2.522316e-12  0.500728  0.394183  0.363874  0.358553  0.316499  0.105554   \n",
       "3  8.998280e-01  0.449466  0.475964  0.390654  0.383425  0.325853  0.012632   \n",
       "4  7.996561e-01  0.428663  0.416640  0.433074  0.391888  0.331376  0.012632   \n",
       "\n",
       "      HIGH1     HIGH2     HIGH3     HIGH4      LOW0      LOW1      LOW2  \\\n",
       "0  0.212298  0.262395  0.303919  0.260756  0.900455  0.605394  0.542480   \n",
       "1  0.142945  0.218419  0.271066  0.242634  0.784590  0.567675  0.465807   \n",
       "2  0.261358  0.248118  0.297203  0.266207  1.000000  0.647487  0.530368   \n",
       "3  0.290327  0.318227  0.314148  0.281150  0.861238  0.738237  0.572493   \n",
       "4  0.209280  0.314105  0.353286  0.279789  0.938343  0.627648  0.605244   \n",
       "\n",
       "       LOW3      LOW4    CLOSE1    CLOSE2    CLOSE3    CLOSE4   VOLUME1  \\\n",
       "0  0.397019  0.359173  0.335271  0.384364  0.380911  0.353711  0.080716   \n",
       "1  0.395165  0.310664  0.305667  0.296482  0.327165  0.327114  0.038346   \n",
       "2  0.405536  0.365846  0.559337  0.404980  0.349991  0.347343  0.153743   \n",
       "3  0.448954  0.366964  0.526650  0.517693  0.412374  0.356886  0.053338   \n",
       "4  0.461051  0.384675  0.451155  0.462874  0.462003  0.385203  0.141906   \n",
       "\n",
       "    VOLUME2   VOLUME3   VOLUME4      ROC5     ROC10     ROC20     ROC30  \\\n",
       "0  0.084997  0.117099  0.050809  0.317632  0.329123  0.379469  0.322539   \n",
       "1  0.030523  0.054908  0.028203  0.284172  0.312786  0.357217  0.308196   \n",
       "2  0.058941  0.079684  0.054173  0.311092  0.306436  0.382636  0.296352   \n",
       "3  0.080969  0.052151  0.026451  0.321595  0.290880  0.361752  0.299346   \n",
       "4  0.074702  0.193854  0.047488  0.314841  0.279950  0.362434  0.307436   \n",
       "\n",
       "      ROC60      MAX5     MAX10     MAX20     MAX30     MAX60      MIN5  \\\n",
       "0  0.489385  0.002645  0.063316  0.193464  0.229166  0.261840  0.597133   \n",
       "1  0.448410  0.003849  0.051854  0.183595  0.219733  0.252808  0.579334   \n",
       "2  0.465814  0.022352  0.060124  0.190716  0.226539  0.259325  0.592176   \n",
       "3  0.465204  0.002675  0.066022  0.195795  0.231393  0.263973  0.601336   \n",
       "4  0.427033  0.002675  0.066022  0.195795  0.231393  0.263973  0.601336   \n",
       "\n",
       "      MIN10     MIN20     MIN30     MIN60       MA5      MA10      MA20  \\\n",
       "0  0.981827  0.981740  0.981651  0.981381  0.365780  0.336235  0.340839   \n",
       "1  0.960675  0.960486  0.960295  0.959710  0.318609  0.293998  0.310850   \n",
       "2  1.000000  1.000000  1.000000  1.000000  0.380070  0.311688  0.319491   \n",
       "3  0.974668  0.974546  0.974423  0.974046  0.421671  0.325667  0.324738   \n",
       "4  0.988744  0.988690  0.988635  0.988468  0.429783  0.324712  0.317994   \n",
       "\n",
       "       MA30      MA60      STD5     STD10     STD20     STD30     STD60  \\\n",
       "0  0.333201  0.388849  0.042353  0.098343  0.159695  0.146417  0.207034   \n",
       "1  0.309907  0.364895  0.098950  0.063478  0.143334  0.140919  0.189089   \n",
       "2  0.322339  0.375840  0.101485  0.039910  0.125853  0.146970  0.180058   \n",
       "3  0.330374  0.382508  0.095919  0.035387  0.117798  0.153186  0.170441   \n",
       "4  0.327559  0.379635  0.074691  0.034982  0.103195  0.155882  0.164720   \n",
       "\n",
       "      BETA5    BETA10    BETA20    BETA30    BETA60     RSQR5    RSQR10  \\\n",
       "0  0.636301  0.648305  0.570547  0.577404  0.521416  0.236434  0.622743   \n",
       "1  0.670610  0.686082  0.586621  0.581267  0.531282  0.578460  0.157109   \n",
       "2  0.672309  0.708749  0.600571  0.574536  0.539064  0.590325  0.017773   \n",
       "3  0.643052  0.715957  0.608033  0.566585  0.546266  0.108763  0.171149   \n",
       "4  0.598372  0.717776  0.619442  0.562569  0.550645  0.271135  0.235801   \n",
       "\n",
       "     RSQR20    RSQR30    RSQR60     RESI5    RESI10    RESI20    RESI30  \\\n",
       "0  0.915534  0.607031  0.100373  0.558938  0.508894  0.602363  0.555440   \n",
       "1  0.835817  0.596974  0.075304  0.589188  0.519562  0.633186  0.593862   \n",
       "2  0.793042  0.642385  0.052533  0.434842  0.421152  0.598299  0.579175   \n",
       "3  0.749807  0.701584  0.034118  0.405685  0.369511  0.578527  0.574170   \n",
       "4  0.703258  0.734654  0.024098  0.496537  0.367399  0.574549  0.584399   \n",
       "\n",
       "     RESI60     QTLU5    QTLU10    QTLU20    QTLU30    QTLU60     QTLD5  \\\n",
       "0  0.739108  0.133720  0.176910  0.244363  0.261329  0.333339  0.688320   \n",
       "1  0.757134  0.104862  0.141381  0.205611  0.234259  0.314904  0.611737   \n",
       "2  0.739230  0.164052  0.150041  0.215525  0.247404  0.327554  0.671547   \n",
       "3  0.726385  0.196569  0.156647  0.222068  0.256781  0.333821  0.736512   \n",
       "4  0.726032  0.196569  0.155599  0.219225  0.256781  0.330818  0.746068   \n",
       "\n",
       "     QTLD10    QTLD20    QTLD30    QTLD60  TSRANK5  TSRANK10  TSRANK20  \\\n",
       "0  0.650492  0.500004  0.406848  0.434894    1.000  0.555556  0.263158   \n",
       "1  0.605687  0.471065  0.385084  0.415047    1.000  0.888889  0.421053   \n",
       "2  0.638014  0.491945  0.400787  0.429367    0.750  0.777778  0.368421   \n",
       "3  0.661071  0.506837  0.410142  0.439581    0.250  0.555556  0.263158   \n",
       "4  0.661071  0.506837  0.406036  0.439581    0.125  0.611111  0.289474   \n",
       "\n",
       "   TSRANK30  TSRANK60      RSV5     RSV10     RSV20     RSV30     RSV60  \\\n",
       "0  0.172414  0.237288  0.993142  0.084408  0.012823  0.006939  0.002919   \n",
       "1  0.275862  0.338983  0.990467  0.195876  0.028768  0.015524  0.006520   \n",
       "2  0.241379  0.271186  0.945497  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.172414  0.237288  0.992992  0.109719  0.017577  0.009554  0.004032   \n",
       "4  0.189655  0.245763  0.992992  0.051918  0.007887  0.004268  0.001796   \n",
       "\n",
       "   IMAX5    IMAX10    IMAX20    IMAX30    IMAX60  IMIN5    IMIN10    IMIN20  \\\n",
       "0   0.50  1.000000  0.947368  0.620690  0.966102   0.75  0.333333  0.157895   \n",
       "1   0.00  1.000000  1.000000  0.655172  0.983051   1.00  0.444444  0.210526   \n",
       "2   0.25  1.000000  1.000000  0.689655  1.000000   0.75  0.555556  0.263158   \n",
       "3   0.50  0.222222  1.000000  0.724138  1.000000   1.00  0.666667  0.315789   \n",
       "4   0.75  0.333333  1.000000  0.758621  0.372881   0.25  0.777778  0.368421   \n",
       "\n",
       "     IMIN30    IMIN60  IMXD5    IMXD10    IMXD20    IMXD30    IMXD60  \\\n",
       "0  0.103448  0.711864  0.375  0.833333  0.894737  0.758621  0.627119   \n",
       "1  0.137931  0.728814  0.000  0.777778  0.894737  0.758621  0.627119   \n",
       "2  0.172414  0.745763  0.250  0.722222  0.868421  0.758621  0.627119   \n",
       "3  0.206897  0.762712  0.250  0.277778  0.842105  0.758621  0.618644   \n",
       "4  0.241379  0.779661  0.750  0.277778  0.815789  0.758621  0.296610   \n",
       "\n",
       "      CORR5    CORR10    CORR20    CORR30    CORR60     CORD5    CORD10  \\\n",
       "0  0.459666  0.945243  0.874062  0.804459  0.597788  0.782145  0.641823   \n",
       "1  0.901394  0.908289  0.879009  0.794301  0.617869  0.882316  0.780526   \n",
       "2  0.871443  0.875968  0.839218  0.807003  0.646536  0.928379  0.771765   \n",
       "3  0.827619  0.807214  0.793769  0.809557  0.665175  0.801138  0.839039   \n",
       "4  0.807913  0.790156  0.743989  0.832225  0.668057  0.769431  0.780645   \n",
       "\n",
       "     CORD20    CORD30    CORD60  CNTP5  CNTP10  CNTP20  CNTP30    CNTP60  \\\n",
       "0  0.686787  0.730910  0.674873    0.4     0.3  0.3750    0.45  0.482759   \n",
       "1  0.748364  0.762580  0.690030    0.6     0.3  0.4375    0.50  0.517241   \n",
       "2  0.714326  0.756236  0.692480    0.4     0.3  0.3750    0.50  0.482759   \n",
       "3  0.683228  0.743335  0.687432    0.4     0.3  0.3750    0.50  0.482759   \n",
       "4  0.667273  0.738520  0.707992    0.4     0.3  0.3125    0.45  0.482759   \n",
       "\n",
       "   CNTN5  CNTN10    CNTN20  CNTN30    CNTN60  CNTD5  CNTD10    CNTD20  \\\n",
       "0    0.6     0.7  0.666667     0.6  0.642857    0.4    0.30  0.354839   \n",
       "1    0.4     0.7  0.600000     0.6  0.607143    0.6    0.30  0.419355   \n",
       "2    0.6     0.7  0.666667     0.6  0.642857    0.4    0.30  0.354839   \n",
       "3    0.6     0.7  0.666667     0.6  0.642857    0.4    0.30  0.354839   \n",
       "4    0.4     0.6  0.666667     0.6  0.607143    0.5    0.35  0.322581   \n",
       "\n",
       "     CNTD30    CNTD60     SUMP5    SUMP10    SUMP20    SUMP30    SUMP60  \\\n",
       "0  0.416667  0.423077  0.647103  0.191657  0.191953  0.313155  0.279049   \n",
       "1  0.444444  0.461538  0.846179  0.312065  0.263531  0.350212  0.313790   \n",
       "2  0.444444  0.423077  0.619028  0.335872  0.193651  0.375421  0.300170   \n",
       "3  0.444444  0.423077  0.549297  0.403661  0.224506  0.370855  0.302611   \n",
       "4  0.416667  0.442308  0.590879  0.468070  0.221205  0.347337  0.335004   \n",
       "\n",
       "      SUMN5    SUMN10    SUMN20    SUMN30    SUMN60     SUMD5    SUMD10  \\\n",
       "0  0.352897  0.808343  0.808047  0.686845  0.720951  0.647103  0.191657   \n",
       "1  0.153821  0.687935  0.736469  0.649788  0.686210  0.846179  0.312065   \n",
       "2  0.380972  0.664128  0.806349  0.624579  0.699830  0.619028  0.335872   \n",
       "3  0.450703  0.596339  0.775494  0.629145  0.697389  0.549297  0.403661   \n",
       "4  0.409121  0.531930  0.778795  0.652663  0.664996  0.590879  0.468070   \n",
       "\n",
       "     SUMD20    SUMD30    SUMD60      VMA5     VMA10     VMA20     VMA30  \\\n",
       "0  0.191953  0.313155  0.279049  0.089666  0.130229  0.143430  0.117604   \n",
       "1  0.263531  0.350212  0.313790  0.039874  0.049824  0.065197  0.053897   \n",
       "2  0.193651  0.375421  0.300170  0.092726  0.094754  0.113668  0.104676   \n",
       "3  0.224506  0.370855  0.302611  0.057481  0.052237  0.068149  0.067968   \n",
       "4  0.221205  0.347337  0.335004  0.116574  0.096104  0.112468  0.120240   \n",
       "\n",
       "      VMA60     VSTD5    VSTD10    VSTD20    VSTD30    VSTD60     WVMA5  \\\n",
       "0  0.145318  0.015632  0.120173  0.191937  0.139051  0.074666  0.317027   \n",
       "1  0.068265  0.023229  0.035044  0.083730  0.059222  0.031176  0.480583   \n",
       "2  0.130952  0.045649  0.065781  0.130095  0.124264  0.066294  0.398962   \n",
       "3  0.085043  0.031984  0.018920  0.068606  0.076131  0.040626  0.297384   \n",
       "4  0.147839  0.056120  0.041191  0.111575  0.142633  0.076917  0.347976   \n",
       "\n",
       "     WVMA10    WVMA20    WVMA30    WVMA60    VSUMP5   VSUMP10   VSUMP20  \\\n",
       "0  0.373854  0.230806  0.190433  0.114214  0.291000  0.477762  0.402884   \n",
       "1  0.299247  0.212613  0.173642  0.113180  0.807726  0.357322  0.529292   \n",
       "2  0.335257  0.232749  0.182593  0.111906  0.462346  0.382043  0.331650   \n",
       "3  0.200706  0.203776  0.177890  0.111822  0.567205  0.389870  0.429412   \n",
       "4  0.265330  0.207570  0.184720  0.120504  0.498840  0.467695  0.404988   \n",
       "\n",
       "    VSUMP30   VSUMP60    VSUMN5   VSUMN10   VSUMN20   VSUMN30   VSUMN60  \\\n",
       "0  0.440317  0.481866  0.709000  0.522238  0.597116  0.559683  0.518134   \n",
       "1  0.514755  0.506248  0.192274  0.642678  0.470708  0.485245  0.493752   \n",
       "2  0.509985  0.477874  0.537654  0.617957  0.668350  0.490015  0.522126   \n",
       "3  0.535544  0.489263  0.432795  0.610130  0.570588  0.464456  0.510737   \n",
       "4  0.511253  0.453605  0.501160  0.532305  0.595012  0.488747  0.546395   \n",
       "\n",
       "     VSUMD5   VSUMD10   VSUMD20   VSUMD30   VSUMD60  month industry  season  \\\n",
       "0  0.291000  0.477762  0.402884  0.440317  0.481866      1       化工  Winter   \n",
       "1  0.807726  0.357322  0.529292  0.514755  0.506248      1       化工  Winter   \n",
       "2  0.462346  0.382043  0.331650  0.509985  0.477874      1       化工  Winter   \n",
       "3  0.567205  0.389870  0.429412  0.535544  0.489263      1       化工  Winter   \n",
       "4  0.498840  0.467695  0.404988  0.511253  0.453605      1       化工  Winter   \n",
       "\n",
       "      label  \n",
       "0  0.542538  \n",
       "1  0.445070  \n",
       "2  0.559156  \n",
       "3  0.611113  \n",
       "4  0.703479  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdcAAAHWCAYAAAAl2MNkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9JElEQVR4nO3de3zPdf/H8efGNjYz5DBUEqHIiqKl6MLkVDpIylVOHVSuIlfElUNHJSbhSmnNOSokwiTLqaWr+V1aoWvkUGMbbWabnb/v3x9d+16+tmFfn+373XeP++32vuX7/ry/n70+n/D0Ob0/XpKMAACAZbxdXQAAAJ6GcAUAwGKEKwAAFiNcAQCwGOEKAIDFCFcAACxGuAIAYDHCFQAAixGuAABYjHAFnHDo0CFFRka6ugyP9/e//10HDx5Ufn6+/u///s/V5QAXjXBFpTd48GAZY9S+fftil0dHRysuLu6Sf06vXr00efLkS15PZREWFqa3335bO3fu1NChQzVhwoQSx0ZGRsoYY2/p6ek6ePCgPv30U913333y8vJyuo6HHnpIzz33nNPfR+VU1dUFABVRy5YtZbPZSvWd3r17a+TIkXr55ZfLqCrP0rVrVxUUFGj48OHKy8u74Pjs7Gw99thjkqTq1aurSZMmuuuuu7Ry5UpFR0erX79+Sk9PL3UdDz/8sNq0aaNZs2aV+ruovAhXwAm5ubmuLqHU/P39debMGVeXcdHq16+vrKysiwpWScrPz9fSpUsd+iZOnKhx48bpzTff1Pz58zVw4MCyKBUolqHRKnMbPHiwMcaY9u3bF7s8OjraxMXFOfQdOnTIREZG2j9XrVrVTJo0yfznP/8xWVlZ5uTJk2b79u2me/fuRpKJjIw0xSn8vr+/v5k+fbo5evSoyc7ONvv37zdjxowpUku1atXMrFmzzIkTJ8zp06fNmjVrTKNGjYwxxkyePNk+bvLkycYYY6699lqzdOlSk5KSYnbv3m0kmeuvv95ERkaagwcPmqysLHP8+HETERFh6tSp4/CzCtdxzTXXmMWLF5tTp06Z5ORk88orrxhJ5vLLLzeff/65SUtLM8ePHzfPP//8Re3vKlWqmJdeeskcOHDAZGdnm0OHDpnXX3/d+Pr62scUZ/DgwSWuMzIy0qSnp5e4fOPGjaagoMBcc8019r67777brFu3ziQkJJjs7Gxz4MAB89JLLxlvb2+H//fnOnTokJFkfHx8zMsvv2x++OEHc+rUKZORkWG2bdtm7rjjDpf/nqa5vnHkCvxXUFCQLrvssiL9Pj4+F/zulClTNH78eH344Yf6/vvvVbNmTd10001q166dNm/erPfff1+NGjVSjx499Ne//rXI97/44gv95S9/UUREhP7973/rzjvv1PTp09W4cWM9//zz9nELFizQgw8+qEWLFum7775Tly5d9OWXX5ZY16effqr4+HhNmDDBft0xLCxMV199tSIjI5WYmKjWrVvriSeeUOvWrXXLLbcUWceKFSu0b98+vfjii+rTp48mTpyolJQUPfnkk9qyZYvGjRunQYMGacaMGfrXv/6l7du3n3dfffjhhxoyZIg+/fRTzZgxQx07dtSECRN07bXX6r777pMk/fWvf9UTTzyhDh062E/1fvvttxf8/1CSxYsX684771RYWJji4+MlSUOGDFFGRobCw8OVkZGhrl276tVXX1XNmjU1duxYSdLrr7+uoKAgXX755Ro9erQkKSMjQ5JUs2ZNPfbYY/r44481f/58BQYGavjw4YqKilKHDh20Z88ep+uFZ3B5wtNormyFR67nc6Ej1//7v/8za9euPe/PmT17tsPRamG7++67jTHGTJgwwaH/k08+MQUFBebqq682ksyNN95ojDEmPDzcYdxHH31U4pHr0qVLi/y8atWqFel78MEHjTHG3HbbbUXWMW/ePHuft7e3OXr0qCkoKDBjx4619wcFBZnMzEyHfVJca9u2rTHGmA8++MChf9q0acYY43DUd6Gj0bPbhcaGhIQYY4yZMWPGeffDe++9ZzIyMhyOoteuXWs/Wj27eXt7Gx8fH4e+oKAgc/z4cfPhhx+6/Pc1zbWNu4WB/3r66afVvXv3Iu1ijkBOnTql1q1bq3nz5qX+ub1791Z+fr7effddh/4ZM2bI29tbvXr1kiT17NlTkvTPf/7TYdzs2bNLXPe8efOK9GVnZ9t/7efnp8suu0zfffedJKldu3ZFxn/44Yf2X9tsNv3www/y9vZWRESEvT8tLU2//PKLrr766hJrkf7cVkkKDw936J8xY4YkqU+fPuf9vrMKjzYDAwPtfWfvhxo1auiyyy7T9u3bFRAQoFatWl1wnTabzX492MvLS7Vr11bVqlX1ww8/FLsfUblwWhj4r++//16xsbFF+lNTU1W3bt3zfnfSpElas2aN4uPjFRcXp40bN2rx4sUX9QhPkyZNdOzYMXsAFNq3b599eeF/CwoKdOjQIYdxBw4cKHHd546VpNq1a2vy5MkaOHCgGjRo4LAsKCioyPijR486fE5LS1NWVpb++OOPIv3FnVY/W+E2nFtzUlKSUlNT7dtqtRo1akiSw93C1113nV577TV17dq1yHYXtx+K8+ijj2rMmDFq1aqVfH197f2//vqrBVWjIuPIFbDA9u3b1axZMw0dOlQ//fSTHnvsMe3evVvDhw93aV1ZWVlF+j755BM9/vjjmjdvnu69916FhYXpzjvvlCR5exf9K6GgoOCi+iRd9POkf54dLz9t2rSR9L9/iAQFBWnr1q0KCQnRpEmT1LdvX3Xv3t1+rbW4/XCuQYMGaeHChTp48KCGDx+uO++8U927d9fXX399Ud+HZ+PIFbBIamqqFixYoAULFiggIEDbtm3TlClT7KdPSwqUI0eOqHv37qpRo4bD0WvhqckjR47Y/1ulShU1bdrU4civNKeia9Wqpe7du2vSpEl69dVXnVrHpSjchmuuuUb79++399evX1+1a9e2b6vVHnnkEdlsNn311VeSpDvuuEN169bVfffd53ADVtOmTYt8t6T/b/3799fBgwftN2EV4jlmSBy5ApaoU6eOw+fMzEwdOHBAfn5+Dn1S0VOO69evV9WqVTVy5EiH/tGjR8tms2nDhg2SpKioKEl/Xhs+29/+9reLrrPwiPPcI8xRo0Zd9Douxfr164v9eYV3RJ/vzmdnjRs3TnfeeadWrFhh/0dJcfvBx8enyL6V/vz/Vtxp4uLW0aFDB4WGhlpaPyomjlwBC+zdu1fffPONYmNjlZKSoptuukn9+/fXnDlz7GMKr+e+++67ioqKUkFBgVasWKG1a9dqy5Ytev3113XVVVdpz5496tGjh+655x7NnDnTfv1u9+7d+uyzzzR69Gj7TUhdunRRixYtJF3cqdb09HRt3bpVY8eOlY+PjxISEtSjR49ij9jKwo8//qgFCxboySefVK1atbR161Z16NBBQ4YM0erVq/XNN984ve6qVatq0KBBkqRq1aqpSZMmuvvuuxUSEqItW7boiSeesI/99ttvlZKSooULF+rdd9+VMUaPPPJIsae1Y2NjNXDgQPujRhkZGVq3bp3WrVun+++/X6tXr9aXX36ppk2basSIEdq7d6/9Gi8qN5ffskyjubJZMYnEhAkTzHfffWdSUlJMZmam2bt3rxk/frypWrWqfYy3t7eZNWuWSUpKMgUFBQ6P5QQEBJgZM2aY33//3eTk5Jhffvml2EkkqlevbmbPnm1OnjxpTp8+bVatWmWuueYaY4xxeDSm8DGayy67rMg6GjVqZFauXGlSUlJMamqqWbFihQkODi7xcZ5z11HSYy/F7afiWpUqVczEiRPNwYMHTU5Ojjly5EiRSSTO93OKa+dO0pGRkWF+/fVX8+mnn5r77rvPeHl5FflOaGio+fbbb01mZqb5/fffzZtvvmnCwsKMMcZ06dLFPs7f398sWbLEpKSkOEwiIcm8+OKL5tChQyYrK8vExsaa3r17m8jIyGIf3aFVrub1318AqKBCQkL073//W4MGDdKyZctcXQ4Acc0VqFCqVatWpG/UqFEqKCjQtm3bXFARgOJwzRWoQMaOHav27dsrOjpa+fn56tWrl3r37q33339fv//+u6vLA3AWl5+bptFoF9e6d+9utm/fbv744w+Tk5Nj4uPjzaRJk0yVKlVcXhuNRvtf45orAAAW45orAAAWI1wBALAYNzRdpEaNGjlM+g0AqJwCAwN17Nix844hXC9Co0aNlJCQ4OoyAABuonHjxucNWML1IhQesTZu3JijVwCoxAIDA5WQkHDBLCBcSyE9PZ1wBQBcEDc0AQBgMcIVAACLEa4AAFiMcAUAwGKEKwAAFiNcAQCwGOEKAIDFCFcAACxGuAIAYDHCFQAAixGuAABYjHAFAMBihCsAABZzabi++OKL+v7773X69GklJSVp9erVatGihcOY6OhoGWMc2nvvvecw5oorrtC6deuUmZmppKQkTZs2TVWqVHEY06VLF8XGxio7O1vx8fEaPHhwmW8fAKDyMq5qGzZsMIMHDzbXXXedadu2rVm3bp05fPiw8ff3t4+Jjo4277//vmnQoIG9BQYG2pd7e3ubH3/80WzatMmEhISYnj17muTkZPP666/bx1x11VUmIyPDTJ8+3bRq1co888wzJi8vz/To0eOi6gwMDDTGGIefS6PRaLTK10qRB64vtrDVrVvXGGPM7bffbu+Ljo42M2fOLPE7PXv2NPn5+aZ+/fr2vieffNKcOnXK+Pj4GEnmzTffNHFxcQ7f+/jjj82GDRus3pk0mtPNr0aAqVm/bpHmVyPA5bXRaLQ/28XmgVtdcw0KCpIkpaSkOPQPGjRIJ06cUFxcnN544w1Vr17dviw0NFRxcXFKTk6290VFRSkoKEitW7e2j9m8ebPDOqOiohQaGlpsHb6+vgoMDHRoQFnz86+uTgP7q8eI4fbWaWB/+flXv/CXAbiVqq4uoJCXl5feeecd7dixQz///LO9f9myZTpy5IiOHTumtm3b6q233lLLli11//33S5KCg4OVlJTksK7Cz8HBwecdExQUpGrVqik7O9th2fjx4zVlyhSrNxG4oIBaQapZr+4lrcOvRkCxgZxzJks5GZmXtG4AF8dtwnXu3Llq06aNbrvtNof++fPn23/9008/6fjx49qyZYuuvvpq/frrr2VSy9SpUxUeHm7/HBgYqISEhDL5WYDVCo+AA2oF2fsyT6Vp5/LPCFegnLhFuM6ePVt9+/ZV586dLxhiu3btkiQ1b95cv/76qxITE9WhQweHMQ0aNJAkJSYm2v9b2Hf2mLS0tCJHrZKUm5ur3Nxcp7cHKA/FHaHa8gvk7V3FkiNgAM5zebjOnj1b9957r+644w4dPnz4guNvuOEGSdLx48clSTExMfrHP/6hevXq6cSJE5KksLAwpaWlae/evfYxvXv3dlhPWFiYYmJirNsQoJwVd4SafPio4jZHu7AqAJKLw3Xu3Ll6+OGH1a9fP6Wnp9uPLguPKK+++mo9/PDDWr9+vf744w+1bdtWM2fO1NatWxUXFydJ2rRpk/bu3avFixdr7NixCg4O1muvvaa5c+fajz7nzZunkSNH6q233tJHH32krl27asCAAerTp4/Lth2wwrlHqBkpqS6sBkAhl94t/PTTT6tWrVraunWrEhMT7e3BBx+U9Ofp2e7du2vTpk3av3+/ZsyYoZUrV+quu+6yr8Nms6lv374qKChQTEyMlixZokWLFmnSpEn2MYcPH1afPn0UFhamPXv2aMyYMXrssce0adOmct9mAIDnc+mRq5eX13mX//7777rjjjsuuJ6jR49e8Ch069atateuXWnKA8rE+a6VAvAMLr/mClQ2pb1WarPZyqs0ABYhXAEXuNhrpTmZZ2QKbKpZ3/HOX450AfdGuAJuLC8nR77Vq+mW/vdwVzBQgRCuQAXAXcFAxeJWcwsDAOAJCFcAACxGuAIAYDHCFQAAixGuAABYjHAFAMBihCsAABYjXAEAsBjhCgCAxQhXAAAsRrgCAGAxwhUAAIsRrgAAWIy34gBlxK9GgPz8qzv08R5WoHIgXIEy4udfXZ0G9uc9rEAlRLgCZYj3sAKVE9dcAQCwGOEKAIDFCFcAACxGuAIAYDHCFQAAixGuAABYjHAFAMBihCsAABYjXAEAsBjhClQSNpvN1SUAlQbTHwKVQE7mGZkCm2rWr+vYfyZLORmZLqoK8FyEK1AJ5OXkyLd6Nd3S/x77iwQyT6Vp5/LPCFegDBCuQCVy7osEAJQNrrkCAGAxwhUAAIsRrgAAWIxwBQDAYoQrAAAWI1wBALAY4QoAgMUIVwAALEa4AgBgMcIVAACLEa4AAFiMcAUAwGKEKwAAFiNcAQCwGOEKAIDFCFcAACxGuAIAYDHCFQAAixGuAABYjHAFKjGbzebqEgCPVNXVBQBwjZzMMzIFNtWsX7fosjNZysnIdEFVgGcgXIFKKi8nR77Vq+mW/vcooFaQvT/zVJp2Lv+McAUuAeEKVHIBtYJUs17Ro1cAzuOaKwAAFiNcAQCwGOEKAIDFCFcAACxGuAIAYDHCFQAAi7k0XF988UV9//33On36tJKSkrR69Wq1aNHCYYyfn5/mzJmjkydPKj09XZ999pnq16/vMOaKK67QunXrlJmZqaSkJE2bNk1VqlRxGNOlSxfFxsYqOztb8fHxGjx4cJlvHwCgcnJpuHbp0kVz587VLbfcorCwMPn4+GjTpk3y9/e3j5k5c6buuusuPfDAA+rSpYsaNWqkVatW2Zd7e3vryy+/lK+vr2699VYNHjxYQ4YM0SuvvGIfc9VVV+nLL79UdHS0brjhBr3zzjv68MMP1aNHj3LdXgBA5eDSSSR69erl8HnIkCE6ceKE2rdvr+3bt6tmzZoaPny4Hn74YUVHR0uShg4dqv3796tjx47atWuXevTooeuuu07du3dXcnKy9uzZo4kTJ+qtt97SlClTlJeXpxEjRujQoUP6+9//Lknav3+/brvtNo0ePVqbNm0q9+0GAHg2t7rmGhT05xRsKSkpkqT27dvL19dXmzdvto/55ZdfdOTIEYWGhkqSQkNDFRcXp+TkZPuYqKgoBQUFqXXr1vYxZ6+jcEzhOs7l6+urwMBAhwaUxK9GgGrWr+vQatSpLW/vKhf+MgCP5DbTH3p5eemdd97Rjh079PPPP0uSgoODlZOTo7S0NIexSUlJCg4Oto9JSkoqsrxw2fnGBAUFqVq1asrOznZYNn78eE2ZMsWybYNn8/Ovrk4D+zvMz5t8+KjiNke7sCoAruQ2R65z585VmzZtNHDgQFeXoqlTp6pmzZr21rhxY1eXBDdXOD9vYfOvydkOoDJziyPX2bNnq2/fvurcubMSEhLs/YmJifLz81NQUJDD0WuDBg2UmJhoH9OhQweH9TVo0MC+rPC/hX1nj0lLSyty1CpJubm5ys3NtWbjAACVjsuPXGfPnq17771XXbt21eHDhx2WxcbGKjc3V926dbP3tWjRQk2aNFFMTIwkKSYmRtdff73q1atnHxMWFqa0tDTt3bvXPubsdRSOKVwHAABWcumR69y5c/Xwww+rX79+Sk9Ptx9dFh5Rnj59WhEREQoPD1dKSopOnz6t2bNn69tvv9WuXbskSZs2bdLevXu1ePFijR07VsHBwXrttdc0d+5c+9HnvHnzNHLkSL311lv66KOP1LVrVw0YMEB9+vRx2bYDADyXS49cn376adWqVUtbt25VYmKivT344IP2MaNHj9a6deu0cuVKbdu2TYmJibrvvvvsy202m/r27auCggLFxMRoyZIlWrRokSZNmmQfc/jwYfXp00dhYWHas2ePxowZo8cee4zHcAAAZcKlR65eXl4XHJOTk6ORI0dq5MiRJY45evToBY9Ct27dqnbt2pW6RgAASsvl11wBAPA0hCsAABYjXAEAsBjhCgCAxQhXAAAs5hYzNAEVhV+NAPn5V7d/tuUXMEE/gCIIV6AUzp2knwn6ARSHcAVKqXCSfknKSEl1cTUA3BHXXAEAsBjhCgCAxQhXAAAsRrgCAGAxwhUAAIsRrgAAWIxwBQDAYoQrAAAWI1wBALAY4QoAgMUIVwAALEa4AgBgMcIVQBE2m83VJQAVGm/FAeAgJ/OMTIFNNevXLbrsTJZyMjJdUBVQsRCuABzk5eTIt3o13dL/Hvt7ayUp81Sadi7/jHAFLgLhCqBYZ7+3FkDpcM0VAACLEa4AAFiMcAUAwGKEKwAAFiNcAQCwGOEKAIDFCFcAACxGuAIAYDHCFQAAixGuAABYjHAFAMBihCsAABYjXAEAsBjhCgCAxQhXAAAsRrgCAGAxwhUAAIsRrgAAWIxwBQDAYoQrAAAWI1wBALAY4QoAgMUIVwAALEa4AgBgMcIVAACLEa4AAFisqqsLANyRX40A+flXd+iz5RfI27uKiyoCUJEQrkAx/Pyrq9PA/gqoFWTvSz58VHGbo11YlevZbDZXlwBUCIQrUIKAWkGqWa+u/XNGSqoLq3G9nMwzMgU21axft+iyM1nKych0QVWAeyJcAVyUvJwc+Vavplv63+NwRJ95Kk07l39GuAJnIVwBlMq5R/QAiuJuYQAALEa4AgBgMcIVAACLEa4AAFiMcAUAwGJOhWvTpk2trgMAAI/hVLgeOHBAW7Zs0aBBg+Tn52d1TQAAVGhOhWu7du30448/Kjw8XImJiZo3b55uvvnmUq/n9ttv1xdffKGEhAQZY9SvXz+H5ZGRkTLGOLQNGzY4jKldu7aWLFmitLQ0paam6sMPP1RAQIDDmOuvv17btm1TVlaWjh49qhdeeKH0Gw0AwEVyKlz37NmjUaNGqVGjRho2bJgaNmyoHTt2KC4uTqNHj1bduhf3gHlAQID27NmjZ555psQxGzZsUHBwsL099NBDDsuXLl2q1q1bKywsTH379lXnzp31wQcf2JcHBgZq06ZNOnLkiNq3b68XXnhBU6ZM0eOPP+7MpgMAcEGXdENTQUGBVq9erQceeEDjxo1T8+bNNX36dP32229auHChgoODz/v9jRs3auLEifr8889LHJOTk6OkpCR7O3XqlH1Zq1at1KtXLz322GP6/vvvtXPnTv3tb3/TwIED1bBhQ0nSoEGD5Ovrq2HDhmnv3r1asWKF3n33XT3//POXsukAAJToksK1ffv2mjt3ro4fP67nn39e06dPV7NmzRQWFqZGjRppzZo1l1zgHXfcoaSkJO3fv1///Oc/VadOHfuy0NBQpaamKjY21t63efNm2Ww2dezY0T5m27ZtysvLs4+JiopSq1atVKtWrWJ/pq+vrwIDAx0aAAAXy6m5hUePHq2hQ4eqZcuWWr9+vR599FGtX79exhhJ0uHDhzVkyBAdPnz4korbuHGjVq1apUOHDqlZs2Z64403tGHDBoWGhspmsyk4OFjJyckO3ykoKFBKSor9qDk4OFiHDh1yGJOUlGRfdvaRcKHx48drypQpl1Q7AKDycipcn3rqKX300UdasGCBEhMTix2TnJys4cOHX1JxK1assP/6p59+0o8//qhff/1Vd9xxh7Zs2XJJ6z6fqVOnKjw83P45MDBQCQkJZfbzAACexalwbdGixQXH5OXladGiRc6svkSHDh3SiRMn1Lx5c23ZskWJiYmqX7++w5gqVaqoTp069tBPTExUgwYNHMYUfi7pHwa5ubnKzc21tHYAQOXh1DXXIUOGqH///kX6+/fvr0cfffSSiypJ48aNddlll+n48eOSpJiYGNWuXVvt2rWzj+natau8vb21a9cu+5jOnTuratX//TsiLCxM+/fvL/aUMAAAl8qpcB0/frxOnjxZpD85OVkTJky46PUEBAQoJCREISEhkv6c+SkkJERXXHGFAgICNG3aNHXs2FFNmjRR165dtWbNGh04cEBRUVGSpP3792vDhg2aP3++br75Zt16662aM2eOli9fbg/gZcuWKTc3VxEREbruuus0YMAAPffccw6nfQEAsJJTp4WvvPLKIjcJSdKRI0d05ZVXXvR6brrpJn3zzTf2zzNnzpQkLViwQE899ZTatm2rwYMHq1atWjp27Jg2bdqkiRMnOpyyHTRokObMmaOvv/5aNptNK1eu1LPPPmtffvr0afXo0UNz585VbGysTp48qVdeeUXz5893YsvhafxqBMjPv7pDny2/QN7eVVxUEQBP4FS4Jicnq23btjpy5IhDf0hIiP7444+LXs/WrVvl5eVV4vKePXtecB2pqakaNGjQecfExcWpc+fOF10XKg8//+rqNLC/AmoF2fuSDx9V3OZoF1YFoKJzKlw//vhjvfvuu0pPT9e2bdskSV26dNGsWbO0fPlySwsEylpArSDVrPe/WcUyUlJdWA0AT+BUuE6cOFFXXXWVvv76a+Xn50uSvL29tWjRolJdcwUAwBM5Fa55eXkaOHCgJk6cqJCQEGVlZSkuLk5Hjx61uj4AACocp8K1UHx8vOLj462qBQAAj+BUuHp7e2vIkCHq1q2b6tevL29vxyd6unXrZklxAABURE6F66xZszRkyBB9+eWX+umnn+xzCgMAACfDdeDAgRowYECRF5cDqJxsNpurSwDcilPhmpubqwMHDlhdC4AKKCfzjEyBTTXr1y267EyWcjIyXVAV4FpOheuMGTP03HPPaeTIkVbXA6CCycvJkW/1arql/z0Ok3FknkrTzuWfEa6olJwK19tuu01/+ctf1KtXL/38888OLyKXpPvvv9+S4gBUHOdOxgFUZk6F66lTp7R69WqrawEAwCM4Fa7Dhg2zug4AADyGU6+ck/58KXm3bt30xBNPqEaNGpKkhg0bKiAgwLLiAACoiJx+5dzGjRt15ZVXys/PT1999ZUyMjI0btw4+fn56amnnrK6TgAAKgynjlxnzZqlH374QbVr11ZWVpa9f/Xq1czOBACo9Jw6cr399tt16623FrlL+PDhw2rcuLElhQEAUFE5deTq7e2tKlWqFOm//PLLlZ6efslFAQBQkTkVrps2bdKoUaPsn40xCggI0Msvv6z169dbVRsAABWSU6eFx4wZo6ioKP3888+qVq2ali1bpmuuuUYnT57UQw89ZHWNAABUKE6Fa0JCgkJCQjRw4EC1bdtWNWrUUEREhJYuXars7GyrawQAoEJx+mXpBQUFWrp0qZYuXWplPQAAVHhOhesjjzxy3uWLFy92qhgAnoVX0aGycvpl6Wfz8fGRv7+/cnNzdebMGcIVAK+iQ6XmVLjWqVOnSF/z5s313nvv6e23377kogBUfLyKDpWZ09dcz3XgwAG9+OKLWrJkia699lqrVgtYwq9GgPz8qzv02fIL5O1d9HltWItX0aEysixcJSk/P1+NGjWycpWAJfz8q6vTwP4OR1DJh48qbnO0C6sC4KmcCte77rrL4bOXl5caNmyokSNHaufOnZYUBljt3COojJRUF1YDwJM5Fa6ff/65w2djjE6cOKEtW7ZozJgxVtQFAECF5VS4FjevMAAA+JPTL0sHAADFc+rIdcaMGRc9ltPEAIDKxqlwvfHGG3XjjTfKx8dHv/zyiySpRYsWKigo0O7du+3jjDHWVAkAQAXiVLiuXbtW6enpGjx4sE6dOiVJqlWrliIjI7V9+3aFh4dbWSMAABWKU9dcx4wZo/Hjx9uDVZJOnTqll156idPAAIBKz6lwrVmzpurVq1ekv169egoMDLzkogAAqMicCtfVq1crMjJS9957rxo3bqzGjRvrvvvuU0REhFatWmV1jQAAVChOXXMdMWKEpk+frmXLlsnHx0fSn1MfRkRE6IUXXrC0QAAAKhqnwjUrK0vPPPOMXnjhBTVr1kySdPDgQZ05c8bS4gAAqIguaRKJhg0bqmHDhoqPjydYAQD4L6fCtU6dOtq8ebP+85//aP369WrYsKEkKSIiQtOnT7e0QAAAKhqnwnXmzJnKy8vTlVde6XDEumLFCvXs2dOy4gB4JpvN5uoSgDLl1DXXHj166M4771RCQoJDf3x8vJo0aWJJYQA8U07mGZkCm2rWL/oC9ZwzWcrJyHRBVYC1nArXgICAYq+x1qlTRzk5OZdcFADPlZeTI9/q1XRL/3scXl6feSpNO5d/RrjCIzh1Wnj79u169NFH7Z+NMfLy8tLYsWMVHR1tWXEAPFfhy+sL29lBC1R0Th25jh07Vl9//bVuuukm+fr6atq0aWrdurXq1KmjTp06WV0jAAAVilNHrj///LNatGihHTt2aM2aNQoICNCqVat044036tdff7W6RgAAKpRSH7lWrVpVGzdu1IgRI/TGG2+URU0AAFRopT5yzc/PV9u2bcuiFgAAPIJTp4WXLFmi4cOHW10LAAAewakbmqpWraphw4ape/fuio2NVWam463zvNMVAFCZlSpcmzZtqsOHD6tNmzbavXu3JKlFixYOY4wx1lUHAEAFVKpwjY+PV8OGDdW1a1dJ0vLly/Xss88qOTm5TIoDAKAiKlW4enl5OXzu1auXAgICLC0IuBR+NQLk51/doc+WXyBv7youqghAZeTUNddC54Yt4Gp+/tXVaWB/h9l+kg8fVdxmZg4DUH5KFa7GmCLXVLnGCndTOK1eoYyUVBdWA6AyKvVp4QULFtgn569WrZrmzZtX5G7h+++/37oKAQCoYEoVrgsXLnT4vGTJEkuLAQDAE5QqXIcNG1ZWdQAA4DGcmqEJAACUjHAFAMBihCsAABYjXAEAsJhLw/X222/XF198oYSEBBlj1K9fvyJjXn75ZR07dkxnzpzRV199pebNmzssr127tpYsWaK0tDSlpqbqww8/LDJr1PXXX69t27YpKytLR48e1QsvvFCm2wUAqNxcGq4BAQHas2ePnnnmmWKXjx07Vs8++6xGjBihjh07KjMzU1FRUfLz87OPWbp0qVq3bq2wsDD17dtXnTt31gcffGBfHhgYqE2bNunIkSNq3769XnjhBU2ZMkWPP/54mW8fAKByuqTpDy/Vxo0btXHjxhKXjxo1Sq+99pq++OILSdKjjz6qpKQk3XPPPVqxYoVatWqlXr166aabblJsbKwk6W9/+5vWr1+vv//97zp+/LgGDRokX19fDRs2THl5edq7d69uuOEGPf/885o/f365bCcAoHJx22uuTZs2VcOGDbV582Z73+nTp7Vr1y6FhoZKkkJDQ5WammoPVknavHmzbDabOnbsaB+zbds25eXl2cdERUWpVatWqlWrVrE/29fXV4GBgQ4NQNmz2WyuLgGwhNuGa3BwsCQpKSnJoT8pKcm+LDg4uMjr7goKCpSSkuIwprh1nP0zzjV+/HidPn3a3hISEi59gwCcV07mGZkCm2rWr1uk+dXg7VuoWFx6WthdTZ06VeHh4fbPgYGBBKyb4dVynicvJ0e+1avplv73OLzVKPNUmnYu/0w5GZnn+TbgXtw2XBMTEyVJDRo0sP+68PO///1v+5j69es7fK9KlSqqU6eO/TuJiYlq0KCBw5jCz2ev92y5ubnKzc21ZDtQNni1nOc6961GQEXktqeFDx06pOPHj6tbt272vsDAQHXs2FExMTGSpJiYGNWuXVvt2rWzj+natau8vb21a9cu+5jOnTuratX//TsiLCxM+/fv16lTp8pnY1AmCv8SLmz+Nbk2DsA9uPxRnJCQEIWEhEj68yamkJAQXXHFFZKkd955Ry+99JLuuusutWnTRosWLdKxY8f0+eefS5L279+vDRs2aP78+br55pt16623as6cOVq+fLmOHz8uSVq2bJlyc3MVERGh6667TgMGDNBzzz3ncNoXAAArufS08E033aRvvvnG/nnmzJmSpAULFmjo0KGaNm2aAgIC9MEHH6hWrVrasWOHevbsaX+frCQNGjRIc+bM0ddffy2bzaaVK1fq2WeftS8/ffq0evTooblz5yo2NlYnT57UK6+8wmM4AIAy49Jw3bp1q7y8vM47ZvLkyZo8eXKJy1NTUzVo0KDzriMuLk6dO3d2qkYAAErLba+5AgBQURGuAABYjHAFAMBihCsAABYjXAEAsBjhCgCAxQhXAG6Pt+WgonHbuYUBQHJ8W45D/5ksJvOH2yJcAbi14t6Ww5ty4O4IVwAVAm/LQUXCNVcAACxGuAIAYDHCFQAAixGuAABYjHAFAMBihCsAABYjXAEAsBjhCgCAxQhXAAAsRrgCAGAxwhUAAIsxtzDcml+NAPn5V3fos+UXyNu7iosqAoALI1zh1vz8q6vTwP72t6FIUvLho4rbHO3CqgDg/AhXuL1z34aSkZLqwmoA4MK45goAgMUIVwAALEa4AqiQbDabq0sASsQ1VwAVTk7mGZkCm2rWr1t02Zks5WRkuqAq4H8IVwAVTl5OjnyrV9Mt/e9xuJM881Sadi7/jHCFyxGuACqsc+8klzhdDPdAuALwGJwuhrsgXAF4DE4Xw10QrgA8TnGni4HyxKM4AABYjHAFAMBihCsAABYjXAEAsBjhCgCAxQhXAAAsRrgCAGAxwhUAAIsRrgAqBeYcRnlihiYAHo85h1HeCFcAHo85h1HeCFe4Bb8aAfLzr+7QZ8svkLd3FRdVBE/EnMMoL4Qr3IKff3V1Gtjf4agi+fBRxW2OdmFVAOAcwhVu49yjioyUVBdWAwDO425hAAAsRrgCAGAxwhUAAIsRrgAAWIxwBQDAYoQrgEqNaRFRFngUp5wVN1mCVLmmYDt3HzBZBFyFaRFRVgjXclbcZAmVbQq2c/cBk0XAVZgWEWWFcHUBpmBz3AdMFgFX488krMY1VwAALMaRKwDAo7jDvS2EKwDAo7jDvS2EKwDA47j6OjrXXAEAsBjhCgCAxdw6XCdPnixjjEPbt2+ffbmfn5/mzJmjkydPKj09XZ999pnq16/vsI4rrrhC69atU2ZmppKSkjRt2jRVqcKEBQCAsuP211x/+uknde/e3f45Pz/f/uuZM2eqT58+euCBB5SWlqY5c+Zo1apVuu222yRJ3t7e+vLLL5WYmKhbb71VDRs21KJFi5SXl6d//OMf5b4tAIDKwe3DNT8/X0lJSUX6a9asqeHDh+vhhx9WdPSfs/sMHTpU+/fvV8eOHbVr1y716NFD1113nbp3767k5GTt2bNHEydO1FtvvaUpU6YoLy+vvDcHAFAJuPVpYUm65pprlJCQoIMHD2rJkiW64oorJEnt27eXr6+vNm/ebB/7yy+/6MiRIwoNDZUkhYaGKi4uTsnJyfYxUVFRCgoKUuvWrUv8mb6+vgoMDHRoAABcLLcO1127dmnIkCHq2bOnnnrqKTVt2lTbt29XjRo1FBwcrJycHKWlpTl8JykpScHBwZKk4ODgIke9hZ8LxxRn/PjxOn36tL0lJCRYvGUAAE/m1qeFN27caP91XFycdu3apSNHjmjAgAHKysoqs587depUhYeH2z8HBgYSsE4obpYU3oCDioJX0eFSuHW4nistLU3/+c9/1Lx5c3311Vfy8/NTUFCQw9FrgwYNlJiYKElKTExUhw4dHNbRoEED+7KS5ObmKjc3twy2oHIpbpYU3oCDioBX0eFSVahwDQgIULNmzbR48WLFxsYqNzdX3bp106pVqyRJLVq0UJMmTRQTEyNJiomJ0T/+8Q/Vq1dPJ06ckCSFhYUpLS1Ne/fuddl2VCbnzpLCG3BQEfAqOlwqtw7Xt99+W2vXrtWRI0fUqFEjvfzyyyooKNDHH3+s06dPKyIiQuHh4UpJSdHp06c1e/Zsffvtt9q1a5ckadOmTdq7d68WL16ssWPHKjg4WK+99prmzp3LkSmAC3L1FHqouNw6XC+//HJ9/PHHuuyyy3TixAnt2LFDt9xyi06ePClJGj16tGw2m1auXCk/Pz9FRUXp6aeftn/fZrOpb9++eu+99xQTE6PMzEwtXLhQkyZNctUmAQAqAbcO14ceeui8y3NycjRy5EiNHDmyxDFHjx5Vnz59rC4NAIASufWjOAAAVESEKwAAFiNcAQCwGOEKAKXA5BK4GG59QxMAuBMml8DFIlwB4CIxuQQuFuEKAKXE5BK4EK65AgBgMY5cccl4+w0AOCJcccl4+w0AOCJcYQnefgMA/8M1VwAALEa4AgBgMcIVAACLEa4AAFiMcAUAwGKEKwBYgAn9cTYexcFFY7IIoHjFTehvs9lUkJcvHz/fouOZ5N/jEa64aEwWARSvuAn9C/9sMMl/5US4olSYLAIo2dl/Pgr/bDDJf+XENVcAACxGuAIAYDHCFQAAixGuAABYjHAFgHLGM7Gej7uFAaAcFfdMrH0Zz796DMIVAMpRcc/ESjz/6mkIVxTBTExA2eP5V89GuKIIZmICXINrsZ6DcEWxmIkJKF9ci/UshCsAuAGuxXoWwrUS49oq4H64FusZCNdKjGurQMXAtdiKh3Ct5Li2Crg3rsVWTIRrJcDpX6Di4lpsxUS4VgKc/gUqPq7FViyEayXB6V8AKD+EKwB4mOIuBUlcoy1PhCsAeJjiLgVxjbZ8Ea4AUEGd7xEdrtG6FuEKABVQSY/o8CSAeyBcPcy511r4gwZ4ppIe0TnfkwBMRlF+CFcPc+61Fh65ATzbxT4JwGQU5Ytw9UBn/2HjkRsAEpNRlDfCtYJi1iUAzuBGp/JBuFZQzLoEAO6LcK3AmHUJgBW40cl6hKub4/QvgLJU3I1ONptNBXn58vHzLTI+O/OMcjPPlGeJFRLh6uY4/QugLBV3o1Ph3zHn3vyUcixRu1auUbUA/yLrIXQdEa5uojQzrXD6F4DVinvKoLi/e4q747ik0D3fEbCnP/5DuLoBZloBUJFcbOiWdARcGR7/IVzdgDMzrQCAuynpLFtlfPyHcHUjnP4FUFl4+h3KhCsAoFyVeCnMg67REq4AgHJ1oUthnnCNlnAFALhEaa7RVrTTyIQrAMCtne+NPu76fC3hCgBwayWdRi7u+Vp3eYSRcAUAVAgX83ytuzzCSLgCACo0d3yHtberCwAAwNMQrgAAWKxShevTTz+tQ4cOKSsrS999951uvvlmV5cEAPBAlSZcBwwYoPDwcL388stq166d9uzZo6ioKNWrV8/VpQEAPEylCdfnn39e8+fP14IFC7Rv3z6NGDFCZ86c0bBhw1xdGgDAw1SKu4V9fHzUvn17TZ061d5njNHmzZsVGhpaZLyvr6/8/PzsnwMDAx3+eykCa9SQLStHeafT/9eZm192/WW57spUI7W7V7871VKRa6xEtduychRYo4ZMVo4uxcXmQKUI17p166pq1apKSkpy6E9KSlKrVq2KjB8/frymTJlSpD8hIaGsSgQAlLVpMy1bVWBgoNLT00tcXinCtbSmTp2q8PBwh746deooJSXlktYbGBiohIQENW7c+Lz/Uyob9kvJ2DfFY7+UjH1TPCv3S2BgoI4dO3beMZUiXE+ePKn8/Hw1aNDAob9BgwZKTEwsMj43N1e5ubkOfVb+Jk1PT+c3fTHYLyVj3xSP/VIy9k3xrNgvF/P9SnFDU15enmJjY9WtWzd7n5eXl7p166aYmBgXVgYA8ESV4shVksLDw7Vw4UL98MMP+v777zVq1CgFBAQoMjLS1aUBADyQqSztmWeeMYcPHzbZ2dnmu+++Mx06dCjXn+/r62smT55sfH19Xb4v3KmxX9g37Bf2jaftF6///gIAAFikUlxzBQCgPBGuAABYjHAFAMBihCsAABYjXC1W2tfa9e/fX/v27VNWVpZ+/PFH9erVq5wqLV+l2S+PPfaYtm3bppSUFKWkpOirr77y6NcDOvsqxAcffFDGGK1evbqMK3SN0u6XoKAgzZkzR8eOHVN2drZ++eUX/jz913PPPaf9+/frzJkzOnr0qMLDwx3mT/cEt99+u7744gslJCTIGKN+/fpd8DtdunRRbGyssrOzFR8fr8GDB1tak8tvkfaUNmDAAJOdnW2GDBlirr32WvP++++blJQUU69evWLHh4aGmry8PPP3v//dtGrVyrzyyismJyfHtG7d2uXb4sr9smTJEvPUU0+ZkJAQ07JlS/PRRx+Z1NRU06hRI5dvi6v3TWFr0qSJ+e2338zWrVvN6tWrXb4drt4vPj4+5vvvvzfr1q0zt956q2nSpInp3Lmzadu2rcu3xdX75qGHHjJZWVnmoYceMk2aNDFhYWEmISHBzJgxw+XbYmXr2bOnefXVV80999xjjDGmX79+5x1/1VVXmYyMDDN9+nTTqlUr88wzz5i8vDzTo0cPq2py/U7xlPbdd9+Z2bNn2z97eXmZ33//3YwbN67Y8cuXLzdr16516IuJiTHvvfeey7fFlfvl3Obt7W3S0tLMI4884vJtcYd94+3tbXbs2GGGDRtmIiMjPTJcS7tfnnzySXPgwAFTtWpVl9fubvtm9uzZZvPmzQ5906dPN9u3b3f5tpRVu5hwffPNN01cXJxD38cff2w2bNhgSQ2cFrZI4WvtNm/ebO8732vtJCk0NNRhvCRFRUWVOL4icma/nMvf318+Pj6X/OIEd+Psvpk0aZKSk5P10UcflUeZ5c6Z/XL33XcrJiZGc+fOVWJiouLi4jR+/Hh5e3vWX3HO7Jtvv/1W7du3t586btq0qXr37q3169eXS83uqqz//q000x+WtdK+1k6SgoODix0fHBxcZnWWN2f2y7neeustHTt2rMgfhIrOmX3TqVMnDR8+XDfccEM5VOgazuyXq6++Wl27dtXSpUvVu3dvNW/eXP/85z/l4+OjV155pTzKLhfO7JuPP/5YdevW1Y4dO+Tl5SUfHx+99957Du+3roxK+vs3KChI1apVU3Z29iWt37P+WQePM27cOA0cOFD33nuvcnIu7SXHFV2NGjW0ePFiPf744/rjjz9cXY5b8fb2VnJysp544gnt3r1bn3zyiV5//XWNGDHC1aW5XJcuXTRhwgQ9/fTTateune6991716dNHL730kqtL82gcuVqktK+1k6TExMRSja+InNkvhcaMGaMXX3xR3bt3V1xcXFmW6RKl3TfNmjVT06ZNtXbtWntf4WnPvLw8tWzZUr/++mvZFl0OnPk9c/z4ceXl5clms9n79u3bp4YNG8rHx0d5eXllWnN5cWbfvPrqq1q8eLEiIiIkST/99JMCAgL0wQcf6PXXX5cxpszrdkcl/f2blpZ2yUetEkeulnHmtXYxMTEO4yUpLCzMo16D5+zr/l544QVNnDhRPXv2VGxsbHmUWu5Ku2/279+vNm3a6IYbbrC3L774QtHR0brhhhv022+/lWf5ZcaZ3zM7d+5U8+bN5eXlZe9r0aKFjh075jHBKjm3b/z9/R3+0SFJBQUF9u9WVuXx96/L7+zylDZgwACTlZVlHn30UdOqVSszb948k5KSYurXr28kmYULF5o33njDPj40NNTk5uaa559/3rRs2dJMnjzZYx/FKc1+GTt2rMnOzjb33XefadCggb0FBAS4fFtcvW/ObZ56t3Bp98vll19u0tLSzLvvvmuuueYa07t3b5OYmGgmTJjg8m1x9b6ZPHmySUtLMw8++KC56qqrTPfu3U18fLxZvny5y7fFyhYQEGBCQkJMSEiIMcaYUaNGmZCQEHPFFVcYSeaNN94wCxcutI8vfBTnrbfeMi1btjRPPfUUj+K4czvfa+2io6NNZGSkw/j+/fub/fv3m+zsbBMXF2d69erl8m1w9X45dOiQKc7kyZNdvh2u3jfnNk8NV2f2yy233GJiYmJMVlaWOXDggBk/frzx9vZ2+Xa4et9UqVLFTJo0ycTHx5szZ86YI0eOmDlz5pigoCCXb4eVrUuXLsX+vVG4LyIjI010dHSR7+zevdtkZ2ebAwcOmMGDB1tWD6+cAwDAYlxzBQDAYoQrAAAWI1wBALAY4QoAgMUIVwAALEa4AgBgMcIVAACLEa4AAFiMcAVwQdHR0Zo5c6arywAqDMIV8HBffPGFNmzYUOyy2267TcYYXX/99eVcFeDZCFfAw0VERCgsLEyNGzcusmzo0KH617/+5ZGv9ANciXAFPNy6det04sQJDRkyxKE/ICBADzzwgD7//HMtW7ZMv//+uzIzM/Xjjz9q4MCB512nMUb9+vVz6EtNTdXgwYPtny+//HKtWLFCqamp+uOPP/T555+rSZMmlm0X4M4IV8DDFRQUaNGiRUXC9YEHHlCVKlW0ZMkSxcbGqk+fPmrTpo0++OADLV68WDfffLPTP7Nq1aqKiopSenq6br/9dnXq1EkZGRnauHGjfHx8LnGLgIrB5a8KotFoZdtatmxpjDGmS5cu9r6tW7eaRYsWFTt+7dq15u2337Z/jo6ONjNnzrR/NsaYfv36OXwnNTXV/squQYMGmX379jks9/HxMZmZmSYsLMzl+4NGK+vGkStQCfzyyy/auXOnhg0bJklq1qyZOnfurIiICHl7e+ull17Sjz/+qD/++EPp6em68847deWVVzr980JCQtS8eXOlp6fbW0pKiqpVq6ZmzZpZtVmA26rq6gIAlI+IiAjNnj1bzzzzjIYOHaoDBw5o69atGjdunJ577jmNGjVKcXFxyszM1DvvvCNfX98S12Wz2eTl5eXQd/bp3ho1aig2NlaDBg0q8t0TJ05Yt1GAmyJcgUrik08+0axZs/Twww/r0Ucf1XvvvSdJ6tSpk9asWaOlS5dKkry8vNSiRQvt3bu3xHWdOHFCDRs2tH9u3ry5AgIC7J93796tBx98UMnJyUpPTy+jLQLcF6eFgUoiMzNTK1as0NSpU9WwYUMtWLBAkhQfH6+wsDCFhoaqVatWev/999WgQYPzrmvLli0aOXKkbrjhBrVv317z5s1Tbm6uffnSpUt18uRJrVmzRrfddpuuuuoqdenSRbNmzSr2kSDA0xCuQCUSERGhOnXqKCoqSsePH5ckvfbaa9q9e7eioqL0zTffKDExUZ9//vl51zNmzBj99ttv2r59u5YtW6bp06frzJkz9uVZWVnq3Lmzjh49qlWrVmnfvn2KiIhQtWrVdPr06bLcRMAteOnPO5sAAIBFOHIFAMBihCsAABYjXAEAsBjhCgCAxQhXAAAsRrgCAGAxwhUAAIsRrgAAWIxwBQDAYoQrAAAWI1wBALDY/wNna7cSye57YwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature_config = {\n",
    "#     \"target_features\": ['label'],\n",
    "#     \"numeric_features\": ['turnover_rate', 'pe_ttm', 'ps_ttm', 'pcf_ncf_ttm', 'pb_mrq', 'KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60'],\n",
    "#     \"integer_categorical_features\": ['month'],\n",
    "#     \"string_categorical_features\": ['industry', 'season'],\n",
    "# }\n",
    "plot_series_dist(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始将DataFrame转换为DataSet...\n"
     ]
    }
   ],
   "source": [
    "# 转换为tensorflow所使用的dataset\n",
    "print(\"开始将DataFrame转换为DataSet...\")\n",
    "train_ds = df_to_dataset(train_df, feature_columns, label_columns, shuffle=True, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val_df, feature_columns, label_columns, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test_df, feature_columns, label_columns, shuffle=False, batch_size=batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.3615577 ]\n",
      " [0.12815645]\n",
      " [0.25016917]\n",
      " ...\n",
      " [0.19307439]\n",
      " [0.61549733]\n",
      " [0.39409232]], shape=(1024, 1), dtype=float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 02:44:19.827356: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for feas, label in train_ds.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型初始化 & 训练...\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 05:06:46.507021: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14b75c1d5880 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-30 05:06:46.507173: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2024-05-30 05:06:46.546793: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-30 05:06:46.696585: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 15s 191ms/step - loss: 0.4610 - val_loss: 0.0761\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 0.2794 - val_loss: 0.0466\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 5s 97ms/step - loss: 0.2065 - val_loss: 0.0371\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.1626 - val_loss: 0.0285\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 0.1266 - val_loss: 0.0236\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 0.0990 - val_loss: 0.0201\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.0816 - val_loss: 0.0181\n",
      "Epoch 8/500\n",
      "11/50 [=====>........................] - ETA: 3s - loss: 0.0697"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 5s 109ms/step - loss: 0.0707 - val_loss: 0.0168\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 0.0611 - val_loss: 0.0160\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 0.0535 - val_loss: 0.0154\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 0.0484 - val_loss: 0.0150\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 5s 90ms/step - loss: 0.0424 - val_loss: 0.0146\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.0384 - val_loss: 0.0143\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 4s 83ms/step - loss: 0.0348 - val_loss: 0.0140\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.0322 - val_loss: 0.0138\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 5s 103ms/step - loss: 0.0298 - val_loss: 0.0135\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 0.0282 - val_loss: 0.0133\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 5s 102ms/step - loss: 0.0256 - val_loss: 0.0131\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 4s 78ms/step - loss: 0.0241 - val_loss: 0.0130\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 0.0228 - val_loss: 0.0128\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 0.0216 - val_loss: 0.0127\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 0.0206 - val_loss: 0.0126\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 4s 83ms/step - loss: 0.0196 - val_loss: 0.0124\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 6s 116ms/step - loss: 0.0190 - val_loss: 0.0124\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 0.0184 - val_loss: 0.0123\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 0.0178 - val_loss: 0.0122\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 5s 99ms/step - loss: 0.0173 - val_loss: 0.0121\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 0.0170 - val_loss: 0.0120\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 4s 90ms/step - loss: 0.0166 - val_loss: 0.0120\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 0.0164 - val_loss: 0.0119\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 0.0161 - val_loss: 0.0118\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 5s 107ms/step - loss: 0.0160 - val_loss: 0.0118\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 4s 87ms/step - loss: 0.0158 - val_loss: 0.0118\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.0155 - val_loss: 0.0117\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 5s 91ms/step - loss: 0.0154 - val_loss: 0.0117\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 4s 85ms/step - loss: 0.0153 - val_loss: 0.0117\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 4s 86ms/step - loss: 0.0152 - val_loss: 0.0117\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 4s 80ms/step - loss: 0.0152 - val_loss: 0.0116\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 0.0149 - val_loss: 0.0117\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 4s 73ms/step - loss: 0.0149 - val_loss: 0.0117\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 5s 94ms/step - loss: 0.0148 - val_loss: 0.0116\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 5s 106ms/step - loss: 0.0147 - val_loss: 0.0116\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 5s 104ms/step - loss: 0.0146 - val_loss: 0.0116\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 0.0146 - val_loss: 0.0116\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 4s 89ms/step - loss: 0.0145 - val_loss: 0.0116\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 5s 98ms/step - loss: 0.0144 - val_loss: 0.0116\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 0.0143 - val_loss: 0.0116\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 5s 101ms/step - loss: 0.0143 - val_loss: 0.0115\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 4s 75ms/step - loss: 0.0141 - val_loss: 0.0116\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 4s 84ms/step - loss: 0.0141 - val_loss: 0.0116\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 5s 92ms/step - loss: 0.0140 - val_loss: 0.0116\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 4s 79ms/step - loss: 0.0139 - val_loss: 0.0116\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 4s 75ms/step - loss: 0.0139 - val_loss: 0.0118\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 5s 93ms/step - loss: 0.0138 - val_loss: 0.0116\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 5s 96ms/step - loss: 0.0137 - val_loss: 0.0117\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 4s 77ms/step - loss: 0.0137 - val_loss: 0.0116\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 4s 88ms/step - loss: 0.0136 - val_loss: 0.0117\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0136Restoring model weights from the end of the best epoch: 48.\n",
      "50/50 [==============================] - 4s 81ms/step - loss: 0.0136 - val_loss: 0.0117\n",
      "Epoch 58: early stopping\n"
     ]
    }
   ],
   "source": [
    "# 准备模型训练\n",
    "print(\"开始模型初始化 & 训练...\")\n",
    "from models.single_task.model_moe import QuantModel\n",
    "model_config = {\n",
    "        \"seed\": 1024,\n",
    "        \"feature_use_embedding\": False,\n",
    "        \"feature_embedding_dims\": 4,\n",
    "        \"numeric_features_with_boundaries\": {k: pd.qcut(train_df[k], q=20, retbins=True, duplicates='drop')[1].tolist() for k in feature_config.get('numeric_features', [])},\n",
    "        \"integer_categorical_features_with_vocab\": {k: list(train_df[k].unique()) for k in feature_config.get('integer_categorical_features', [])},\n",
    "        \"string_categorical_features_with_vocab\": {k: list(train_df[k].unique()) for k in feature_config.get('string_categorical_features', [])},\n",
    "    }\n",
    "model = QuantModel(config=model_config)\n",
    "\n",
    "initial_learning_rate = 5e-4\n",
    "\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#     initial_learning_rate,\n",
    "#     decay_steps=5 * (len(train_df) // batch_size), # 每5个batch进行一次调整\n",
    "#     decay_rate=0.9,\n",
    "#     staircase=True)\n",
    "model.compile(\n",
    "    # optimizer=tf.keras.optimizers.Adam(lr_schedule),\n",
    "    optimizer=tf.keras.optimizers.Adam(initial_learning_rate),\n",
    "    # loss=tf.keras.losses.MeanSquaredError(),\n",
    "    # loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "    loss=tf.keras.losses.Huber(),\n",
    "    # metrics=[\n",
    "    #     # tf.keras.metrics.MeanSquaredError(),\n",
    "    #     # tf.keras.metrics.MeanAbsoluteError(),\n",
    "    # ],\n",
    "    )\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='min',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "baseline_history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks = [early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics(baseline_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(f'./tf_models/{benchmark}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始保存回测预测结果...\n",
      "59/59 [==============================] - 7s 108ms/step\n"
     ]
    }
   ],
   "source": [
    "# 输出回测预测\n",
    "print(\"开始保存回测预测结果...\")\n",
    "model_pred_result = model.predict(test_ds)\n",
    "output_df = test_data[['code', 'code_name', 'datetime']]\n",
    "output_df['label'] = test_df['label']\n",
    "output_df['label_pred'] = model_pred_result\n",
    "output_df = output_df.rename(columns={\n",
    "    'code': 'stock_code',\n",
    "    'code_name': 'stock_name'\n",
    "})\n",
    "output_df.to_pickle(f'../../Offline/backtest/backtest_data/test/{benchmark}_{test_start_date}_回归任务_v6.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('../../Offline/backtest/backtest_data/test/000016_2019-01-01_回归任务_v5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_prediction(model_prediction):\n",
    "    def get_stock_for_buy(group):\n",
    "        # group = group[group[\"label_pred\"] > self.params.buy_pred_upper_bound]  # 使用模型预测打分的分布（> 0.9的quantile）进行买入过滤\n",
    "        select_n = group.nlargest(3, \"label_pred\")\n",
    "        return select_n.to_dict(\"records\")\n",
    "\n",
    "    def get_stock_for_sell(group):\n",
    "        # group = group[group[\"label_pred\"] < self.params.sell_pred_lower_bound]  # 使用模型预测打分的分布（< 0.1的quantile）进行卖出过滤\n",
    "        select_n = group.nsmallest(3, \"label_pred\")\n",
    "        return select_n.to_dict(\"records\")\n",
    "\n",
    "    stock_for_buy = model_prediction.groupby(\"datetime\").apply(get_stock_for_buy).to_dict()\n",
    "    stock_for_sell = model_prediction.groupby(\"datetime\").apply(get_stock_for_sell).to_dict()\n",
    "    return stock_for_buy, stock_for_sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_for_buy, stock_for_sell = get_model_prediction(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_for_buy['2019-01-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
