{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import akshare as ak\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import sqlite3\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from database.downloader.downloader_base import DownloaderBase\n",
    "import database.database_config as db_config\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conn = sqlite3.connect('../database/hh_quant.db')\n",
    "db_downloader = DownloaderBase(db_conn, db_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock_code_list = ak.stock_info_a_code_name()['code'] # 获取A股所有股票列表\n",
    "# stock_code_list = ak.index_stock_cons(\"000016\")['品种代码'].tolist() # 获取中证50的股票代码列表\n",
    "stock_code_list = set(ak.index_stock_cons(\"000300\")['品种代码'].tolist()) # 获取沪深300的股票代码列表\n",
    "\n",
    "# 获取所有股票列表\n",
    "# stock_code_list = set(db_downloader._download_stock_base_info()['stock_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [00:14<00:00, 20.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_label(dataframe):\n",
    "    N = 5\n",
    "    df = dataframe.copy()\n",
    "    # 标签构建\n",
    "    df['close_N_max'] = df['close'].rolling(window=N).max().shift(-N)\n",
    "    df['close_N_min'] = df['close'].rolling(window=N).min().shift(-N)\n",
    "    df['max_return'] = df['close_N_max'] / df['open'].shift(-1) - 1 # 计算5日间最大收益率\n",
    "    df['min_return'] = df['close_N_min'] / df['open'].shift(-1) - 1 # 计算5日间最小收益率\n",
    "    # 极值处理\n",
    "    df['max_return'] = np.clip(df['max_return'], np.nanquantile(df['max_return'], 0.01), np.nanquantile(df['max_return'], 0.99))\n",
    "    df['min_return'] = np.clip(df['min_return'], np.nanquantile(df['min_return'], 0.01), np.nanquantile(df['min_return'], 0.99))\n",
    "    # 过滤一字涨停情况\n",
    "    df = df[df['high'].shift(-1) != df['low'].shift(-1)]\n",
    "    return df[['datetime', 'max_return', 'min_return']]\n",
    "\n",
    "stock_df_list = []\n",
    "for stock_code in tqdm(stock_code_list):\n",
    "    stock_data = db_downloader._download_stock_history_info(stock_code)\n",
    "    stock_label = build_label(stock_data)\n",
    "    stock_factor_date = db_downloader._download_stock_factor_date_info()\n",
    "    stock_factor_qlib = db_downloader._download_stock_factor_qlib_info(stock_code)\n",
    "    stock_df = stock_data.merge(stock_label, on=['datetime']).merge(stock_factor_date, on=['datetime']).merge(stock_factor_qlib, on=['stock_code', 'datetime'])\n",
    "    stock_df = stock_df.dropna()\n",
    "    if not stock_df.empty:\n",
    "        stock_df_list.append(stock_df)\n",
    "\n",
    "len(stock_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(stock_df_list)\n",
    "# print([i for i in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow处理原始数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_data_type(df, columns, dtype):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].astype(dtype)\n",
    "    return df\n",
    "\n",
    "def get_numeric_boundaries(series, num_bins=20):\n",
    "    if series.nunique() < num_bins:\n",
    "        boundaries = sorted(series.unique())\n",
    "    else:\n",
    "        boundaries = pd.qcut(series, num_bins, retbins=True, duplicates='drop')[1].tolist()\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (103279, 200)\n",
      "validation_data_size: (16684, 200)\n"
     ]
    }
   ],
   "source": [
    "# 6. 选择固定时间区间的数据\n",
    "train_start_date = pd.to_datetime('2010-01-01')\n",
    "train_end_date = pd.to_datetime('2019-12-31')\n",
    "val_start_date = pd.to_datetime('2020-01-01')\n",
    "val_end_date = pd.to_datetime('2020-12-31')\n",
    "\n",
    "train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "validation_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "\n",
    "print(f\"train_data_size: {train_data.shape}\")\n",
    "print(f\"validation_data_size: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = [\"max_return\"]\n",
    "\n",
    "NUMERIC_FEATURES = ['KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE0', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME0', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORR5', 'CORR10', 'CORR20', 'CORR30', 'CORR60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60']\n",
    "INTEGER_CATEGORICAL_FEATURES = ['weekday', 'day_of_month', 'month']\n",
    "STRING_CATEGORICAL_FEATURES = ['day_of_week', 'season']\n",
    "\n",
    "train_data = transfer_data_type(train_data, NUMERIC_FEATURES, 'float32')\n",
    "train_data = transfer_data_type(train_data, INTEGER_CATEGORICAL_FEATURES, 'int64')\n",
    "train_data = transfer_data_type(train_data, STRING_CATEGORICAL_FEATURES, 'string')\n",
    "\n",
    "validation_data = transfer_data_type(validation_data, NUMERIC_FEATURES, 'float32')\n",
    "validation_data = transfer_data_type(validation_data, INTEGER_CATEGORICAL_FEATURES, 'int64')\n",
    "validation_data = transfer_data_type(validation_data, STRING_CATEGORICAL_FEATURES, 'string')\n",
    "\n",
    "NUMERIC_FEATURES_WITH_BOUNDARIES = {k: get_numeric_boundaries(train_data[k])  for k in NUMERIC_FEATURES}\n",
    "INTEGER_CATEGORICAL_FEATURES_WITH_VOCAB = {k: train_data[k].unique() for k in INTEGER_CATEGORICAL_FEATURES}\n",
    "STRING_CATEGORICAL_FEATURES_WITH_VOCAB = {k: train_data[k].unique() for k in STRING_CATEGORICAL_FEATURES}\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURES + INTEGER_CATEGORICAL_FEATURES + STRING_CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=True, batch_size=32):\n",
    "  features = dataframe[feature_cols]\n",
    "  labels = dataframe[label_cols]\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(features))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n",
    "\n",
    "train_ds = df_to_dataset(train_data, FEATURE_NAMES, TARGET_FEATURE_NAME, shuffle=True)\n",
    "val_ds = df_to_dataset(validation_data, FEATURE_NAMES, TARGET_FEATURE_NAME, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Senet(tf.keras.layers.Layer):\n",
    "    def __init__(self, reduction_ratio=3, seed=1024, **kwargs):\n",
    "        super(Senet, self).__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.seed = seed  \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.field_size = len(input_shape)\n",
    "        self.reduction_size = max(1, self.field_size // self.reduction_ratio)\n",
    "        self.scale_layer = tf.keras.layers.Dense(units=self.reduction_size, activation='relu')\n",
    "        self.expand_layer = tf.keras.layers.Dense(units=self.field_size, activation='relu')\n",
    "        super(Senet, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        inputs = [tf.expand_dims(i, axis=1) for i in inputs]\n",
    "        inputs = tf.concat(inputs, axis=1) # [B, N, dim]\n",
    "        Z = tf.reduce_mean(inputs, axis=-1) # [B, N]\n",
    "        A_1 = self.scale_layer(Z) # [B, X]\n",
    "        A_2 = self.expand_layer(A_1) # [B, N]\n",
    "        scale_inputs = tf.multiply(inputs, tf.expand_dims(A_2, axis=-1))\n",
    "        output = scale_inputs + inputs # skip-connection\n",
    "        return output # [B, N, dim]\n",
    "\n",
    "\n",
    "class Dnn(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, activation=\"relu\", dropout_rate=0.2, use_bn=False, seed=1024, **kwargs):\n",
    "        super(Dnn, self).__init__(**kwargs)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        self.seed = seed\n",
    "        self.dense_layers = []\n",
    "        self.dropout_layers = []\n",
    "        self.bn_layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        for units in self.hidden_units:\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(units=units, activation=self.activation))\n",
    "            self.dropout_layers.append(tf.keras.layers.Dropout(rate=self.dropout_rate, seed=self.seed))\n",
    "            if self.use_bn:\n",
    "                self.bn_layers.append(tf.keras.layers.BatchNormalization())\n",
    "        super(Dnn, self).build(input_shape)  # Be sure to call this at the end\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            x = self.dense_layers[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn_layers[i](x, training=training)\n",
    "            x = self.dropout_layers[i](x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantModel(tf.keras.Model):\n",
    "\tdef __init__(self, config, **kwargs):\n",
    "\t\tsuper(QuantModel, self).__init__(**kwargs)\n",
    "\t\tself.config = config\n",
    "\n",
    "\t\t# 添加属性来存储预定义的层\n",
    "\t\tself.lookup_layers = {}\n",
    "\t\tself.embedding_layers = {}\n",
    "\n",
    "        # 创建连续特征的离散化层和嵌入层\n",
    "\t\tfor feature_name, boundaries in self.config.get(\"numeric_features_with_boundaries\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.Discretization(bin_boundaries=boundaries, output_mode='int', name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(boundaries) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "        # 创建整数特征的查找层和嵌入层\n",
    "\t\tfor feature_name, vocab in self.config.get(\"integer_categorical_features_with_vocab\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.IntegerLookup(vocabulary=vocab, name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(vocab) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "\t\t# 创建字符串特征的查找层和嵌入层\n",
    "\t\tfor feature_name, vocab in self.config.get(\"string_categorical_features_with_vocab\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.StringLookup(vocabulary=vocab, name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(vocab) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "\n",
    "\t\tself.senet_layer = Senet(\n",
    "\t\t\treduction_ratio=self.config.get('reduction_ratio', 3), \n",
    "\t\t\tseed=self.config.get('seed', 1024),\n",
    "\t\t)\n",
    "\n",
    "\t\tself.dnn_layer_1 = Dnn(\n",
    "\t\t\thidden_units=self.config.get('dnn_hidden_units', [64,32]),\n",
    "\t\t\tactivation=self.config.get('dnn_activation', 'relu'),\n",
    "\t\t\tdropout_rate=self.config.get('dnn_dropout', 0.2),\n",
    "\t\t\tuse_bn=self.config.get('dnn_use_bn', True)\n",
    "\t\t)\n",
    "\t\tself.output_layer_1 = tf.keras.layers.Dense(1, activation=None) # max_return\n",
    "\t\t\n",
    "\t\t# self.dnn_layer_2 = Dnn(\n",
    "\t\t# \thidden_units=self.config.get('dnn_hidden_units', [64,32]),\n",
    "\t\t# \tactivation=self.config.get('dnn_activation', 'relu'),\n",
    "\t\t# \tdropout_rate=self.config.get('dnn_dropout', 0.2),\n",
    "\t\t# \tuse_bn=self.config.get('dnn_use_bn', True)\n",
    "\t\t# )\n",
    "\t\t# self.output_layer_2 = tf.keras.layers.Dense(1, activation=None) # min_return\n",
    "\t\t\n",
    "\n",
    "\tdef call(self, inputs, training=False):\n",
    "\t\t# 确保inputs是一个字典类型，每个键值对应一个特征输入\n",
    "\t\tif not isinstance(inputs, dict): \n",
    "\t\t\traise ValueError('The inputs to the model should be a dictionary where keys are feature names.')\n",
    "\t\tencoded_features = []\n",
    "    \t# 现在使用已经实例化的层来编码输入\n",
    "\t\tfor feature_name, feature_value in inputs.items():\n",
    "        \t# 使用预定义的查找层和嵌入层\n",
    "\t\t\tlookup_layer = self.lookup_layers[feature_name]\n",
    "\t\t\tembedding_layer = self.embedding_layers[feature_name]\n",
    "\t\t\tencoded_feature = embedding_layer(lookup_layer(feature_value))\n",
    "\t\t\tencoded_features.append(encoded_feature)\n",
    "\n",
    "\t\tsenet_output = self.senet_layer(encoded_features, training=training)\n",
    "\t\tsenet_output = tf.keras.layers.Flatten()(senet_output) # [B, N * dim]\n",
    "\t\tdnn_output_1 = self.dnn_layer_1(senet_output, training=training)\n",
    "\t\t# dnn_output_2 = self.dnn_layer_2(senet_output, training=training)\n",
    "\t\toutput_1 = self.output_layer_1(dnn_output_1, training=training)\n",
    "\t\t# output_2 = self.output_layer_2(dnn_output_2, training=training)\n",
    "\t\treturn output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3228/3228 - 27s - loss: 0.1292 - output_1_loss: 0.0563 - output_2_loss: 0.0729 - val_loss: 0.0078 - val_output_1_loss: 0.0039 - val_output_2_loss: 0.0039 - 27s/epoch - 8ms/step\n",
      "Epoch 2/20\n",
      "3228/3228 - 20s - loss: 0.0053 - output_1_loss: 0.0026 - output_2_loss: 0.0026 - val_loss: 0.0080 - val_output_1_loss: 0.0040 - val_output_2_loss: 0.0041 - 20s/epoch - 6ms/step\n",
      "Epoch 3/20\n",
      "3228/3228 - 20s - loss: 0.0053 - output_1_loss: 0.0027 - output_2_loss: 0.0027 - val_loss: 0.0078 - val_output_1_loss: 0.0039 - val_output_2_loss: 0.0039 - 20s/epoch - 6ms/step\n",
      "Epoch 4/20\n",
      "3228/3228 - 20s - loss: 0.0053 - output_1_loss: 0.0026 - output_2_loss: 0.0027 - val_loss: 0.0079 - val_output_1_loss: 0.0039 - val_output_2_loss: 0.0039 - 20s/epoch - 6ms/step\n",
      "Epoch 5/20\n",
      "3228/3228 - 20s - loss: 0.0051 - output_1_loss: 0.0026 - output_2_loss: 0.0026 - val_loss: 0.0078 - val_output_1_loss: 0.0039 - val_output_2_loss: 0.0039 - 20s/epoch - 6ms/step\n",
      "Epoch 6/20\n",
      "3228/3228 - 20s - loss: 0.0050 - output_1_loss: 0.0025 - output_2_loss: 0.0025 - val_loss: 0.0080 - val_output_1_loss: 0.0040 - val_output_2_loss: 0.0040 - 20s/epoch - 6ms/step\n",
      "Epoch 7/20\n",
      "3228/3228 - 20s - loss: 0.0049 - output_1_loss: 0.0024 - output_2_loss: 0.0024 - val_loss: 0.0082 - val_output_1_loss: 0.0041 - val_output_2_loss: 0.0041 - 20s/epoch - 6ms/step\n",
      "Epoch 8/20\n",
      "3228/3228 - 20s - loss: 0.0048 - output_1_loss: 0.0024 - output_2_loss: 0.0024 - val_loss: 0.0081 - val_output_1_loss: 0.0040 - val_output_2_loss: 0.0040 - 20s/epoch - 6ms/step\n",
      "Epoch 9/20\n",
      "3228/3228 - 19s - loss: 0.0047 - output_1_loss: 0.0024 - output_2_loss: 0.0024 - val_loss: 0.0084 - val_output_1_loss: 0.0042 - val_output_2_loss: 0.0042 - 19s/epoch - 6ms/step\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m loss_weight \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer, loss\u001b[38;5;241m=\u001b[39mloss, loss_weights\u001b[38;5;241m=\u001b[39mloss_weight)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# early_stopping = tf.keras.callbacks.EarlyStopping(\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     monitor='val_loss',\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     verbose=1,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#     restore_best_weights=True,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m args \u001b[38;5;241m=\u001b[39m args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 132\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:239\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    229\u001b[0m lookup_func_type, lookup_func_context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    230\u001b[0m     function_type_utils\u001b[38;5;241m.\u001b[39mmake_canonicalized_monomorphic_type(\n\u001b[1;32m    231\u001b[0m         args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_cache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlookup_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_func_context\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/function_cache.py:48\u001b[0m, in \u001b[0;36mFunctionCache.lookup\u001b[0;34m(self, function_type, context)\u001b[0m\n\u001b[1;32m     46\u001b[0m context \u001b[38;5;241m=\u001b[39m context \u001b[38;5;129;01mor\u001b[39;00m FunctionContext()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_dict:\n\u001b[0;32m---> 48\u001b[0m   dispatch_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dispatch_type:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary[(context, dispatch_type)]\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/type_dispatch.py:85\u001b[0m, in \u001b[0;36mTypeDispatchTable.dispatch\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m request\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# For known non-exact matches.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# (self._dispatch cache does not contain exact matches)\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_cache\u001b[49m:\n\u001b[1;32m     86\u001b[0m   \u001b[38;5;66;03m# Move to the front of LRU cache.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache\u001b[38;5;241m.\u001b[39mpop(request)\n\u001b[1;32m     88\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_cache[request] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/function_type.py:456\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/core/function/polymorphism/function_type.py:155\u001b[0m, in \u001b[0;36mParameter.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/framework/type_spec.py:555\u001b[0m, in \u001b[0;36mTypeSpec.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 555\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_cmp_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GithubProject/hh_quant/.venv/lib/python3.10/site-packages/tensorflow/python/framework/tensor.py:895\u001b[0m, in \u001b[0;36mDenseSpec.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 895\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"seed\": 1024,\n",
    "    \"reduction_ratio\": 3,\n",
    "    \"dnn_hidden_units\": [128,64,32],\n",
    "    \"dnn_activation\": 'relu',\n",
    "    \"dnn_dropout\": 0.2,\n",
    "    \"dnn_use_bn\": True,\n",
    "    \"numeric_features_with_boundaries\": NUMERIC_FEATURES_WITH_BOUNDARIES,\n",
    "    \"integer_categorical_features_with_vocab\": INTEGER_CATEGORICAL_FEATURES_WITH_VOCAB,\n",
    "    \"string_categorical_features_with_vocab\": STRING_CATEGORICAL_FEATURES_WITH_VOCAB,\n",
    "    \"feature_embedding_dims\": 6\n",
    "}\n",
    "\n",
    "model = QuantModel(model_config)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-3)\n",
    "loss = [tf.keras.losses.MeanSquaredError(), tf.keras.losses.MeanSquaredError()]\n",
    "loss_weight = [1.0, 1.0]\n",
    "model.compile(optimizer, loss=loss, loss_weights=loss_weight)\n",
    "model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds, \n",
    "        epochs=20,\n",
    "        verbose=2,\n",
    "        callbacks=[])\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     verbose=1,\n",
    "#     patience=10,\n",
    "#     mode='max',\n",
    "#     restore_best_weights=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.save('./stock_selection_base_model')\n",
    "# reloaded_model = tf.keras.models.load_model('./stock_selection_base_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522/522 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_code</th>\n",
       "      <th>stock_name</th>\n",
       "      <th>datetime</th>\n",
       "      <th>max_return</th>\n",
       "      <th>max_return_pred</th>\n",
       "      <th>min_return</th>\n",
       "      <th>min_return_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>002648</td>\n",
       "      <td>卫星化学</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>0.014517</td>\n",
       "      <td>-0.005147</td>\n",
       "      <td>-0.013400</td>\n",
       "      <td>-0.005147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>002648</td>\n",
       "      <td>卫星化学</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>-0.016313</td>\n",
       "      <td>-0.005824</td>\n",
       "      <td>-0.039152</td>\n",
       "      <td>-0.005824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>002648</td>\n",
       "      <td>卫星化学</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>-0.008943</td>\n",
       "      <td>-0.022136</td>\n",
       "      <td>-0.008943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>002648</td>\n",
       "      <td>卫星化学</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>0.005562</td>\n",
       "      <td>-0.010409</td>\n",
       "      <td>-0.017241</td>\n",
       "      <td>-0.010409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>002648</td>\n",
       "      <td>卫星化学</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>0.014590</td>\n",
       "      <td>-0.009161</td>\n",
       "      <td>-0.003928</td>\n",
       "      <td>-0.009161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stock_code stock_name    datetime  max_return  max_return_pred  \\\n",
       "1858     002648       卫星化学  2020-01-02    0.014517        -0.005147   \n",
       "1859     002648       卫星化学  2020-01-03   -0.016313        -0.005824   \n",
       "1860     002648       卫星化学  2020-01-06    0.001107        -0.008943   \n",
       "1861     002648       卫星化学  2020-01-07    0.005562        -0.010409   \n",
       "1862     002648       卫星化学  2020-01-08    0.014590        -0.009161   \n",
       "\n",
       "      min_return  min_return_pred  \n",
       "1858   -0.013400        -0.005147  \n",
       "1859   -0.039152        -0.005824  \n",
       "1860   -0.022136        -0.008943  \n",
       "1861   -0.017241        -0.010409  \n",
       "1862   -0.003928        -0.009161  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_return_pred, min_return_pred = model.predict(val_ds)\n",
    "\n",
    "output_df = validation_data[['stock_code', 'stock_name', 'datetime']]\n",
    "output_df['max_return'] = validation_data['max_return']\n",
    "output_df['max_return_pred'] = max_return_pred.squeeze()\n",
    "output_df['min_return'] = validation_data['min_return']\n",
    "output_df['min_return_pred'] = min_return_pred.squeeze()\n",
    "\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7756142506142506, 'recall': 0.9811951200559484}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def calculate_performance(dataframe, target_col, threshold):\n",
    "    y_true = dataframe[target_col] >= threshold\n",
    "    y_pred = dataframe[target_col+'_pred'] >= threshold\n",
    "    output = {'precision': precision_score(y_true, y_pred), 'recall': recall_score(y_true, y_pred)}\n",
    "    return output\n",
    "\n",
    "calculate_performance(output_df, 'min_return', -0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = backtest_df[['stock_code', 'datetime', 'prediction']].rename(columns={\n",
    "#     'stock_code': 'instrument',\n",
    "#     'datetime': 'date',\n",
    "#     'prediction': 'pred'\n",
    "# })\n",
    "# output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('./stock_selection_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
