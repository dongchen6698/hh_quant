{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import akshare as ak\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 获取A股所有股票列表\n",
    "# stock_code_list = ak.stock_info_a_code_name()['code']\n",
    "stock_code_list = ak.index_stock_cons(\"000016\")['品种代码'].tolist() # 获取中证50的股票代码列表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 获取基础原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_history_info(stock_code):\n",
    "    result = ak.stock_zh_a_hist(symbol=stock_code, adjust='hfq').rename(\n",
    "            columns={\n",
    "                \"日期\": \"datetime\",\n",
    "                \"开盘\": \"open\",\n",
    "                \"最高\": \"high\",\n",
    "                \"最低\": \"low\",\n",
    "                \"收盘\": \"close\",\n",
    "                \"成交量\": \"volume\",\n",
    "                \"成交额\": \"turnover\",\n",
    "                \"振幅\": \"amplitude\",\n",
    "                \"涨跌幅\": \"change_pct\",\n",
    "                \"涨跌额\": \"change_amount\",\n",
    "                \"换手率\": \"turnover_rate\",\n",
    "            }\n",
    "        )\n",
    "    result = result[['datetime', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    result.insert(0, 'stock_code', stock_code)\n",
    "    return result\n",
    "\n",
    "def get_stock_label(dataframe, expression_excutor):\n",
    "    # 计算收益：5日收盘价(作为卖出价格)除以明日开盘价(作为买入价格)\n",
    "    expression_1 = \"shift(close,-5)/shift(open,-1)-1\"\n",
    "    # 极值处理：用1%和99%分位的值做clip\n",
    "    expression_2 = \"clip(label,all_quantile(label, 0.01),all_quantile(label,0.99))\"\n",
    "    # 过滤掉一字涨停的情况 (设置label为NaN，在后续处理和训练中会忽略NaN的label)\n",
    "    expression_3 = \"where(shift(high,-1)=shift(low,-1), nan, label)\"\n",
    "\n",
    "    dataframe['label'] = expression_excutor.excute(dataframe, expression_1)\n",
    "    dataframe['label'] = expression_excutor.excute(dataframe, expression_2)\n",
    "    dataframe['label'] = expression_excutor.excute(dataframe, expression_3)\n",
    "    return dataframe\n",
    "\n",
    "def get_basic_factor(dataframe, expression_excutor):\n",
    "    alpha_dict = json.loads(open('./alpha_184.json', \"r\").read())\n",
    "    for alpha_name, alpha_expression in alpha_dict.items():\n",
    "        dataframe[alpha_name] = expression_excutor.excute(dataframe, alpha_expression)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:34<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from expression_excutor import AlphaExpressionExcutor\n",
    "expression_excutor = AlphaExpressionExcutor()\n",
    "\n",
    "stock_data_list = []\n",
    "for stock_code in tqdm(stock_code_list):\n",
    "    stock_data = get_stock_history_info(stock_code)\n",
    "    stock_data = get_stock_label(stock_data, expression_excutor)\n",
    "    stock_data = get_basic_factor(stock_data, expression_excutor)\n",
    "    stock_data = stock_data.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    stock_data_list.append(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stock_code', 'datetime', 'open', 'high', 'low', 'close', 'volume', 'label', 'KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE0', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME0', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60']\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(stock_data_list)\n",
    "print([i for i in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_size: (93655, 182)\n",
      "validation_data_size: (11027, 182)\n"
     ]
    }
   ],
   "source": [
    "# 6. 选择固定时间区间的数据\n",
    "train_start_date = pd.to_datetime('2010-01-01')\n",
    "train_end_date = pd.to_datetime('2019-12-31')\n",
    "val_start_date = pd.to_datetime('2020-01-01')\n",
    "val_end_date = pd.to_datetime('2020-12-31')\n",
    "\n",
    "train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "validation_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "\n",
    "print(f\"train_data_size: {train_data.shape}\")\n",
    "print(f\"validation_data_size: {validation_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow处理原始数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_boundaries(series, num_bins=20):\n",
    "    if series.nunique() < num_bins:\n",
    "        boundaries = sorted(series.unique())\n",
    "    else:\n",
    "        boundaries = pd.qcut(series, num_bins, retbins=True, duplicates='drop')[1].tolist()\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = [\"label\"]\n",
    "\n",
    "NUMERIC_FEATURES = ['KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE0', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME0', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60']\n",
    "NUMERIC_FEATURES_WITH_BOUNDARIES = {k: get_numeric_boundaries(train_data[k])  for k in NUMERIC_FEATURES}\n",
    "\n",
    "INTEGER_CATEGORICAL_FEATURES = []\n",
    "INTEGER_CATEGORICAL_FEATURES_WITH_VOCAB = {}\n",
    "\n",
    "STRING_CATEGORICAL_FEATURES = []\n",
    "STRING_CATEGORICAL_FEATURES_WITH_VOCAB = {}\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURES + INTEGER_CATEGORICAL_FEATURES + STRING_CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Senet(tf.keras.layers.Layer):\n",
    "    def __init__(self, reduction_ratio=3, seed=1024, **kwargs):\n",
    "        super(Senet, self).__init__(**kwargs)\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.seed = seed  \n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.field_size = len(input_shape)\n",
    "        self.reduction_size = max(1, self.field_size // self.reduction_ratio)\n",
    "        self.scale_layer = tf.keras.layers.Dense(units=self.reduction_size, activation='relu')\n",
    "        self.expand_layer = tf.keras.layers.Dense(units=self.field_size, activation='relu')\n",
    "        super(Senet, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        inputs = [tf.expand_dims(i, axis=1) for i in inputs]\n",
    "        inputs = tf.concat(inputs, axis=1) # [B, N, dim]\n",
    "        Z = tf.reduce_mean(inputs, axis=-1) # [B, N]\n",
    "        A_1 = self.scale_layer(Z) # [B, X]\n",
    "        A_2 = self.expand_layer(A_1) # [B, N]\n",
    "        scale_inputs = tf.multiply(inputs, tf.expand_dims(A_2, axis=-1))\n",
    "        output = scale_inputs + inputs # skip-connection\n",
    "        return output # [B, N, dim]\n",
    "\n",
    "\n",
    "class Dnn(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, activation=\"relu\", dropout_rate=0.2, use_bn=False, seed=1024, **kwargs):\n",
    "        super(Dnn, self).__init__(**kwargs)\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        self.seed = seed\n",
    "        self.dense_layers = []\n",
    "        self.dropout_layers = []\n",
    "        self.bn_layers = []\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        for units in self.hidden_units:\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(units=units, activation=self.activation))\n",
    "            self.dropout_layers.append(tf.keras.layers.Dropout(rate=self.dropout_rate, seed=self.seed))\n",
    "            if self.use_bn:\n",
    "                self.bn_layers.append(tf.keras.layers.BatchNormalization())\n",
    "        super(Dnn, self).build(input_shape)  # Be sure to call this at the end\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            x = self.dense_layers[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn_layers[i](x, training=training)\n",
    "            x = self.dropout_layers[i](x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantModel(tf.keras.Model):\n",
    "\tdef __init__(self, config, **kwargs):\n",
    "\t\tsuper(QuantModel, self).__init__(**kwargs)\n",
    "\t\tself.config = config\n",
    "\n",
    "\t\t# 添加属性来存储预定义的层\n",
    "\t\tself.lookup_layers = {}\n",
    "\t\tself.embedding_layers = {}\n",
    "\n",
    "        # 创建连续特征的离散化层和嵌入层\n",
    "\t\tfor feature_name, boundaries in self.config.get(\"numeric_features_with_boundaries\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.Discretization(bin_boundaries=boundaries, output_mode='int', name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(boundaries) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "        # 创建整数特征的查找层和嵌入层\n",
    "\t\tfor feature_name, vocab in self.config.get(\"integer_categorical_features_with_vocab\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.IntegerLookup(vocabulary=vocab, name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(vocab) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "\t\t# 创建字符串特征的查找层和嵌入层\n",
    "\t\tfor feature_name, vocab in self.config.get(\"string_categorical_features_with_vocab\").items():\n",
    "\t\t\tself.lookup_layers[feature_name] = tf.keras.layers.StringLookup(vocabulary=vocab, name=f'{feature_name}_lookup')\n",
    "\t\t\tself.embedding_layers[feature_name] = tf.keras.layers.Embedding(input_dim=len(vocab) + 1, output_dim=self.config.get(\"feature_embedding_dims\", 6), name=f'{feature_name}_embedding')\n",
    "\n",
    "\t\tself.senet_layer = Senet(\n",
    "\t\t\treduction_ratio=self.config.get('reduction_ratio', 3), \n",
    "\t\t\tseed=self.config.get('seed', 1024),\n",
    "\t\t)\n",
    "\t\tself.dnn_layer = Dnn(\n",
    "\t\t\thidden_units=self.config.get('dnn_hidden_units', [64,32]),\n",
    "\t\t\tactivation=self.config.get('dnn_activation', 'relu'),\n",
    "\t\t\tdropout_rate=self.config.get('dnn_dropout', 0.2),\n",
    "\t\t\tuse_bn=self.config.get('dnn_use_bn', True)\n",
    "\t\t)\n",
    "\t\tself.output_layer = tf.keras.layers.Dense(1, activation=None)\n",
    "\t\t\n",
    "\n",
    "\tdef call(self, inputs, training=None):\n",
    "\t\t# 确保inputs是一个字典类型，每个键值对应一个特征输入\n",
    "\t\tif not isinstance(inputs, dict): \n",
    "\t\t\traise ValueError('The inputs to the model should be a dictionary where keys are feature names.')\n",
    "\t\tencoded_features = []\n",
    "    \t# 现在使用已经实例化的层来编码输入\n",
    "\t\tfor feature_name in inputs:\n",
    "        \t# 使用预定义的查找层和嵌入层\n",
    "\t\t\tlookup_layer = self.lookup_layers[feature_name]\n",
    "\t\t\tembedding_layer = self.embedding_layers[feature_name]\n",
    "\t\t\tencoded_feature = embedding_layer(lookup_layer(inputs[feature_name]))\n",
    "\t\t\tencoded_features.append(encoded_feature)\n",
    "\n",
    "\t\tsenet_output = self.senet_layer(encoded_features, training=training)\n",
    "\t\tsenet_output = tf.keras.layers.Flatten()(senet_output) # [B, N * dim]\n",
    "\t\tdnn_output = self.dnn_layer(senet_output, training=training)\n",
    "\t\toutput = self.output_layer(dnn_output, training=training)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, feature_cols, label_cols, shuffle=True, batch_size=32):\n",
    "  features = dataframe[feature_cols]\n",
    "  labels = dataframe[label_cols]\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(features))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n",
    "\n",
    "train_ds = df_to_dataset(train_data, FEATURE_NAMES, TARGET_FEATURE_NAME, shuffle=True)\n",
    "val_ds = df_to_dataset(validation_data, FEATURE_NAMES, TARGET_FEATURE_NAME, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2927/2927 - 21s - loss: 0.0867 - val_loss: 0.0029 - 21s/epoch - 7ms/step\n",
      "Epoch 2/10\n",
      "2927/2927 - 15s - loss: 0.0023 - val_loss: 0.0032 - 15s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "2927/2927 - 15s - loss: 0.0023 - val_loss: 0.0030 - 15s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "2927/2927 - 15s - loss: 0.0023 - val_loss: 0.0030 - 15s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "2927/2927 - 15s - loss: 0.0023 - val_loss: 0.0030 - 15s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "2927/2927 - 15s - loss: 0.0022 - val_loss: 0.0030 - 15s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "2927/2927 - 15s - loss: 0.0022 - val_loss: 0.0030 - 15s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "2927/2927 - 16s - loss: 0.0021 - val_loss: 0.0031 - 16s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "2927/2927 - 17s - loss: 0.0021 - val_loss: 0.0031 - 17s/epoch - 6ms/step\n",
      "Epoch 10/10\n",
      "2927/2927 - 15s - loss: 0.0020 - val_loss: 0.0031 - 15s/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x3867e3160>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"seed\": 1024,\n",
    "    \"reduction_ratio\": 3,\n",
    "    \"dnn_hidden_units\": [64,32],\n",
    "    \"dnn_activation\": 'relu',\n",
    "    \"dnn_dropout\": 0.2,\n",
    "    \"dnn_use_bn\": True,\n",
    "    \"numeric_features_with_boundaries\": NUMERIC_FEATURES_WITH_BOUNDARIES,\n",
    "    \"integer_categorical_features_with_vocab\": INTEGER_CATEGORICAL_FEATURES_WITH_VOCAB,\n",
    "    \"string_categorical_features_with_vocab\": STRING_CATEGORICAL_FEATURES_WITH_VOCAB,\n",
    "    \"feature_embedding_dims\": 6\n",
    "}\n",
    "\n",
    "model = QuantModel(model_config)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "model.compile(optimizer, loss=loss)\n",
    "model.fit(\n",
    "        train_ds, \n",
    "        validation_data=val_ds, \n",
    "        epochs=10,\n",
    "        verbose=2,\n",
    "        callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.save('./stock_selection_base_model')\n",
    "# reloaded_model = tf.keras.models.load_model('./stock_selection_base_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - 2s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "test_labels = []\n",
    "for _, labels in val_ds:\n",
    "    test_labels.extend(labels.numpy().squeeze())\n",
    "\n",
    "test_predictions = model.predict(val_ds).squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5470</th>\n",
       "      <td>0.310501</td>\n",
       "      <td>-0.030083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>0.304501</td>\n",
       "      <td>0.007330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>0.279538</td>\n",
       "      <td>-0.039140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>0.275968</td>\n",
       "      <td>-0.067975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5469</th>\n",
       "      <td>0.260039</td>\n",
       "      <td>-0.022301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2793</th>\n",
       "      <td>0.256914</td>\n",
       "      <td>-0.012903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>0.252496</td>\n",
       "      <td>0.014615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>0.248619</td>\n",
       "      <td>-0.021992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>0.245190</td>\n",
       "      <td>-0.009643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5394</th>\n",
       "      <td>0.245190</td>\n",
       "      <td>-0.002916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5275</th>\n",
       "      <td>0.245190</td>\n",
       "      <td>-0.024575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5274</th>\n",
       "      <td>0.244524</td>\n",
       "      <td>0.019611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>0.241715</td>\n",
       "      <td>0.077173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>0.241715</td>\n",
       "      <td>0.008431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>0.241321</td>\n",
       "      <td>0.028403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4712</th>\n",
       "      <td>0.238495</td>\n",
       "      <td>0.010147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>0.238495</td>\n",
       "      <td>0.019892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4590</th>\n",
       "      <td>0.238495</td>\n",
       "      <td>0.003051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4589</th>\n",
       "      <td>0.238495</td>\n",
       "      <td>-0.006093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>0.238495</td>\n",
       "      <td>-0.008092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      true_label  prediction\n",
       "5470    0.310501   -0.030083\n",
       "3016    0.304501    0.007330\n",
       "5518    0.279538   -0.039140\n",
       "5468    0.275968   -0.067975\n",
       "5469    0.260039   -0.022301\n",
       "2793    0.256914   -0.012903\n",
       "2795    0.252496    0.014615\n",
       "3015    0.248619   -0.021992\n",
       "5204    0.245190   -0.009643\n",
       "5394    0.245190   -0.002916\n",
       "5275    0.245190   -0.024575\n",
       "5274    0.244524    0.019611\n",
       "1772    0.241715    0.077173\n",
       "1773    0.241715    0.008431\n",
       "5517    0.241321    0.028403\n",
       "4712    0.238495    0.010147\n",
       "4609    0.238495    0.019892\n",
       "4590    0.238495    0.003051\n",
       "4589    0.238495   -0.006093\n",
       "4588    0.238495   -0.008092"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df['true_label'] = test_labels\n",
    "test_df['prediction'] = test_predictions\n",
    "test_df.sort_values(by=['true_label'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(true_positives_scores, bins=50, alpha=0.5, label='True Positives')\n",
    "# plt.hist(false_positives_scores, bins=50, alpha=0.5, label='False Positives')\n",
    "# plt.xlabel('Scores')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import sklearn\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# def plot_cm(labels, predictions, p=0.5):\n",
    "#     cm = confusion_matrix(labels, predictions > p)\n",
    "#     plt.figure(figsize=(5,5))\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "#     plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "#     plt.ylabel('Actual label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "\n",
    "#     print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "#     print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "#     print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "#     print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "#     print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "# plot_cm(test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
