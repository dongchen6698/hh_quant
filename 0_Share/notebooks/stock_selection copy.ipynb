{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import akshare as ak\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 获取A股所有股票列表\n",
    "# stock_code_list = ak.stock_info_a_code_name()['code']\n",
    "# stock_code_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 获取基础原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_history_info(stock_code):\n",
    "    \"\"\"\n",
    "    前复权（Forward Adjusted）:\n",
    "    前复权数据将历史价格向前调整，使得最近的价格不变，而历史价格按照股票的分红、送股和转增等因素进行了调整。这种方式使得历史价格反映了如果投资者持有股票至今所应该得到的收益。前复权数据通常用于图表分析，使得价格连续性更好，并且更适用于基于趋势分析的投资策略。\n",
    "\n",
    "    后复权（Backward Adjusted）:\n",
    "    后复权数据将历史价格向后调整，使得历史价格保持不变，而最新的价格根据历史的分红和股权变动进行调整。后复权数据通常用于回测，因为它更贴近实际的交易价格，可以较准确地反映历史的交易情况。\n",
    "\n",
    "    不复权（Unadjusted）:\n",
    "    不复权数据是指没有经过任何调整的原始交易数据。不复权数据反映了实际的市场成交价格，但由于不考虑分红和股权变动，因此它不适合用于长期的价格分析。\n",
    "    \"\"\"\n",
    "    result = ak.stock_zh_a_hist(symbol=stock_code, adjust='hfq').rename(\n",
    "            columns={\n",
    "                \"日期\": \"datetime\",\n",
    "                \"开盘\": \"open\",\n",
    "                \"最高\": \"high\",\n",
    "                \"最低\": \"low\",\n",
    "                \"收盘\": \"close\",\n",
    "                \"成交量\": \"volume\",\n",
    "                \"成交额\": \"turnover\",\n",
    "                \"振幅\": \"amplitude\",\n",
    "                \"涨跌幅\": \"change_pct\",\n",
    "                \"涨跌额\": \"change_amount\",\n",
    "                \"换手率\": \"turnover_rate\",\n",
    "            }\n",
    "        )\n",
    "    result = result[['datetime', 'open', 'high', 'low', 'close', 'volume']]\n",
    "    result.insert(0, 'stock_code', stock_code)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code = '000001'\n",
    "dataframe = get_stock_history_info(stock_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from expression_excutor import AlphaExpressionExcutor\n",
    "expression_excutor = AlphaExpressionExcutor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算收益：5日收盘价(作为卖出价格)除以明日开盘价(作为买入价格)\n",
    "expression_1 = \"shift(close,-5)/shift(open,-1)\"\n",
    "# 极值处理：用1%和99%分位的值做clip\n",
    "expression_2 = \"clip(label,all_quantile(label, 0.01),all_quantile(label,0.99))\"\n",
    "# 过滤掉一字涨停的情况 (设置label为NaN，在后续处理和训练中会忽略NaN的label)\n",
    "expression_3 = \"where(shift(high,-1)=shift(low,-1), nan, label)\"\n",
    "\n",
    "dataframe['label'] = expression_excutor.excute(dataframe, expression_1)\n",
    "dataframe['label'] = expression_excutor.excute(dataframe, expression_2)\n",
    "dataframe['label'] = expression_excutor.excute(dataframe, expression_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_dict = json.loads(open('./alpha_184.json', \"r\").read())\n",
    "\n",
    "for alpha_name, alpha_expression in alpha_dict.items():\n",
    "        dataframe[alpha_name] = expression_excutor.excute(dataframe, alpha_expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe.replace([np.inf, -np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stock_code', 'datetime', 'open', 'high', 'low', 'close', 'volume', 'label', 'KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2', 'KLOW', 'KLOW2', 'KSFT', 'KSFT2', 'OPEN0', 'OPEN1', 'OPEN2', 'OPEN3', 'OPEN4', 'HIGH0', 'HIGH1', 'HIGH2', 'HIGH3', 'HIGH4', 'LOW0', 'LOW1', 'LOW2', 'LOW3', 'LOW4', 'CLOSE0', 'CLOSE1', 'CLOSE2', 'CLOSE3', 'CLOSE4', 'VOLUME0', 'VOLUME1', 'VOLUME2', 'VOLUME3', 'VOLUME4', 'ROC5', 'ROC10', 'ROC20', 'ROC30', 'ROC60', 'MAX5', 'MAX10', 'MAX20', 'MAX30', 'MAX60', 'MIN5', 'MIN10', 'MIN20', 'MIN30', 'MIN60', 'MA5', 'MA10', 'MA20', 'MA30', 'MA60', 'STD5', 'STD10', 'STD20', 'STD30', 'STD60', 'BETA5', 'BETA10', 'BETA20', 'BETA30', 'BETA60', 'RSQR5', 'RSQR10', 'RSQR20', 'RSQR30', 'RSQR60', 'RESI5', 'RESI10', 'RESI20', 'RESI30', 'RESI60', 'QTLU5', 'QTLU10', 'QTLU20', 'QTLU30', 'QTLU60', 'QTLD5', 'QTLD10', 'QTLD20', 'QTLD30', 'QTLD60', 'TSRANK5', 'TSRANK10', 'TSRANK20', 'TSRANK30', 'TSRANK60', 'RSV5', 'RSV10', 'RSV20', 'RSV30', 'RSV60', 'IMAX5', 'IMAX10', 'IMAX20', 'IMAX30', 'IMAX60', 'IMIN5', 'IMIN10', 'IMIN20', 'IMIN30', 'IMIN60', 'IMXD5', 'IMXD10', 'IMXD20', 'IMXD30', 'IMXD60', 'CORD5', 'CORD10', 'CORD20', 'CORD30', 'CORD60', 'CNTP5', 'CNTP10', 'CNTP20', 'CNTP30', 'CNTP60', 'CNTN5', 'CNTN10', 'CNTN20', 'CNTN30', 'CNTN60', 'CNTD5', 'CNTD10', 'CNTD20', 'CNTD30', 'CNTD60', 'SUMP5', 'SUMP10', 'SUMP20', 'SUMP30', 'SUMP60', 'SUMN5', 'SUMN10', 'SUMN20', 'SUMN30', 'SUMN60', 'SUMD5', 'SUMD10', 'SUMD20', 'SUMD30', 'SUMD60', 'VMA5', 'VMA10', 'VMA20', 'VMA30', 'VMA60', 'VSTD5', 'VSTD10', 'VSTD20', 'VSTD30', 'VSTD60', 'WVMA5', 'WVMA10', 'WVMA20', 'WVMA30', 'WVMA60', 'VSUMP5', 'VSUMP10', 'VSUMP20', 'VSUMP30', 'VSUMP60', 'VSUMN5', 'VSUMN10', 'VSUMN20', 'VSUMN30', 'VSUMN60', 'VSUMD5', 'VSUMD10', 'VSUMD20', 'VSUMD30', 'VSUMD60']\n"
     ]
    }
   ],
   "source": [
    "print([i for i in df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 选择固定时间区间的数据\n",
    "train_start_date = pd.to_datetime('2018-01-01')\n",
    "train_end_date = pd.to_datetime('2021-12-31')\n",
    "val_start_date = pd.to_datetime('2022-01-01')\n",
    "val_end_date = pd.to_datetime('2022-04-01')\n",
    "\n",
    "train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "validation_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 15:46:27.711016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow处理原始数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_boundaries(series, num_bins=40):\n",
    "    boundaries = pd.qcut(series, num_bins, retbins=True, duplicates='drop')[1].tolist()\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_FEATURE_NAME = [\"label\"]\n",
    "\n",
    "NUMERIC_FEATURES = ['KMID','KLEN','KMID2','KUP','KUP2','KLOW','KLOW2','KSFT','KSFT2','OPEN0','OPEN1','OPEN2','OPEN3','OPEN4','HIGH0','HIGH1','HIGH2','HIGH3','HIGH4','LOW0','LOW1','LOW2','LOW3','LOW4','CLOSE0','CLOSE1','CLOSE2','CLOSE3','CLOSE4','VOLUME0','VOLUME1','VOLUME2','VOLUME3','VOLUME4','ROC5','ROC10','ROC20','ROC30','ROC60','MAX5','MAX10','MAX20','MAX30','MAX60','MIN5','MIN10','MIN20','MIN30','MIN60','MA5','MA10','MA20','MA30','MA60','STD5','STD10','STD20','STD30','STD60','BETA5','BETA10','BETA20','BETA30','BETA60','RSQR5','RSQR10','RSQR20','RSQR30','RSQR60','RESI5','RESI10','RESI20','RESI30','RESI60','QTLU5','QTLU10','QTLU20','QTLU30','QTLU60','QTLD5','QTLD10','QTLD20','QTLD30','QTLD60','TSRANK5','TSRANK10','TSRANK20','TSRANK30','TSRANK60','RSV5','RSV10','RSV20','RSV30','RSV60','IMAX5','IMAX10','IMAX20','IMAX30','IMAX60','IMIN5','IMIN10','IMIN20','IMIN30','IMIN60','IMXD5','IMXD10','IMXD20','IMXD30','IMXD60','CORD5','CORD10','CORD20','CORD30','CORD60','CNTP5','CNTP10','CNTP20','CNTP30','CNTP60','CNTN5','CNTN10','CNTN20','CNTN30','CNTN60','CNTD5','CNTD10','CNTD20','CNTD30','CNTD60','SUMP5','SUMP10','SUMP20','SUMP30','SUMP60','SUMN5','SUMN10','SUMN20','SUMN30','SUMN60','SUMD5','SUMD10','SUMD20','SUMD30','SUMD60','VMA5','VMA10','VMA20','VMA30','VMA60','VSTD5','VSTD10','VSTD20','VSTD30','VSTD60','WVMA5','WVMA10','WVMA20','WVMA30','WVMA60','VSUMP5','VSUMP10','VSUMP20','VSUMP30','VSUMP60','VSUMN5','VSUMN10','VSUMN20','VSUMN30','VSUMN60','VSUMD5','VSUMD10','VSUMD20','VSUMD30','VSUMD60']\n",
    "NUMERIC_FEATURES_WITH_BOUNDARIES = {k:get_numeric_boundaries(train_data[k])  for k in NUMERIC_FEATURES}\n",
    "\n",
    "INTEGER_CATEGORICAL_FEATURES = []\n",
    "INTEGER_CATEGORICAL_FEATURES_WITH_VOCAB = {}\n",
    "\n",
    "STRING_CATEGORICAL_FEATURES = []\n",
    "STRING_CATEGORICAL_FEATURES_WITH_VOCAB = {}\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURES + INTEGER_CATEGORICAL_FEATURES + STRING_CATEGORICAL_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_data[FEATURE_NAMES], train_data[TARGET_FEATURE_NAME]\n",
    "X_val, y_val = validation_data[FEATURE_NAMES], validation_data[TARGET_FEATURE_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    # 处理连续特征\n",
    "    for feature_name in NUMERIC_FEATURES:\n",
    "        inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "\n",
    "    # 处理INTEGER离散特征\n",
    "    for feature_name in INTEGER_CATEGORICAL_FEATURES:\n",
    "        inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"int32\"\n",
    "            )\n",
    "\n",
    "    # 处理STRING离散特征\n",
    "    for feature_name in STRING_CATEGORICAL_FEATURES:\n",
    "        inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"string\"\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    embedding_dim = 6\n",
    "\n",
    "    # 处理连续特征\n",
    "    for feature_name, boundaries in NUMERIC_FEATURES_WITH_BOUNDARIES.items():\n",
    "        \n",
    "        lookup_layer = tf.keras.layers.Discretization(bin_boundaries=boundaries,output_mode='int')\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=len(boundaries) + 1, output_dim=embedding_dim\n",
    "        )\n",
    "        encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    # 处理INTEGER离散特征\n",
    "    for feature_name, integer_vocab in INTEGER_CATEGORICAL_FEATURES_WITH_VOCAB.items():\n",
    "        lookup_layer = tf.keras.layers.IntegerLookup(vocabulary=integer_vocab)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=len(integer_vocab) + 1, output_dim=embedding_dim\n",
    "        )\n",
    "        encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        encoded_features.append(encoded_feature)\n",
    "    \n",
    "    # 处理STRING离散特征\n",
    "    for feature_name, string_vocab in STRING_CATEGORICAL_FEATURES_WITH_VOCAB.items():\n",
    "        lookup_layer = tf.keras.layers.StringLookup(vocabulary=string_vocab)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=len(string_vocab) + 1, output_dim=embedding_dim\n",
    "        )\n",
    "        encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    return encoded_features\n",
    "\n",
    "inputs = create_model_inputs()\n",
    "encoded_features = encode_inputs(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Senet(tf.keras.layers.Layer):\n",
    "    def __init__(self, reduction_ratio=3, seed=1024, **kwargs):\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.seed = seed\n",
    "        super(Senet, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.field_size = len(input_shape)\n",
    "        self.reduction_size = max(1, self.field_size // self.reduction_ratio)\n",
    "        self.scale_layer = tf.keras.layers.Dense(units=self.reduction_size, activation='relu')\n",
    "        self.expand_layer = tf.keras.layers.Dense(units=self.field_size, activation='relu')\n",
    "        super(Senet, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        inputs = tf.concat(inputs, axis=1) # [B, N, dim]\n",
    "        Z = tf.reduce_mean(inputs, axis=-1) # [B, N]\n",
    "        A_1 = self.scale_layer(Z) # [B, X]\n",
    "        A_2 = self.expand_layer(A_1) # [B, N]\n",
    "        scale_inputs = tf.multiply(inputs, tf.expand_dims(A_2, axis=-1))\n",
    "        output = scale_inputs + inputs # skip-connection\n",
    "        return output # [B, N, dim]\n",
    "\n",
    "\n",
    "class Dnn(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_units, activation=\"relu\", dropout_rate=0.2, use_bn=False, seed=1024, **kwargs):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_bn = use_bn\n",
    "        self.seed = seed\n",
    "        self.dense_layers = []\n",
    "        self.dropout_layers = []\n",
    "        self.bn_layers = []\n",
    "        super(Dnn, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        for units in self.hidden_units:\n",
    "            self.dense_layers.append(tf.keras.layers.Dense(units=units, activation=self.activation))\n",
    "            self.dropout_layers.append(tf.keras.layers.Dropout(rate=self.dropout_rate, seed=self.seed))\n",
    "            if self.use_bn:\n",
    "                self.bn_layers.append(tf.keras.layers.BatchNormalization())\n",
    "        super(Dnn, self).build(input_shape)  # Be sure to call this at the end\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        for i in range(len(self.hidden_units)):\n",
    "            x = self.dense_layers[i](x)\n",
    "            if self.use_bn:\n",
    "                x = self.bn_layers[i](x, training=training)\n",
    "            x = self.dropout_layers[i](x, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantModel(tf.keras.Model):\n",
    "\tdef __init__(self, config, **kwargs):\n",
    "\t\tsuper(QuantModel, self).__init__(**kwargs)\n",
    "\t\tself.config = config\n",
    "\n",
    "\t\tself.senet_layer = Senet(\n",
    "\t\t\treduction_ratio=self.config.get('reduction_ratio', 3), \n",
    "\t\t\tseed=self.config.get('seed', 1024),\n",
    "\t\t)\n",
    "\t\tself.dnn_layer = Dnn(\n",
    "\t\t\thidden_units=self.config.get('dnn_hidden_units', [64,32]),\n",
    "\t\t\tactivation=self.config.get('dnn_activation', 'relu'),\n",
    "\t\t\tdropout_rate=self.config.get('dnn_dropout', 0.2),\n",
    "\t\t\tuse_bn=self.config.get('dnn_use_bn', True)\n",
    "\t\t)\n",
    "\t\tself.output_layer = tf.keras.layers.Dense(1, activation=None)\n",
    "\t\t\n",
    "\n",
    "\tdef call(self, inputs, training=None):\n",
    "\t\tencoded_features = encode_inputs(inputs)\n",
    "\t\tsenet_output = self.senet_layer(encoded_features, training=training)\n",
    "\t\tsenet_output = tf.keras.layers.Flatten(senet_output) # [B, N * dim]\n",
    "\t\tdnn_output = self.dnn_layer(senet_output, training=training)\n",
    "\t\toutput = self.output_layer(dnn_output, training=training)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_code</th>\n",
       "      <th>datetime</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>label</th>\n",
       "      <th>KMID</th>\n",
       "      <th>KLEN</th>\n",
       "      <th>KMID2</th>\n",
       "      <th>KUP</th>\n",
       "      <th>KUP2</th>\n",
       "      <th>KLOW</th>\n",
       "      <th>KLOW2</th>\n",
       "      <th>KSFT</th>\n",
       "      <th>KSFT2</th>\n",
       "      <th>OPEN0</th>\n",
       "      <th>OPEN1</th>\n",
       "      <th>OPEN2</th>\n",
       "      <th>OPEN3</th>\n",
       "      <th>OPEN4</th>\n",
       "      <th>HIGH0</th>\n",
       "      <th>HIGH1</th>\n",
       "      <th>HIGH2</th>\n",
       "      <th>HIGH3</th>\n",
       "      <th>HIGH4</th>\n",
       "      <th>LOW0</th>\n",
       "      <th>LOW1</th>\n",
       "      <th>LOW2</th>\n",
       "      <th>LOW3</th>\n",
       "      <th>LOW4</th>\n",
       "      <th>CLOSE0</th>\n",
       "      <th>CLOSE1</th>\n",
       "      <th>CLOSE2</th>\n",
       "      <th>CLOSE3</th>\n",
       "      <th>CLOSE4</th>\n",
       "      <th>VOLUME0</th>\n",
       "      <th>VOLUME1</th>\n",
       "      <th>VOLUME2</th>\n",
       "      <th>VOLUME3</th>\n",
       "      <th>VOLUME4</th>\n",
       "      <th>ROC5</th>\n",
       "      <th>ROC10</th>\n",
       "      <th>ROC20</th>\n",
       "      <th>ROC30</th>\n",
       "      <th>ROC60</th>\n",
       "      <th>MAX5</th>\n",
       "      <th>MAX10</th>\n",
       "      <th>MAX20</th>\n",
       "      <th>MAX30</th>\n",
       "      <th>MAX60</th>\n",
       "      <th>MIN5</th>\n",
       "      <th>MIN10</th>\n",
       "      <th>MIN20</th>\n",
       "      <th>MIN30</th>\n",
       "      <th>MIN60</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA10</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA30</th>\n",
       "      <th>MA60</th>\n",
       "      <th>STD5</th>\n",
       "      <th>STD10</th>\n",
       "      <th>STD20</th>\n",
       "      <th>STD30</th>\n",
       "      <th>STD60</th>\n",
       "      <th>BETA5</th>\n",
       "      <th>BETA10</th>\n",
       "      <th>BETA20</th>\n",
       "      <th>BETA30</th>\n",
       "      <th>BETA60</th>\n",
       "      <th>RSQR5</th>\n",
       "      <th>RSQR10</th>\n",
       "      <th>RSQR20</th>\n",
       "      <th>RSQR30</th>\n",
       "      <th>RSQR60</th>\n",
       "      <th>RESI5</th>\n",
       "      <th>RESI10</th>\n",
       "      <th>RESI20</th>\n",
       "      <th>RESI30</th>\n",
       "      <th>RESI60</th>\n",
       "      <th>QTLU5</th>\n",
       "      <th>QTLU10</th>\n",
       "      <th>QTLU20</th>\n",
       "      <th>QTLU30</th>\n",
       "      <th>QTLU60</th>\n",
       "      <th>QTLD5</th>\n",
       "      <th>QTLD10</th>\n",
       "      <th>QTLD20</th>\n",
       "      <th>QTLD30</th>\n",
       "      <th>QTLD60</th>\n",
       "      <th>TSRANK5</th>\n",
       "      <th>TSRANK10</th>\n",
       "      <th>TSRANK20</th>\n",
       "      <th>TSRANK30</th>\n",
       "      <th>TSRANK60</th>\n",
       "      <th>RSV5</th>\n",
       "      <th>RSV10</th>\n",
       "      <th>RSV20</th>\n",
       "      <th>RSV30</th>\n",
       "      <th>RSV60</th>\n",
       "      <th>IMAX5</th>\n",
       "      <th>IMAX10</th>\n",
       "      <th>IMAX20</th>\n",
       "      <th>IMAX30</th>\n",
       "      <th>IMAX60</th>\n",
       "      <th>IMIN5</th>\n",
       "      <th>IMIN10</th>\n",
       "      <th>IMIN20</th>\n",
       "      <th>IMIN30</th>\n",
       "      <th>IMIN60</th>\n",
       "      <th>IMXD5</th>\n",
       "      <th>IMXD10</th>\n",
       "      <th>IMXD20</th>\n",
       "      <th>IMXD30</th>\n",
       "      <th>IMXD60</th>\n",
       "      <th>CORD5</th>\n",
       "      <th>CORD10</th>\n",
       "      <th>CORD20</th>\n",
       "      <th>CORD30</th>\n",
       "      <th>CORD60</th>\n",
       "      <th>CNTP5</th>\n",
       "      <th>CNTP10</th>\n",
       "      <th>CNTP20</th>\n",
       "      <th>CNTP30</th>\n",
       "      <th>CNTP60</th>\n",
       "      <th>CNTN5</th>\n",
       "      <th>CNTN10</th>\n",
       "      <th>CNTN20</th>\n",
       "      <th>CNTN30</th>\n",
       "      <th>CNTN60</th>\n",
       "      <th>CNTD5</th>\n",
       "      <th>CNTD10</th>\n",
       "      <th>CNTD20</th>\n",
       "      <th>CNTD30</th>\n",
       "      <th>CNTD60</th>\n",
       "      <th>SUMP5</th>\n",
       "      <th>SUMP10</th>\n",
       "      <th>SUMP20</th>\n",
       "      <th>SUMP30</th>\n",
       "      <th>SUMP60</th>\n",
       "      <th>SUMN5</th>\n",
       "      <th>SUMN10</th>\n",
       "      <th>SUMN20</th>\n",
       "      <th>SUMN30</th>\n",
       "      <th>SUMN60</th>\n",
       "      <th>SUMD5</th>\n",
       "      <th>SUMD10</th>\n",
       "      <th>SUMD20</th>\n",
       "      <th>SUMD30</th>\n",
       "      <th>SUMD60</th>\n",
       "      <th>VMA5</th>\n",
       "      <th>VMA10</th>\n",
       "      <th>VMA20</th>\n",
       "      <th>VMA30</th>\n",
       "      <th>VMA60</th>\n",
       "      <th>VSTD5</th>\n",
       "      <th>VSTD10</th>\n",
       "      <th>VSTD20</th>\n",
       "      <th>VSTD30</th>\n",
       "      <th>VSTD60</th>\n",
       "      <th>WVMA5</th>\n",
       "      <th>WVMA10</th>\n",
       "      <th>WVMA20</th>\n",
       "      <th>WVMA30</th>\n",
       "      <th>WVMA60</th>\n",
       "      <th>VSUMP5</th>\n",
       "      <th>VSUMP10</th>\n",
       "      <th>VSUMP20</th>\n",
       "      <th>VSUMP30</th>\n",
       "      <th>VSUMP60</th>\n",
       "      <th>VSUMN5</th>\n",
       "      <th>VSUMN10</th>\n",
       "      <th>VSUMN20</th>\n",
       "      <th>VSUMN30</th>\n",
       "      <th>VSUMN60</th>\n",
       "      <th>VSUMD5</th>\n",
       "      <th>VSUMD10</th>\n",
       "      <th>VSUMD20</th>\n",
       "      <th>VSUMD30</th>\n",
       "      <th>VSUMD60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6367</th>\n",
       "      <td>000001</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2228.23</td>\n",
       "      <td>2322.50</td>\n",
       "      <td>2223.36</td>\n",
       "      <td>2285.12</td>\n",
       "      <td>2081593</td>\n",
       "      <td>0.953869</td>\n",
       "      <td>0.025531</td>\n",
       "      <td>0.044493</td>\n",
       "      <td>0.573835</td>\n",
       "      <td>0.016776</td>\n",
       "      <td>0.377043</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.049122</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.245915</td>\n",
       "      <td>0.975104</td>\n",
       "      <td>0.965148</td>\n",
       "      <td>0.970128</td>\n",
       "      <td>0.991467</td>\n",
       "      <td>0.968706</td>\n",
       "      <td>1.016358</td>\n",
       "      <td>0.980798</td>\n",
       "      <td>0.982929</td>\n",
       "      <td>1.005689</td>\n",
       "      <td>0.999287</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>0.957324</td>\n",
       "      <td>0.951635</td>\n",
       "      <td>0.963726</td>\n",
       "      <td>0.963726</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971551</td>\n",
       "      <td>0.965148</td>\n",
       "      <td>0.970837</td>\n",
       "      <td>0.997156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.472194</td>\n",
       "      <td>0.746078</td>\n",
       "      <td>0.656501</td>\n",
       "      <td>0.539821</td>\n",
       "      <td>0.967993</td>\n",
       "      <td>0.932432</td>\n",
       "      <td>0.971551</td>\n",
       "      <td>1.039118</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>1.016358</td>\n",
       "      <td>1.016358</td>\n",
       "      <td>1.016358</td>\n",
       "      <td>1.016358</td>\n",
       "      <td>1.016358</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.004376</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.013128</td>\n",
       "      <td>0.026257</td>\n",
       "      <td>0.980938</td>\n",
       "      <td>0.978733</td>\n",
       "      <td>0.965967</td>\n",
       "      <td>0.983451</td>\n",
       "      <td>0.927796</td>\n",
       "      <td>0.016323</td>\n",
       "      <td>0.013121</td>\n",
       "      <td>0.019814</td>\n",
       "      <td>0.039477</td>\n",
       "      <td>0.072978</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>-0.002354</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>0.003846</td>\n",
       "      <td>0.044889</td>\n",
       "      <td>0.245784</td>\n",
       "      <td>0.275671</td>\n",
       "      <td>0.539084</td>\n",
       "      <td>0.017781</td>\n",
       "      <td>0.017135</td>\n",
       "      <td>0.018259</td>\n",
       "      <td>0.050688</td>\n",
       "      <td>-0.018306</td>\n",
       "      <td>0.997724</td>\n",
       "      <td>0.990325</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>1.001707</td>\n",
       "      <td>0.986058</td>\n",
       "      <td>0.969700</td>\n",
       "      <td>0.968564</td>\n",
       "      <td>0.951350</td>\n",
       "      <td>0.955048</td>\n",
       "      <td>0.846229</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>23.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.983836</td>\n",
       "      <td>0.983765</td>\n",
       "      <td>0.983695</td>\n",
       "      <td>0.983478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.085307</td>\n",
       "      <td>0.291894</td>\n",
       "      <td>0.393290</td>\n",
       "      <td>0.297331</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.543103</td>\n",
       "      <td>0.467836</td>\n",
       "      <td>0.582988</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.456897</td>\n",
       "      <td>0.532164</td>\n",
       "      <td>0.417012</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.086205</td>\n",
       "      <td>-0.064327</td>\n",
       "      <td>0.165975</td>\n",
       "      <td>0.682919</td>\n",
       "      <td>0.693010</td>\n",
       "      <td>0.684679</td>\n",
       "      <td>0.778932</td>\n",
       "      <td>0.695393</td>\n",
       "      <td>0.206222</td>\n",
       "      <td>0.241635</td>\n",
       "      <td>0.219330</td>\n",
       "      <td>0.258516</td>\n",
       "      <td>0.359090</td>\n",
       "      <td>0.762316</td>\n",
       "      <td>0.976635</td>\n",
       "      <td>0.942986</td>\n",
       "      <td>0.873075</td>\n",
       "      <td>1.285542</td>\n",
       "      <td>0.596880</td>\n",
       "      <td>0.586130</td>\n",
       "      <td>0.528651</td>\n",
       "      <td>0.472186</td>\n",
       "      <td>0.512911</td>\n",
       "      <td>0.403120</td>\n",
       "      <td>0.413870</td>\n",
       "      <td>0.471349</td>\n",
       "      <td>0.527814</td>\n",
       "      <td>0.487089</td>\n",
       "      <td>0.193759</td>\n",
       "      <td>0.172261</td>\n",
       "      <td>0.057302</td>\n",
       "      <td>-0.055627</td>\n",
       "      <td>0.025821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6368</th>\n",
       "      <td>000001</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>2289.99</td>\n",
       "      <td>2311.12</td>\n",
       "      <td>2203.85</td>\n",
       "      <td>2224.98</td>\n",
       "      <td>2962498</td>\n",
       "      <td>1.010965</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>0.046843</td>\n",
       "      <td>-0.606041</td>\n",
       "      <td>0.009227</td>\n",
       "      <td>0.196980</td>\n",
       "      <td>0.009227</td>\n",
       "      <td>0.196980</td>\n",
       "      <td>-0.028389</td>\n",
       "      <td>-0.606041</td>\n",
       "      <td>1.029218</td>\n",
       "      <td>1.001461</td>\n",
       "      <td>0.991236</td>\n",
       "      <td>0.996351</td>\n",
       "      <td>1.018265</td>\n",
       "      <td>1.038715</td>\n",
       "      <td>1.043830</td>\n",
       "      <td>1.007308</td>\n",
       "      <td>1.009497</td>\n",
       "      <td>1.032872</td>\n",
       "      <td>0.990503</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.983200</td>\n",
       "      <td>0.977357</td>\n",
       "      <td>0.989775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.027029</td>\n",
       "      <td>0.997811</td>\n",
       "      <td>0.991236</td>\n",
       "      <td>0.997079</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702648</td>\n",
       "      <td>0.331786</td>\n",
       "      <td>0.524230</td>\n",
       "      <td>0.461289</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>0.996351</td>\n",
       "      <td>0.997811</td>\n",
       "      <td>1.081812</td>\n",
       "      <td>0.864134</td>\n",
       "      <td>1.038715</td>\n",
       "      <td>1.038715</td>\n",
       "      <td>1.038715</td>\n",
       "      <td>1.038715</td>\n",
       "      <td>1.038715</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.004494</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.013483</td>\n",
       "      <td>0.026967</td>\n",
       "      <td>1.002631</td>\n",
       "      <td>1.005553</td>\n",
       "      <td>0.992186</td>\n",
       "      <td>1.007306</td>\n",
       "      <td>0.955139</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.013257</td>\n",
       "      <td>0.020388</td>\n",
       "      <td>0.038235</td>\n",
       "      <td>0.074274</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>-0.001988</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.220526</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.306869</td>\n",
       "      <td>0.209467</td>\n",
       "      <td>0.523765</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>-0.005872</td>\n",
       "      <td>-0.010322</td>\n",
       "      <td>0.021517</td>\n",
       "      <td>-0.045936</td>\n",
       "      <td>1.005406</td>\n",
       "      <td>1.017093</td>\n",
       "      <td>1.012710</td>\n",
       "      <td>1.027029</td>\n",
       "      <td>1.012710</td>\n",
       "      <td>0.995910</td>\n",
       "      <td>0.994743</td>\n",
       "      <td>0.977065</td>\n",
       "      <td>0.980863</td>\n",
       "      <td>0.869248</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.962647</td>\n",
       "      <td>0.962566</td>\n",
       "      <td>0.962403</td>\n",
       "      <td>0.962238</td>\n",
       "      <td>0.961735</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.283575</td>\n",
       "      <td>-0.114669</td>\n",
       "      <td>0.337841</td>\n",
       "      <td>0.273088</td>\n",
       "      <td>0.265908</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374037</td>\n",
       "      <td>0.510814</td>\n",
       "      <td>0.502990</td>\n",
       "      <td>0.435781</td>\n",
       "      <td>0.563437</td>\n",
       "      <td>0.625963</td>\n",
       "      <td>0.489186</td>\n",
       "      <td>0.497010</td>\n",
       "      <td>0.564219</td>\n",
       "      <td>0.436563</td>\n",
       "      <td>-0.251926</td>\n",
       "      <td>0.021628</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>-0.128439</td>\n",
       "      <td>0.126874</td>\n",
       "      <td>0.603991</td>\n",
       "      <td>0.505999</td>\n",
       "      <td>0.501997</td>\n",
       "      <td>0.552571</td>\n",
       "      <td>0.501075</td>\n",
       "      <td>0.258543</td>\n",
       "      <td>0.214745</td>\n",
       "      <td>0.192167</td>\n",
       "      <td>0.192437</td>\n",
       "      <td>0.258827</td>\n",
       "      <td>0.824741</td>\n",
       "      <td>0.889581</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.861786</td>\n",
       "      <td>1.251970</td>\n",
       "      <td>0.808625</td>\n",
       "      <td>0.542153</td>\n",
       "      <td>0.553568</td>\n",
       "      <td>0.516407</td>\n",
       "      <td>0.537413</td>\n",
       "      <td>0.191375</td>\n",
       "      <td>0.457847</td>\n",
       "      <td>0.446432</td>\n",
       "      <td>0.483593</td>\n",
       "      <td>0.462587</td>\n",
       "      <td>0.617250</td>\n",
       "      <td>0.084305</td>\n",
       "      <td>0.107135</td>\n",
       "      <td>0.032815</td>\n",
       "      <td>0.074826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369</th>\n",
       "      <td>000001</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>2223.36</td>\n",
       "      <td>2231.48</td>\n",
       "      <td>2192.48</td>\n",
       "      <td>2211.98</td>\n",
       "      <td>1854509</td>\n",
       "      <td>1.014001</td>\n",
       "      <td>-0.005118</td>\n",
       "      <td>0.017541</td>\n",
       "      <td>-0.291795</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.208205</td>\n",
       "      <td>0.008771</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.005145</td>\n",
       "      <td>1.035267</td>\n",
       "      <td>1.007346</td>\n",
       "      <td>0.997061</td>\n",
       "      <td>1.002206</td>\n",
       "      <td>1.008816</td>\n",
       "      <td>1.044820</td>\n",
       "      <td>1.049964</td>\n",
       "      <td>1.013228</td>\n",
       "      <td>1.015430</td>\n",
       "      <td>0.991184</td>\n",
       "      <td>0.996325</td>\n",
       "      <td>1.005145</td>\n",
       "      <td>0.988978</td>\n",
       "      <td>0.983101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.005877</td>\n",
       "      <td>1.033065</td>\n",
       "      <td>1.003675</td>\n",
       "      <td>0.997061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.597457</td>\n",
       "      <td>1.122450</td>\n",
       "      <td>0.530014</td>\n",
       "      <td>0.837435</td>\n",
       "      <td>1.002939</td>\n",
       "      <td>1.000737</td>\n",
       "      <td>0.987509</td>\n",
       "      <td>1.135933</td>\n",
       "      <td>0.873620</td>\n",
       "      <td>1.008816</td>\n",
       "      <td>1.008816</td>\n",
       "      <td>1.008816</td>\n",
       "      <td>1.008816</td>\n",
       "      <td>1.008816</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>0.009042</td>\n",
       "      <td>0.013563</td>\n",
       "      <td>0.027125</td>\n",
       "      <td>1.007936</td>\n",
       "      <td>1.011389</td>\n",
       "      <td>0.998641</td>\n",
       "      <td>1.008695</td>\n",
       "      <td>0.962858</td>\n",
       "      <td>0.014449</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.020361</td>\n",
       "      <td>0.030737</td>\n",
       "      <td>0.073990</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>-0.001269</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>-0.001239</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>0.007815</td>\n",
       "      <td>0.082178</td>\n",
       "      <td>0.268310</td>\n",
       "      <td>0.125837</td>\n",
       "      <td>0.505647</td>\n",
       "      <td>-0.009552</td>\n",
       "      <td>-0.005678</td>\n",
       "      <td>-0.015577</td>\n",
       "      <td>0.009264</td>\n",
       "      <td>-0.051731</td>\n",
       "      <td>1.011315</td>\n",
       "      <td>1.023071</td>\n",
       "      <td>1.018662</td>\n",
       "      <td>1.030715</td>\n",
       "      <td>1.018662</td>\n",
       "      <td>0.999412</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982807</td>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.874943</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>11.5</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0.991242</td>\n",
       "      <td>0.991222</td>\n",
       "      <td>0.991182</td>\n",
       "      <td>0.991142</td>\n",
       "      <td>0.991020</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.335234</td>\n",
       "      <td>-0.101648</td>\n",
       "      <td>0.339191</td>\n",
       "      <td>0.291733</td>\n",
       "      <td>0.270499</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.480396</td>\n",
       "      <td>0.497884</td>\n",
       "      <td>0.517454</td>\n",
       "      <td>0.386502</td>\n",
       "      <td>0.558583</td>\n",
       "      <td>0.519604</td>\n",
       "      <td>0.502116</td>\n",
       "      <td>0.482546</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.441417</td>\n",
       "      <td>-0.039209</td>\n",
       "      <td>-0.004232</td>\n",
       "      <td>0.034908</td>\n",
       "      <td>-0.226997</td>\n",
       "      <td>0.117166</td>\n",
       "      <td>1.017471</td>\n",
       "      <td>0.848660</td>\n",
       "      <td>0.808245</td>\n",
       "      <td>0.869844</td>\n",
       "      <td>0.811198</td>\n",
       "      <td>0.392981</td>\n",
       "      <td>0.339072</td>\n",
       "      <td>0.309821</td>\n",
       "      <td>0.293380</td>\n",
       "      <td>0.410056</td>\n",
       "      <td>1.021606</td>\n",
       "      <td>0.830552</td>\n",
       "      <td>0.904752</td>\n",
       "      <td>0.866688</td>\n",
       "      <td>1.244961</td>\n",
       "      <td>0.563466</td>\n",
       "      <td>0.557445</td>\n",
       "      <td>0.509336</td>\n",
       "      <td>0.476564</td>\n",
       "      <td>0.519540</td>\n",
       "      <td>0.436534</td>\n",
       "      <td>0.442555</td>\n",
       "      <td>0.490664</td>\n",
       "      <td>0.523436</td>\n",
       "      <td>0.480460</td>\n",
       "      <td>0.126931</td>\n",
       "      <td>0.114891</td>\n",
       "      <td>0.018673</td>\n",
       "      <td>-0.046872</td>\n",
       "      <td>0.039081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>000001</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>2205.48</td>\n",
       "      <td>2228.23</td>\n",
       "      <td>2195.73</td>\n",
       "      <td>2220.11</td>\n",
       "      <td>1210313</td>\n",
       "      <td>1.022044</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.014736</td>\n",
       "      <td>0.450154</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.249846</td>\n",
       "      <td>0.004421</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.007373</td>\n",
       "      <td>0.500308</td>\n",
       "      <td>0.993410</td>\n",
       "      <td>1.001464</td>\n",
       "      <td>1.031476</td>\n",
       "      <td>1.003657</td>\n",
       "      <td>0.993410</td>\n",
       "      <td>1.003657</td>\n",
       "      <td>1.005121</td>\n",
       "      <td>1.040993</td>\n",
       "      <td>1.046119</td>\n",
       "      <td>1.009518</td>\n",
       "      <td>0.989019</td>\n",
       "      <td>0.987555</td>\n",
       "      <td>0.992676</td>\n",
       "      <td>1.001464</td>\n",
       "      <td>0.985357</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996338</td>\n",
       "      <td>1.002194</td>\n",
       "      <td>1.029282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.532256</td>\n",
       "      <td>2.447712</td>\n",
       "      <td>1.719880</td>\n",
       "      <td>0.812117</td>\n",
       "      <td>0.993410</td>\n",
       "      <td>1.017567</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>1.076866</td>\n",
       "      <td>0.871885</td>\n",
       "      <td>1.003657</td>\n",
       "      <td>1.003657</td>\n",
       "      <td>1.003657</td>\n",
       "      <td>1.003657</td>\n",
       "      <td>1.003657</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.004504</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.013513</td>\n",
       "      <td>0.027026</td>\n",
       "      <td>1.005563</td>\n",
       "      <td>1.005929</td>\n",
       "      <td>0.996705</td>\n",
       "      <td>1.002439</td>\n",
       "      <td>0.961468</td>\n",
       "      <td>0.013425</td>\n",
       "      <td>0.013062</td>\n",
       "      <td>0.019086</td>\n",
       "      <td>0.027456</td>\n",
       "      <td>0.072995</td>\n",
       "      <td>-0.003294</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.150541</td>\n",
       "      <td>0.056428</td>\n",
       "      <td>0.183972</td>\n",
       "      <td>0.061060</td>\n",
       "      <td>0.488085</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>-0.001317</td>\n",
       "      <td>-0.009850</td>\n",
       "      <td>0.008736</td>\n",
       "      <td>-0.047609</td>\n",
       "      <td>1.007611</td>\n",
       "      <td>1.018153</td>\n",
       "      <td>1.014932</td>\n",
       "      <td>1.019324</td>\n",
       "      <td>1.014932</td>\n",
       "      <td>0.999268</td>\n",
       "      <td>0.996338</td>\n",
       "      <td>0.983602</td>\n",
       "      <td>0.983014</td>\n",
       "      <td>0.872326</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>13.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>43.5</td>\n",
       "      <td>0.996348</td>\n",
       "      <td>0.996339</td>\n",
       "      <td>0.996323</td>\n",
       "      <td>0.996306</td>\n",
       "      <td>0.996255</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.293804</td>\n",
       "      <td>-0.176231</td>\n",
       "      <td>0.308076</td>\n",
       "      <td>0.285791</td>\n",
       "      <td>0.267780</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.545460</td>\n",
       "      <td>0.443937</td>\n",
       "      <td>0.550322</td>\n",
       "      <td>0.429532</td>\n",
       "      <td>0.559484</td>\n",
       "      <td>0.454540</td>\n",
       "      <td>0.556063</td>\n",
       "      <td>0.449678</td>\n",
       "      <td>0.570468</td>\n",
       "      <td>0.440516</td>\n",
       "      <td>0.090920</td>\n",
       "      <td>-0.112127</td>\n",
       "      <td>0.100644</td>\n",
       "      <td>-0.140936</td>\n",
       "      <td>0.118968</td>\n",
       "      <td>1.502393</td>\n",
       "      <td>1.277651</td>\n",
       "      <td>1.239610</td>\n",
       "      <td>1.299256</td>\n",
       "      <td>1.251669</td>\n",
       "      <td>0.646278</td>\n",
       "      <td>0.527998</td>\n",
       "      <td>0.474074</td>\n",
       "      <td>0.434810</td>\n",
       "      <td>0.621103</td>\n",
       "      <td>1.078485</td>\n",
       "      <td>0.956484</td>\n",
       "      <td>0.949877</td>\n",
       "      <td>0.849424</td>\n",
       "      <td>1.241542</td>\n",
       "      <td>0.460167</td>\n",
       "      <td>0.479722</td>\n",
       "      <td>0.501110</td>\n",
       "      <td>0.461363</td>\n",
       "      <td>0.510139</td>\n",
       "      <td>0.539833</td>\n",
       "      <td>0.520278</td>\n",
       "      <td>0.498890</td>\n",
       "      <td>0.538637</td>\n",
       "      <td>0.489861</td>\n",
       "      <td>-0.079667</td>\n",
       "      <td>-0.040556</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>-0.077274</td>\n",
       "      <td>0.020278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6371</th>\n",
       "      <td>000001</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>2211.98</td>\n",
       "      <td>2218.48</td>\n",
       "      <td>2148.60</td>\n",
       "      <td>2164.85</td>\n",
       "      <td>2158621</td>\n",
       "      <td>1.093092</td>\n",
       "      <td>-0.021307</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>-0.674442</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.093017</td>\n",
       "      <td>0.007346</td>\n",
       "      <td>0.232541</td>\n",
       "      <td>-0.016899</td>\n",
       "      <td>-0.534917</td>\n",
       "      <td>1.021771</td>\n",
       "      <td>1.018768</td>\n",
       "      <td>1.027027</td>\n",
       "      <td>1.057805</td>\n",
       "      <td>1.029277</td>\n",
       "      <td>1.024773</td>\n",
       "      <td>1.029277</td>\n",
       "      <td>1.030778</td>\n",
       "      <td>1.067566</td>\n",
       "      <td>1.072823</td>\n",
       "      <td>0.992494</td>\n",
       "      <td>1.014264</td>\n",
       "      <td>1.012763</td>\n",
       "      <td>1.018015</td>\n",
       "      <td>1.027027</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.025526</td>\n",
       "      <td>1.021771</td>\n",
       "      <td>1.027776</td>\n",
       "      <td>1.055556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560688</td>\n",
       "      <td>0.859117</td>\n",
       "      <td>1.372403</td>\n",
       "      <td>0.964316</td>\n",
       "      <td>1.025526</td>\n",
       "      <td>1.042040</td>\n",
       "      <td>1.009760</td>\n",
       "      <td>1.120119</td>\n",
       "      <td>0.879876</td>\n",
       "      <td>1.024773</td>\n",
       "      <td>1.024773</td>\n",
       "      <td>1.024773</td>\n",
       "      <td>1.024773</td>\n",
       "      <td>1.024773</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.009239</td>\n",
       "      <td>0.013858</td>\n",
       "      <td>0.027716</td>\n",
       "      <td>1.026126</td>\n",
       "      <td>1.027402</td>\n",
       "      <td>1.021659</td>\n",
       "      <td>1.024024</td>\n",
       "      <td>0.988012</td>\n",
       "      <td>0.019816</td>\n",
       "      <td>0.016084</td>\n",
       "      <td>0.020015</td>\n",
       "      <td>0.022603</td>\n",
       "      <td>0.073567</td>\n",
       "      <td>-0.011336</td>\n",
       "      <td>-0.002079</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.818138</td>\n",
       "      <td>0.153208</td>\n",
       "      <td>0.071894</td>\n",
       "      <td>0.017132</td>\n",
       "      <td>0.453784</td>\n",
       "      <td>-0.003453</td>\n",
       "      <td>-0.018045</td>\n",
       "      <td>-0.030276</td>\n",
       "      <td>-0.019151</td>\n",
       "      <td>-0.071723</td>\n",
       "      <td>1.033332</td>\n",
       "      <td>1.032731</td>\n",
       "      <td>1.040839</td>\n",
       "      <td>1.042340</td>\n",
       "      <td>1.040839</td>\n",
       "      <td>1.017416</td>\n",
       "      <td>1.021170</td>\n",
       "      <td>1.004204</td>\n",
       "      <td>1.004204</td>\n",
       "      <td>0.894893</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.975771</td>\n",
       "      <td>0.975716</td>\n",
       "      <td>0.975606</td>\n",
       "      <td>0.975494</td>\n",
       "      <td>0.975154</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.108308</td>\n",
       "      <td>-0.282224</td>\n",
       "      <td>0.224420</td>\n",
       "      <td>0.226055</td>\n",
       "      <td>0.243918</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.033333</td>\n",
       "      <td>0.362906</td>\n",
       "      <td>0.386189</td>\n",
       "      <td>0.486315</td>\n",
       "      <td>0.394461</td>\n",
       "      <td>0.553836</td>\n",
       "      <td>0.637094</td>\n",
       "      <td>0.613811</td>\n",
       "      <td>0.513685</td>\n",
       "      <td>0.605539</td>\n",
       "      <td>0.446164</td>\n",
       "      <td>-0.274189</td>\n",
       "      <td>-0.227622</td>\n",
       "      <td>-0.027370</td>\n",
       "      <td>-0.211078</td>\n",
       "      <td>0.107673</td>\n",
       "      <td>0.951305</td>\n",
       "      <td>0.781948</td>\n",
       "      <td>0.713734</td>\n",
       "      <td>0.721195</td>\n",
       "      <td>0.712770</td>\n",
       "      <td>0.291952</td>\n",
       "      <td>0.276419</td>\n",
       "      <td>0.273733</td>\n",
       "      <td>0.231611</td>\n",
       "      <td>0.347074</td>\n",
       "      <td>0.776483</td>\n",
       "      <td>0.784337</td>\n",
       "      <td>0.923774</td>\n",
       "      <td>0.844199</td>\n",
       "      <td>1.215696</td>\n",
       "      <td>0.625607</td>\n",
       "      <td>0.601353</td>\n",
       "      <td>0.529784</td>\n",
       "      <td>0.485727</td>\n",
       "      <td>0.522230</td>\n",
       "      <td>0.374393</td>\n",
       "      <td>0.398647</td>\n",
       "      <td>0.470216</td>\n",
       "      <td>0.514273</td>\n",
       "      <td>0.477770</td>\n",
       "      <td>0.251215</td>\n",
       "      <td>0.202707</td>\n",
       "      <td>0.059569</td>\n",
       "      <td>-0.028545</td>\n",
       "      <td>0.044459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stock_code    datetime     open     high      low    close   volume  \\\n",
       "6367     000001  2018-01-02  2228.23  2322.50  2223.36  2285.12  2081593   \n",
       "6368     000001  2018-01-03  2289.99  2311.12  2203.85  2224.98  2962498   \n",
       "6369     000001  2018-01-04  2223.36  2231.48  2192.48  2211.98  1854509   \n",
       "6370     000001  2018-01-05  2205.48  2228.23  2195.73  2220.11  1210313   \n",
       "6371     000001  2018-01-08  2211.98  2218.48  2148.60  2164.85  2158621   \n",
       "\n",
       "         label      KMID      KLEN     KMID2       KUP      KUP2      KLOW  \\\n",
       "6367  0.953869  0.025531  0.044493  0.573835  0.016776  0.377043  0.002186   \n",
       "6368  1.010965 -0.028389  0.046843 -0.606041  0.009227  0.196980  0.009227   \n",
       "6369  1.014001 -0.005118  0.017541 -0.291795  0.003652  0.208205  0.008771   \n",
       "6370  1.022044  0.006633  0.014736  0.450154  0.003682  0.249846  0.004421   \n",
       "6371  1.093092 -0.021307  0.031592 -0.674442  0.002939  0.093017  0.007346   \n",
       "\n",
       "         KLOW2      KSFT     KSFT2     OPEN0     OPEN1     OPEN2     OPEN3  \\\n",
       "6367  0.049122  0.010941  0.245915  0.975104  0.965148  0.970128  0.991467   \n",
       "6368  0.196980 -0.028389 -0.606041  1.029218  1.001461  0.991236  0.996351   \n",
       "6369  0.500000  0.000000  0.000000  1.005145  1.035267  1.007346  0.997061   \n",
       "6370  0.300000  0.007373  0.500308  0.993410  1.001464  1.031476  1.003657   \n",
       "6371  0.232541 -0.016899 -0.534917  1.021771  1.018768  1.027027  1.057805   \n",
       "\n",
       "         OPEN4     HIGH0     HIGH1     HIGH2     HIGH3     HIGH4      LOW0  \\\n",
       "6367  0.968706  1.016358  0.980798  0.982929  1.005689  0.999287  0.972973   \n",
       "6368  1.018265  1.038715  1.043830  1.007308  1.009497  1.032872  0.990503   \n",
       "6369  1.002206  1.008816  1.044820  1.049964  1.013228  1.015430  0.991184   \n",
       "6370  0.993410  1.003657  1.005121  1.040993  1.046119  1.009518  0.989019   \n",
       "6371  1.029277  1.024773  1.029277  1.030778  1.067566  1.072823  0.992494   \n",
       "\n",
       "          LOW1      LOW2      LOW3      LOW4  CLOSE0    CLOSE1    CLOSE2  \\\n",
       "6367  0.957324  0.951635  0.963726  0.963726     1.0  0.971551  0.965148   \n",
       "6368  0.999272  0.983200  0.977357  0.989775     1.0  1.027029  0.997811   \n",
       "6369  0.996325  1.005145  0.988978  0.983101     1.0  1.005877  1.033065   \n",
       "6370  0.987555  0.992676  1.001464  0.985357     1.0  0.996338  1.002194   \n",
       "6371  1.014264  1.012763  1.018015  1.027027     1.0  1.025526  1.021771   \n",
       "\n",
       "        CLOSE3    CLOSE4  VOLUME0   VOLUME1   VOLUME2   VOLUME3   VOLUME4  \\\n",
       "6367  0.970837  0.997156      1.0  0.472194  0.746078  0.656501  0.539821   \n",
       "6368  0.991236  0.997079      1.0  0.702648  0.331786  0.524230  0.461289   \n",
       "6369  1.003675  0.997061      1.0  1.597457  1.122450  0.530014  0.837435   \n",
       "6370  1.029282  1.000000      1.0  1.532256  2.447712  1.719880  0.812117   \n",
       "6371  1.027776  1.055556      1.0  0.560688  0.859117  1.372403  0.964316   \n",
       "\n",
       "          ROC5     ROC10     ROC20     ROC30     ROC60      MAX5     MAX10  \\\n",
       "6367  0.967993  0.932432  0.971551  1.039118  0.829300  1.016358  1.016358   \n",
       "6368  1.024108  0.996351  0.997811  1.081812  0.864134  1.038715  1.038715   \n",
       "6369  1.002939  1.000737  0.987509  1.135933  0.873620  1.008816  1.008816   \n",
       "6370  0.993410  1.017567  0.965592  1.076866  0.871885  1.003657  1.003657   \n",
       "6371  1.025526  1.042040  1.009760  1.120119  0.879876  1.024773  1.024773   \n",
       "\n",
       "         MAX20     MAX30     MAX60      MIN5     MIN10     MIN20     MIN30  \\\n",
       "6367  1.016358  1.016358  1.016358  0.002188  0.004376  0.008752  0.013128   \n",
       "6368  1.038715  1.038715  1.038715  0.002247  0.004494  0.008989  0.013483   \n",
       "6369  1.008816  1.008816  1.008816  0.002260  0.004521  0.009042  0.013563   \n",
       "6370  1.003657  1.003657  1.003657  0.002252  0.004504  0.009009  0.013513   \n",
       "6371  1.024773  1.024773  1.024773  0.002310  0.004619  0.009239  0.013858   \n",
       "\n",
       "         MIN60       MA5      MA10      MA20      MA30      MA60      STD5  \\\n",
       "6367  0.026257  0.980938  0.978733  0.965967  0.983451  0.927796  0.016323   \n",
       "6368  0.026967  1.002631  1.005553  0.992186  1.007306  0.955139  0.014019   \n",
       "6369  0.027125  1.007936  1.011389  0.998641  1.008695  0.962858  0.014449   \n",
       "6370  0.027026  1.005563  1.005929  0.996705  1.002439  0.961468  0.013425   \n",
       "6371  0.027716  1.026126  1.027402  1.021659  1.024024  0.988012  0.019816   \n",
       "\n",
       "         STD10     STD20     STD30     STD60     BETA5    BETA10    BETA20  \\\n",
       "6367  0.013121  0.019814  0.039477  0.072978  0.000640  0.000918  0.001660   \n",
       "6368  0.013257  0.020388  0.038235  0.074274  0.004164  0.000071  0.001909   \n",
       "6369  0.013403  0.020361  0.030737  0.073990  0.000808 -0.001269  0.001783   \n",
       "6370  0.013062  0.019086  0.027456  0.072995 -0.003294 -0.001025  0.001384   \n",
       "6371  0.016084  0.020015  0.022603  0.073567 -0.011336 -0.002079  0.000907   \n",
       "\n",
       "        BETA30    BETA60     RSQR5    RSQR10    RSQR20    RSQR30    RSQR60  \\\n",
       "6367 -0.002354  0.003068  0.003846  0.044889  0.245784  0.275671  0.539084   \n",
       "6368 -0.001988  0.003078  0.220526  0.000262  0.306869  0.209467  0.523765   \n",
       "6369 -0.001239  0.003013  0.007815  0.082178  0.268310  0.125837  0.505647   \n",
       "6370 -0.000771  0.002920  0.150541  0.056428  0.183972  0.061060  0.488085   \n",
       "6371 -0.000336  0.002838  0.818138  0.153208  0.071894  0.017132  0.453784   \n",
       "\n",
       "         RESI5    RESI10    RESI20    RESI30    RESI60     QTLU5    QTLU10  \\\n",
       "6367  0.017781  0.017135  0.018259  0.050688 -0.018306  0.997724  0.990325   \n",
       "6368 -0.010958 -0.005872 -0.010322  0.021517 -0.045936  1.005406  1.017093   \n",
       "6369 -0.009552 -0.005678 -0.015577  0.009264 -0.051731  1.011315  1.023071   \n",
       "6370  0.001026 -0.001317 -0.009850  0.008736 -0.047609  1.007611  1.018153   \n",
       "6371 -0.003453 -0.018045 -0.030276 -0.019151 -0.071723  1.033332  1.032731   \n",
       "\n",
       "        QTLU20    QTLU30    QTLU60     QTLD5    QTLD10    QTLD20    QTLD30  \\\n",
       "6367  0.986058  1.001707  0.986058  0.969700  0.968564  0.951350  0.955048   \n",
       "6368  1.012710  1.027029  1.012710  0.995910  0.994743  0.977065  0.980863   \n",
       "6369  1.018662  1.030715  1.018662  0.999412  1.000000  0.982807  0.986627   \n",
       "6370  1.014932  1.019324  1.014932  0.999268  0.996338  0.983602  0.983014   \n",
       "6371  1.040839  1.042340  1.040839  1.017416  1.021170  1.004204  1.004204   \n",
       "\n",
       "        QTLD60  TSRANK5  TSRANK10  TSRANK20  TSRANK30  TSRANK60      RSV5  \\\n",
       "6367  0.846229      5.0      10.0      20.0      23.5      52.5  0.983871   \n",
       "6368  0.869248      4.0       6.0      15.0      18.0      46.0  0.962647   \n",
       "6369  0.874943      2.0       2.5       9.5      11.5      38.5  0.991242   \n",
       "6370  0.872326      2.5       5.5      13.5      17.5      43.5  0.996348   \n",
       "6371  0.894893      1.0       1.0       3.0       4.0      27.0  0.975771   \n",
       "\n",
       "         RSV10     RSV20     RSV30     RSV60  IMAX5  IMAX10  IMAX20    IMAX30  \\\n",
       "6367  0.983836  0.983765  0.983695  0.983478    0.0     0.0    0.00  0.933333   \n",
       "6368  0.962566  0.962403  0.962238  0.961735    0.2     0.1    0.05  0.966667   \n",
       "6369  0.991222  0.991182  0.991142  0.991020    0.4     0.2    0.10  0.966667   \n",
       "6370  0.996339  0.996323  0.996306  0.996255    0.6     0.3    0.15  0.966667   \n",
       "6371  0.975716  0.975606  0.975494  0.975154    0.8     0.4    0.20  0.966667   \n",
       "\n",
       "        IMAX60  IMIN5  IMIN10  IMIN20    IMIN30    IMIN60  IMXD5  IMXD10  \\\n",
       "6367  0.466667    0.4     0.9    0.50  0.333333  0.666667   -0.4    -0.9   \n",
       "6368  0.483333    0.6     0.3    0.55  0.366667  0.683333   -0.4    -0.2   \n",
       "6369  0.500000    0.8     0.4    0.60  0.400000  0.700000   -0.4    -0.2   \n",
       "6370  0.516667    0.8     0.5    0.65  0.433333  0.716667   -0.2    -0.2   \n",
       "6371  0.533333    0.0     0.0    0.70  0.466667  0.733333    0.8     0.4   \n",
       "\n",
       "      IMXD20    IMXD30  IMXD60     CORD5    CORD10    CORD20    CORD30  \\\n",
       "6367    -0.5  0.600000    -0.2  0.085307  0.291894  0.393290  0.297331   \n",
       "6368    -0.5  0.600000    -0.2  0.283575 -0.114669  0.337841  0.273088   \n",
       "6369    -0.5  0.566667    -0.2  0.335234 -0.101648  0.339191  0.291733   \n",
       "6370    -0.5  0.533333    -0.2  0.293804 -0.176231  0.308076  0.285791   \n",
       "6371    -0.5  0.500000    -0.2  0.108308 -0.282224  0.224420  0.226055   \n",
       "\n",
       "        CORD60  CNTP5  CNTP10  CNTP20    CNTP30    CNTP60  CNTN5  CNTN10  \\\n",
       "6367  0.271429    0.6     0.5    0.45  0.466667  0.500000    0.4     0.5   \n",
       "6368  0.265908    0.4     0.4    0.45  0.433333  0.483333    0.6     0.6   \n",
       "6369  0.270499    0.4     0.4    0.45  0.400000  0.466667    0.6     0.6   \n",
       "6370  0.267780    0.6     0.4    0.50  0.433333  0.466667    0.4     0.6   \n",
       "6371  0.243918    0.4     0.4    0.45  0.400000  0.466667    0.6     0.6   \n",
       "\n",
       "      CNTN20    CNTN30    CNTN60  CNTD5  CNTD10  CNTD20    CNTD30    CNTD60  \\\n",
       "6367    0.50  0.500000  0.466667    0.2     0.0   -0.05 -0.033333  0.033333   \n",
       "6368    0.55  0.533333  0.483333   -0.2    -0.2   -0.10 -0.100000  0.000000   \n",
       "6369    0.55  0.566667  0.500000   -0.2    -0.2   -0.10 -0.166667 -0.033333   \n",
       "6370    0.50  0.533333  0.500000    0.2    -0.2    0.00 -0.100000 -0.033333   \n",
       "6371    0.55  0.566667  0.500000   -0.2    -0.2   -0.10 -0.166667 -0.033333   \n",
       "\n",
       "         SUMP5    SUMP10    SUMP20    SUMP30    SUMP60     SUMN5    SUMN10  \\\n",
       "6367  0.666667  0.692308  0.543103  0.467836  0.582988  0.333333  0.307692   \n",
       "6368  0.374037  0.510814  0.502990  0.435781  0.563437  0.625963  0.489186   \n",
       "6369  0.480396  0.497884  0.517454  0.386502  0.558583  0.519604  0.502116   \n",
       "6370  0.545460  0.443937  0.550322  0.429532  0.559484  0.454540  0.556063   \n",
       "6371  0.362906  0.386189  0.486315  0.394461  0.553836  0.637094  0.613811   \n",
       "\n",
       "        SUMN20    SUMN30    SUMN60     SUMD5    SUMD10    SUMD20    SUMD30  \\\n",
       "6367  0.456897  0.532164  0.417012  0.333333  0.384615  0.086205 -0.064327   \n",
       "6368  0.497010  0.564219  0.436563 -0.251926  0.021628  0.005981 -0.128439   \n",
       "6369  0.482546  0.613498  0.441417 -0.039209 -0.004232  0.034908 -0.226997   \n",
       "6370  0.449678  0.570468  0.440516  0.090920 -0.112127  0.100644 -0.140936   \n",
       "6371  0.513685  0.605539  0.446164 -0.274189 -0.227622 -0.027370 -0.211078   \n",
       "\n",
       "        SUMD60      VMA5     VMA10     VMA20     VMA30     VMA60     VSTD5  \\\n",
       "6367  0.165975  0.682919  0.693010  0.684679  0.778932  0.695393  0.206222   \n",
       "6368  0.126874  0.603991  0.505999  0.501997  0.552571  0.501075  0.258543   \n",
       "6369  0.117166  1.017471  0.848660  0.808245  0.869844  0.811198  0.392981   \n",
       "6370  0.118968  1.502393  1.277651  1.239610  1.299256  1.251669  0.646278   \n",
       "6371  0.107673  0.951305  0.781948  0.713734  0.721195  0.712770  0.291952   \n",
       "\n",
       "        VSTD10    VSTD20    VSTD30    VSTD60     WVMA5    WVMA10    WVMA20  \\\n",
       "6367  0.241635  0.219330  0.258516  0.359090  0.762316  0.976635  0.942986   \n",
       "6368  0.214745  0.192167  0.192437  0.258827  0.824741  0.889581  0.871287   \n",
       "6369  0.339072  0.309821  0.293380  0.410056  1.021606  0.830552  0.904752   \n",
       "6370  0.527998  0.474074  0.434810  0.621103  1.078485  0.956484  0.949877   \n",
       "6371  0.276419  0.273733  0.231611  0.347074  0.776483  0.784337  0.923774   \n",
       "\n",
       "        WVMA30    WVMA60    VSUMP5   VSUMP10   VSUMP20   VSUMP30   VSUMP60  \\\n",
       "6367  0.873075  1.285542  0.596880  0.586130  0.528651  0.472186  0.512911   \n",
       "6368  0.861786  1.251970  0.808625  0.542153  0.553568  0.516407  0.537413   \n",
       "6369  0.866688  1.244961  0.563466  0.557445  0.509336  0.476564  0.519540   \n",
       "6370  0.849424  1.241542  0.460167  0.479722  0.501110  0.461363  0.510139   \n",
       "6371  0.844199  1.215696  0.625607  0.601353  0.529784  0.485727  0.522230   \n",
       "\n",
       "        VSUMN5   VSUMN10   VSUMN20   VSUMN30   VSUMN60    VSUMD5   VSUMD10  \\\n",
       "6367  0.403120  0.413870  0.471349  0.527814  0.487089  0.193759  0.172261   \n",
       "6368  0.191375  0.457847  0.446432  0.483593  0.462587  0.617250  0.084305   \n",
       "6369  0.436534  0.442555  0.490664  0.523436  0.480460  0.126931  0.114891   \n",
       "6370  0.539833  0.520278  0.498890  0.538637  0.489861 -0.079667 -0.040556   \n",
       "6371  0.374393  0.398647  0.470216  0.514273  0.477770  0.251215  0.202707   \n",
       "\n",
       "       VSUMD20   VSUMD30   VSUMD60  \n",
       "6367  0.057302 -0.055627  0.025821  \n",
       "6368  0.107135  0.032815  0.074826  \n",
       "6369  0.018673 -0.046872  0.039081  \n",
       "6370  0.002220 -0.077274  0.020278  \n",
       "6371  0.059569 -0.028545  0.044459  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"quant_model_2\" (type QuantModel).\n\nin user code:\n\n    File \"/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/ipykernel_33444/3240950507.py\", line 21, in call  *\n        senet_output = self.senet_layer(encoded_features, training=training)\n    File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/__autograph_generated_filee6_kdqat.py\", line 12, in tf__call\n        A_1 = ag__.converted_call(ag__.ld(self).scale_layer, (ag__.ld(Z),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'senet_3' (type Senet).\n    \n    in user code:\n    \n        File \"/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/ipykernel_33444/2064477366.py\", line 17, in call  *\n            A_1 = self.scale_layer(Z) # [B, X]\n        File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    \n    Call arguments received by layer 'senet_3' (type Senet):\n      • inputs=['tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)']\n      • training=None\n\n\nCall arguments received by layer \"quant_model_2\" (type QuantModel):\n  • inputs={'KMID': 'tf.Tensor(shape=(None,), dtype=float32)', 'KLEN': 'tf.Tensor(shape=(None,), dtype=float32)', 'KMID2': 'tf.Tensor(shape=(None,), dtype=float32)', 'KUP': 'tf.Tensor(shape=(None,), dtype=float32)', 'KUP2': 'tf.Tensor(shape=(None,), dtype=float32)', 'KLOW': 'tf.Tensor(shape=(None,), dtype=float32)', 'KLOW2': 'tf.Tensor(shape=(None,), dtype=float32)', 'KSFT': 'tf.Tensor(shape=(None,), dtype=float32)', 'KSFT2': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN0': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN1': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN2': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN3': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN4': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH0': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH1': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH2': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH3': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH4': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW0': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW1': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW2': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW3': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW4': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE0': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE1': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE2': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE3': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE4': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME0': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME1': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME2': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME3': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME4': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC5': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC10': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC20': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC30': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC60': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX5': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX10': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX20': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX30': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX60': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA5': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA10': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA20': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA30': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA60': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA5': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA10': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA20': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA30': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA60': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR5': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR10': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR20': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR30': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR60': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI5': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI10': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI20': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI30': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI60': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU5': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU10': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU20': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU30': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU60': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK5': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK10': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK20': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK30': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK60': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV5': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV10': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV20': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV30': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV60': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX5': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX10': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX20': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX30': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX60': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP5': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP10': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP20': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP30': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP60': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP5': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP10': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP20': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP30': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP60': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA5': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA10': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA20': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA30': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD60': 'tf.Tensor(shape=(None,), dtype=float32)'}\n  • training=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m model_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduction_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdnn_use_bn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m     10\u001b[0m quant_model \u001b[38;5;241m=\u001b[39m QuantModel(model_config)\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39m[v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()], outputs\u001b[38;5;241m=\u001b[39m\u001b[43mquant_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mMeanSquaredError()\n",
      "File \u001b[0;32m~/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/__autograph_generated_filef640aoia.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     10\u001b[0m encoded_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(encode_inputs), (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 11\u001b[0m senet_output \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msenet_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m senet_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten, (ag__\u001b[38;5;241m.\u001b[39mld(senet_output),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     13\u001b[0m dnn_output \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mdnn_layer, (ag__\u001b[38;5;241m.\u001b[39mld(senet_output),), \u001b[38;5;28mdict\u001b[39m(training\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(training)), fscope)\n",
      "File \u001b[0;32m/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/__autograph_generated_filee6_kdqat.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconcat, (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[1;32m     11\u001b[0m Z \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreduce_mean, (ag__\u001b[38;5;241m.\u001b[39mld(inputs),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[0;32m---> 12\u001b[0m A_1 \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m A_2 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_layer, (ag__\u001b[38;5;241m.\u001b[39mld(A_1),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m scale_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmultiply, (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mexpand_dims, (ag__\u001b[38;5;241m.\u001b[39mld(A_2),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"quant_model_2\" (type QuantModel).\n\nin user code:\n\n    File \"/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/ipykernel_33444/3240950507.py\", line 21, in call  *\n        senet_output = self.senet_layer(encoded_features, training=training)\n    File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/__autograph_generated_filee6_kdqat.py\", line 12, in tf__call\n        A_1 = ag__.converted_call(ag__.ld(self).scale_layer, (ag__.ld(Z),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'senet_3' (type Senet).\n    \n    in user code:\n    \n        File \"/var/folders/vc/j8df25m509sdsv8x_v9j7gk00000ks/T/ipykernel_33444/2064477366.py\", line 17, in call  *\n            A_1 = self.scale_layer(Z) # [B, X]\n        File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/Users/alsc/VscodeProject/hh_quant/.venv/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 253, in assert_input_compatibility\n            raise ValueError(\n    \n        ValueError: Input 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    \n    Call arguments received by layer 'senet_3' (type Senet):\n      • inputs=['tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)', 'tf.Tensor(shape=(None, 6), dtype=float32)']\n      • training=None\n\n\nCall arguments received by layer \"quant_model_2\" (type QuantModel):\n  • inputs={'KMID': 'tf.Tensor(shape=(None,), dtype=float32)', 'KLEN': 'tf.Tensor(shape=(None,), dtype=float32)', 'KMID2': 'tf.Tensor(shape=(None,), dtype=float32)', 'KUP': 'tf.Tensor(shape=(None,), dtype=float32)', 'KUP2': 'tf.Tensor(shape=(None,), dtype=float32)', 'KLOW': 'tf.Tensor(shape=(None,), dtype=float32)', 'KLOW2': 'tf.Tensor(shape=(None,), dtype=float32)', 'KSFT': 'tf.Tensor(shape=(None,), dtype=float32)', 'KSFT2': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN0': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN1': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN2': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN3': 'tf.Tensor(shape=(None,), dtype=float32)', 'OPEN4': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH0': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH1': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH2': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH3': 'tf.Tensor(shape=(None,), dtype=float32)', 'HIGH4': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW0': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW1': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW2': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW3': 'tf.Tensor(shape=(None,), dtype=float32)', 'LOW4': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE0': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE1': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE2': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE3': 'tf.Tensor(shape=(None,), dtype=float32)', 'CLOSE4': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME0': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME1': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME2': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME3': 'tf.Tensor(shape=(None,), dtype=float32)', 'VOLUME4': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC5': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC10': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC20': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC30': 'tf.Tensor(shape=(None,), dtype=float32)', 'ROC60': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX5': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX10': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX20': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX30': 'tf.Tensor(shape=(None,), dtype=float32)', 'MAX60': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'MIN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA5': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA10': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA20': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA30': 'tf.Tensor(shape=(None,), dtype=float32)', 'MA60': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'STD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA5': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA10': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA20': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA30': 'tf.Tensor(shape=(None,), dtype=float32)', 'BETA60': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR5': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR10': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR20': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR30': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSQR60': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI5': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI10': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI20': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI30': 'tf.Tensor(shape=(None,), dtype=float32)', 'RESI60': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU5': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU10': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU20': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU30': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLU60': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'QTLD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK5': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK10': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK20': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK30': 'tf.Tensor(shape=(None,), dtype=float32)', 'TSRANK60': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV5': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV10': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV20': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV30': 'tf.Tensor(shape=(None,), dtype=float32)', 'RSV60': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX5': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX10': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX20': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX30': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMAX60': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMIN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'IMXD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'CORD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP5': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP10': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP20': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP30': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTP60': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'CNTD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP5': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP10': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP20': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP30': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMP60': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'SUMD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VMA60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSTD60': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA5': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA10': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA20': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA30': 'tf.Tensor(shape=(None,), dtype=float32)', 'WVMA60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMP60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMN60': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD5': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD10': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD20': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD30': 'tf.Tensor(shape=(None,), dtype=float32)', 'VSUMD60': 'tf.Tensor(shape=(None,), dtype=float32)'}\n  • training=None"
     ]
    }
   ],
   "source": [
    "model_config = {\n",
    "    \"seed\": 1024,\n",
    "    \"reduction_ratio\": 3,\n",
    "    \"dnn_hidden_units\": [64,32],\n",
    "    \"dnn_activation\": 'relu',\n",
    "    \"dnn_dropout\": 0.2,\n",
    "    \"dnn_use_bn\": True\n",
    "}\n",
    "\n",
    "quant_model = QuantModel(model_config)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[v for k, v in inputs.items()], outputs=quant_model(inputs))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer, loss=loss)\n",
    "model.fit(X_train, y_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_boundaries(series, num_bins=40):\n",
    "    boundaries = pd.qcut(series, num_bins, retbins=True, duplicates='drop')[1].tolist()\n",
    "    return boundaries\n",
    "\n",
    "TARGET_FEATURE_NAME = \"label1\"\n",
    "\n",
    "# 连续特征分桶\n",
    "NUMERIC_FEATURES = {\n",
    "    'total_shares': get_numeric_boundaries(train_data['total_shares']),\n",
    "    'circulating_shares': get_numeric_boundaries(train_data['circulating_shares']),\n",
    "    'total_market_cap': get_numeric_boundaries(train_data['total_market_cap']),\n",
    "    'circulating_market_cap': get_numeric_boundaries(train_data['circulating_market_cap']),\n",
    "    'turnover_rate': get_numeric_boundaries(train_data['turnover_rate']),\n",
    "    'pe': get_numeric_boundaries(train_data['pe']),\n",
    "    'pe_ttm': get_numeric_boundaries(train_data['pe_ttm']),\n",
    "    'pb': get_numeric_boundaries(train_data['pb']),\n",
    "    'ps': get_numeric_boundaries(train_data['ps']),\n",
    "    'ps_ttm': get_numeric_boundaries(train_data['ps_ttm']),\n",
    "    'total_mv': get_numeric_boundaries(train_data['total_mv']),\n",
    "    \"KMID\": get_numeric_boundaries(train_data[\"KMID\"]),\n",
    "    \"KLEN\": get_numeric_boundaries(train_data[\"KLEN\"]),\n",
    "    \"KMID2\": get_numeric_boundaries(train_data[\"KMID2\"]),\n",
    "    \"KUP\": get_numeric_boundaries(train_data[\"KUP\"]),\n",
    "    \"KUP2\": get_numeric_boundaries(train_data[\"KUP2\"]),\n",
    "    \"KLOW\": get_numeric_boundaries(train_data[\"KLOW\"]),\n",
    "    \"KLOW2\": get_numeric_boundaries(train_data[\"KLOW2\"]),\n",
    "    \"KSFT\": get_numeric_boundaries(train_data[\"KSFT\"]),\n",
    "    \"KSFT2\": get_numeric_boundaries(train_data[\"KSFT2\"]),\n",
    "    \"OPEN0\": get_numeric_boundaries(train_data[\"OPEN0\"]),\n",
    "    \"OPEN1\": get_numeric_boundaries(train_data[\"OPEN1\"]),\n",
    "    \"OPEN2\": get_numeric_boundaries(train_data[\"OPEN2\"]),\n",
    "    \"OPEN3\": get_numeric_boundaries(train_data[\"OPEN3\"]),\n",
    "    \"OPEN4\": get_numeric_boundaries(train_data[\"OPEN4\"]),\n",
    "    \"HIGH0\": get_numeric_boundaries(train_data[\"HIGH0\"]),\n",
    "    \"HIGH1\": get_numeric_boundaries(train_data[\"HIGH1\"]),\n",
    "    \"HIGH2\": get_numeric_boundaries(train_data[\"HIGH2\"]),\n",
    "    \"HIGH3\": get_numeric_boundaries(train_data[\"HIGH3\"]),\n",
    "    \"HIGH4\": get_numeric_boundaries(train_data[\"HIGH4\"]),\n",
    "    \"LOW0\": get_numeric_boundaries(train_data[\"LOW0\"]),\n",
    "    \"LOW1\": get_numeric_boundaries(train_data[\"LOW1\"]),\n",
    "    \"LOW2\": get_numeric_boundaries(train_data[\"LOW2\"]),\n",
    "    \"LOW3\": get_numeric_boundaries(train_data[\"LOW3\"]),\n",
    "    \"LOW4\": get_numeric_boundaries(train_data[\"LOW4\"]),\n",
    "    \"CLOSE0\": get_numeric_boundaries(train_data[\"CLOSE0\"]),\n",
    "    \"CLOSE1\": get_numeric_boundaries(train_data[\"CLOSE1\"]),\n",
    "    \"CLOSE2\": get_numeric_boundaries(train_data[\"CLOSE2\"]),\n",
    "    \"CLOSE3\": get_numeric_boundaries(train_data[\"CLOSE3\"]),\n",
    "    \"CLOSE4\": get_numeric_boundaries(train_data[\"CLOSE4\"]),\n",
    "    \"VWAP0\": get_numeric_boundaries(train_data[\"VWAP0\"]),\n",
    "    \"VWAP1\": get_numeric_boundaries(train_data[\"VWAP1\"]),\n",
    "    \"VWAP2\": get_numeric_boundaries(train_data[\"VWAP2\"]),\n",
    "    \"VWAP3\": get_numeric_boundaries(train_data[\"VWAP3\"]),\n",
    "    \"VWAP4\": get_numeric_boundaries(train_data[\"VWAP4\"]),\n",
    "    \"VOLUME0\": get_numeric_boundaries(train_data[\"VOLUME0\"]),\n",
    "    \"VOLUME1\": get_numeric_boundaries(train_data[\"VOLUME1\"]),\n",
    "    \"VOLUME2\": get_numeric_boundaries(train_data[\"VOLUME2\"]),\n",
    "    \"VOLUME3\": get_numeric_boundaries(train_data[\"VOLUME3\"]),\n",
    "    \"VOLUME4\": get_numeric_boundaries(train_data[\"VOLUME4\"]),\n",
    "    \"ROC5\": get_numeric_boundaries(train_data[\"ROC5\"]),\n",
    "    \"ROC10\": get_numeric_boundaries(train_data[\"ROC10\"]),\n",
    "    \"ROC20\": get_numeric_boundaries(train_data[\"ROC20\"]),\n",
    "    \"ROC30\": get_numeric_boundaries(train_data[\"ROC30\"]),\n",
    "    \"ROC60\": get_numeric_boundaries(train_data[\"ROC60\"]),\n",
    "    \"MAX5\": get_numeric_boundaries(train_data[\"MAX5\"]),\n",
    "    \"MAX10\": get_numeric_boundaries(train_data[\"MAX10\"]),\n",
    "    \"MAX20\": get_numeric_boundaries(train_data[\"MAX20\"]),\n",
    "    \"MAX30\": get_numeric_boundaries(train_data[\"MAX30\"]),\n",
    "    \"MAX60\": get_numeric_boundaries(train_data[\"MAX60\"]),\n",
    "    \"MIN5\": get_numeric_boundaries(train_data[\"MIN5\"]),\n",
    "    \"MIN10\": get_numeric_boundaries(train_data[\"MIN10\"]),\n",
    "    \"MIN20\": get_numeric_boundaries(train_data[\"MIN20\"]),\n",
    "    \"MIN30\": get_numeric_boundaries(train_data[\"MIN30\"]),\n",
    "    \"MIN60\": get_numeric_boundaries(train_data[\"MIN60\"]),\n",
    "    \"MA5\": get_numeric_boundaries(train_data[\"MA5\"]),\n",
    "    \"MA10\": get_numeric_boundaries(train_data[\"MA10\"]),\n",
    "    \"MA20\": get_numeric_boundaries(train_data[\"MA20\"]),\n",
    "    \"MA30\": get_numeric_boundaries(train_data[\"MA30\"]),\n",
    "    \"MA60\": get_numeric_boundaries(train_data[\"MA60\"]),\n",
    "    \"STD5\": get_numeric_boundaries(train_data[\"STD5\"]),\n",
    "    \"STD10\": get_numeric_boundaries(train_data[\"STD10\"]),\n",
    "    \"STD20\": get_numeric_boundaries(train_data[\"STD20\"]),\n",
    "    \"STD30\": get_numeric_boundaries(train_data[\"STD30\"]),\n",
    "    \"STD60\": get_numeric_boundaries(train_data[\"STD60\"]),\n",
    "    \"BETA5\": get_numeric_boundaries(train_data[\"BETA5\"]),\n",
    "    \"BETA10\": get_numeric_boundaries(train_data[\"BETA10\"]),\n",
    "    \"BETA20\": get_numeric_boundaries(train_data[\"BETA20\"]),\n",
    "    \"BETA30\": get_numeric_boundaries(train_data[\"BETA30\"]),\n",
    "    \"BETA60\": get_numeric_boundaries(train_data[\"BETA60\"]),\n",
    "    \"RSQR5\": get_numeric_boundaries(train_data[\"RSQR5\"]),\n",
    "    \"RSQR10\": get_numeric_boundaries(train_data[\"RSQR10\"]),\n",
    "    \"RSQR20\": get_numeric_boundaries(train_data[\"RSQR20\"]),\n",
    "    \"RSQR30\": get_numeric_boundaries(train_data[\"RSQR30\"]),\n",
    "    \"RSQR60\": get_numeric_boundaries(train_data[\"RSQR60\"]),\n",
    "    \"RESI5\": get_numeric_boundaries(train_data[\"RESI5\"]),\n",
    "    \"RESI10\": get_numeric_boundaries(train_data[\"RESI10\"]),\n",
    "    \"RESI20\": get_numeric_boundaries(train_data[\"RESI20\"]),\n",
    "    \"RESI30\": get_numeric_boundaries(train_data[\"RESI30\"]),\n",
    "    \"RESI60\": get_numeric_boundaries(train_data[\"RESI60\"]),\n",
    "    \"QTLU5\": get_numeric_boundaries(train_data[\"QTLU5\"]),\n",
    "    \"QTLU10\": get_numeric_boundaries(train_data[\"QTLU10\"]),\n",
    "    \"QTLU20\": get_numeric_boundaries(train_data[\"QTLU20\"]),\n",
    "    \"QTLU30\": get_numeric_boundaries(train_data[\"QTLU30\"]),\n",
    "    \"QTLU60\": get_numeric_boundaries(train_data[\"QTLU60\"]),\n",
    "    \"QTLD5\": get_numeric_boundaries(train_data[\"QTLD5\"]),\n",
    "    \"QTLD10\": get_numeric_boundaries(train_data[\"QTLD10\"]),\n",
    "    \"QTLD20\": get_numeric_boundaries(train_data[\"QTLD20\"]),\n",
    "    \"QTLD30\": get_numeric_boundaries(train_data[\"QTLD30\"]),\n",
    "    \"QTLD60\": get_numeric_boundaries(train_data[\"QTLD60\"]),\n",
    "    # \"TSRANK5\": get_numeric_boundaries(train_data[\"TSRANK5\"]),\n",
    "    # \"TSRANK10\": get_numeric_boundaries(train_data[\"TSRANK10\"]),\n",
    "    # \"TSRANK20\": get_numeric_boundaries(train_data[\"TSRANK20\"]),\n",
    "    # \"TSRANK30\": get_numeric_boundaries(train_data[\"TSRANK30\"]),\n",
    "    # \"TSRANK60\": get_numeric_boundaries(train_data[\"TSRANK60\"]),\n",
    "    \"RSV5\": get_numeric_boundaries(train_data[\"RSV5\"]),\n",
    "    \"RSV10\": get_numeric_boundaries(train_data[\"RSV10\"]),\n",
    "    \"RSV20\": get_numeric_boundaries(train_data[\"RSV20\"]),\n",
    "    \"RSV30\": get_numeric_boundaries(train_data[\"RSV30\"]),\n",
    "    \"RSV60\": get_numeric_boundaries(train_data[\"RSV60\"]),\n",
    "    \"IMAX5\": get_numeric_boundaries(train_data[\"IMAX5\"]),\n",
    "    \"IMAX10\": get_numeric_boundaries(train_data[\"IMAX10\"]),\n",
    "    \"IMAX20\": get_numeric_boundaries(train_data[\"IMAX20\"]),\n",
    "    \"IMAX30\": get_numeric_boundaries(train_data[\"IMAX30\"]),\n",
    "    \"IMAX60\": get_numeric_boundaries(train_data[\"IMAX60\"]),\n",
    "    \"IMIN5\": get_numeric_boundaries(train_data[\"IMIN5\"]),\n",
    "    \"IMIN10\": get_numeric_boundaries(train_data[\"IMIN10\"]),\n",
    "    \"IMIN20\": get_numeric_boundaries(train_data[\"IMIN20\"]),\n",
    "    \"IMIN30\": get_numeric_boundaries(train_data[\"IMIN30\"]),\n",
    "    \"IMIN60\": get_numeric_boundaries(train_data[\"IMIN60\"]),\n",
    "    \"IMXD5\": get_numeric_boundaries(train_data[\"IMXD5\"]),\n",
    "    \"IMXD10\": get_numeric_boundaries(train_data[\"IMXD10\"]),\n",
    "    \"IMXD20\": get_numeric_boundaries(train_data[\"IMXD20\"]),\n",
    "    \"IMXD30\": get_numeric_boundaries(train_data[\"IMXD30\"]),\n",
    "    \"IMXD60\": get_numeric_boundaries(train_data[\"IMXD60\"]),\n",
    "    \"CORR5\": get_numeric_boundaries(train_data[\"CORR5\"]),\n",
    "    \"CORR10\": get_numeric_boundaries(train_data[\"CORR10\"]),\n",
    "    \"CORR20\": get_numeric_boundaries(train_data[\"CORR20\"]),\n",
    "    \"CORR30\": get_numeric_boundaries(train_data[\"CORR30\"]),\n",
    "    \"CORR60\": get_numeric_boundaries(train_data[\"CORR60\"]),\n",
    "    \"CORD5\": get_numeric_boundaries(train_data[\"CORD5\"]),\n",
    "    \"CORD10\": get_numeric_boundaries(train_data[\"CORD10\"]),\n",
    "    \"CORD20\": get_numeric_boundaries(train_data[\"CORD20\"]),\n",
    "    \"CORD30\": get_numeric_boundaries(train_data[\"CORD30\"]),\n",
    "    \"CORD60\": get_numeric_boundaries(train_data[\"CORD60\"]),\n",
    "    \"CNTP5\": get_numeric_boundaries(train_data[\"CNTP5\"]),\n",
    "    \"CNTP10\": get_numeric_boundaries(train_data[\"CNTP10\"]),\n",
    "    \"CNTP20\": get_numeric_boundaries(train_data[\"CNTP20\"]),\n",
    "    \"CNTP30\": get_numeric_boundaries(train_data[\"CNTP30\"]),\n",
    "    \"CNTP60\": get_numeric_boundaries(train_data[\"CNTP60\"]),\n",
    "    \"CNTN5\": get_numeric_boundaries(train_data[\"CNTN5\"]),\n",
    "    \"CNTN10\": get_numeric_boundaries(train_data[\"CNTN10\"]),\n",
    "    \"CNTN20\": get_numeric_boundaries(train_data[\"CNTN20\"]),\n",
    "    \"CNTN30\": get_numeric_boundaries(train_data[\"CNTN30\"]),\n",
    "    \"CNTN60\": get_numeric_boundaries(train_data[\"CNTN60\"]),\n",
    "    \"CNTD5\": get_numeric_boundaries(train_data[\"CNTD5\"]),\n",
    "    \"CNTD10\": get_numeric_boundaries(train_data[\"CNTD10\"]),\n",
    "    \"CNTD20\": get_numeric_boundaries(train_data[\"CNTD20\"]),\n",
    "    \"CNTD30\": get_numeric_boundaries(train_data[\"CNTD30\"]),\n",
    "    \"CNTD60\": get_numeric_boundaries(train_data[\"CNTD60\"]),\n",
    "    \"SUMP5\": get_numeric_boundaries(train_data[\"SUMP5\"]),\n",
    "    \"SUMP10\": get_numeric_boundaries(train_data[\"SUMP10\"]),\n",
    "    \"SUMP20\": get_numeric_boundaries(train_data[\"SUMP20\"]),\n",
    "    \"SUMP30\": get_numeric_boundaries(train_data[\"SUMP30\"]),\n",
    "    \"SUMP60\": get_numeric_boundaries(train_data[\"SUMP60\"]),\n",
    "    \"SUMN5\": get_numeric_boundaries(train_data[\"SUMN5\"]),\n",
    "    \"SUMN10\": get_numeric_boundaries(train_data[\"SUMN10\"]),\n",
    "    \"SUMN20\": get_numeric_boundaries(train_data[\"SUMN20\"]),\n",
    "    \"SUMN30\": get_numeric_boundaries(train_data[\"SUMN30\"]),\n",
    "    \"SUMN60\": get_numeric_boundaries(train_data[\"SUMN60\"]),\n",
    "    \"SUMD5\": get_numeric_boundaries(train_data[\"SUMD5\"]),\n",
    "    \"SUMD10\": get_numeric_boundaries(train_data[\"SUMD10\"]),\n",
    "    \"SUMD20\": get_numeric_boundaries(train_data[\"SUMD20\"]),\n",
    "    \"SUMD30\": get_numeric_boundaries(train_data[\"SUMD30\"]),\n",
    "    \"SUMD60\": get_numeric_boundaries(train_data[\"SUMD60\"]),\n",
    "    \"VMA5\": get_numeric_boundaries(train_data[\"VMA5\"]),\n",
    "    \"VMA10\": get_numeric_boundaries(train_data[\"VMA10\"]),\n",
    "    \"VMA20\": get_numeric_boundaries(train_data[\"VMA20\"]),\n",
    "    \"VMA30\": get_numeric_boundaries(train_data[\"VMA30\"]),\n",
    "    \"VMA60\": get_numeric_boundaries(train_data[\"VMA60\"]),\n",
    "    \"VSTD5\": get_numeric_boundaries(train_data[\"VSTD5\"]),\n",
    "    \"VSTD10\": get_numeric_boundaries(train_data[\"VSTD10\"]),\n",
    "    \"VSTD20\": get_numeric_boundaries(train_data[\"VSTD20\"]),\n",
    "    \"VSTD30\": get_numeric_boundaries(train_data[\"VSTD30\"]),\n",
    "    \"VSTD60\": get_numeric_boundaries(train_data[\"VSTD60\"]),\n",
    "    \"WVMA5\": get_numeric_boundaries(train_data[\"WVMA5\"]),\n",
    "    \"WVMA10\": get_numeric_boundaries(train_data[\"WVMA10\"]),\n",
    "    \"WVMA20\": get_numeric_boundaries(train_data[\"WVMA20\"]),\n",
    "    \"WVMA30\": get_numeric_boundaries(train_data[\"WVMA30\"]),\n",
    "    \"WVMA60\": get_numeric_boundaries(train_data[\"WVMA60\"]),\n",
    "    \"VSUMP5\": get_numeric_boundaries(train_data[\"VSUMP5\"]),\n",
    "    \"VSUMP10\": get_numeric_boundaries(train_data[\"VSUMP10\"]),\n",
    "    \"VSUMP20\": get_numeric_boundaries(train_data[\"VSUMP20\"]),\n",
    "    \"VSUMP30\": get_numeric_boundaries(train_data[\"VSUMP30\"]),\n",
    "    \"VSUMP60\": get_numeric_boundaries(train_data[\"VSUMP60\"]),\n",
    "    \"VSUMN5\": get_numeric_boundaries(train_data[\"VSUMN5\"]),\n",
    "    \"VSUMN10\": get_numeric_boundaries(train_data[\"VSUMN10\"]),\n",
    "    \"VSUMN20\": get_numeric_boundaries(train_data[\"VSUMN20\"]),\n",
    "    \"VSUMN30\": get_numeric_boundaries(train_data[\"VSUMN30\"]),\n",
    "    \"VSUMN60\": get_numeric_boundaries(train_data[\"VSUMN60\"]),\n",
    "    \"VSUMD5\": get_numeric_boundaries(train_data[\"VSUMD5\"]),\n",
    "    \"VSUMD10\": get_numeric_boundaries(train_data[\"VSUMD10\"]),\n",
    "    \"VSUMD20\": get_numeric_boundaries(train_data[\"VSUMD20\"]),\n",
    "    \"VSUMD30\": get_numeric_boundaries(train_data[\"VSUMD30\"]),\n",
    "    \"VSUMD60\": get_numeric_boundaries(train_data[\"VSUMD60\"]),\n",
    "}\n",
    "\n",
    "# 离散特征embedding\n",
    "INTEGER_CATEGORICAL_FEATURES = {\n",
    "    'weekday': train_data['weekday'].unique().tolist(),\n",
    "    'day_of_month': train_data['day_of_month'].unique().tolist(),\n",
    "    'month': train_data['month'].unique().tolist(),\n",
    "    # 'TSRANK5': train_data['TSRANK5'].unique().tolist(),\n",
    "    # 'TSRANK10': train_data['TSRANK10'].unique().tolist(),\n",
    "    # 'TSRANK20': train_data['TSRANK20'].unique().tolist(),\n",
    "    # 'TSRANK30': train_data['TSRANK30'].unique().tolist(),\n",
    "    # 'TSRANK60': train_data['TSRANK60'].unique().tolist(),\n",
    "}\n",
    "STRING_CATEGORICAL_FEATURES = {\n",
    "    'industry': train_data['industry'].unique().tolist(),\n",
    "    'day_of_week': train_data['day_of_week'].unique().tolist(),\n",
    "    'season': train_data['season'].unique().tolist(),\n",
    "}\n",
    "\n",
    "\n",
    "FEATURE_NAMES = list(NUMERIC_FEATURES.keys()) + list(INTEGER_CATEGORICAL_FEATURES.keys()) + list(STRING_CATEGORICAL_FEATURES.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURES:\n",
    "            inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "        elif feature_name in INTEGER_CATEGORICAL_FEATURES:\n",
    "            inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"int32\"\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"string\"\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    embedding_dim = 6\n",
    "    is_embedding = True\n",
    "\n",
    "    # 处理连续特征\n",
    "    for feature_name, boundaries in NUMERIC_FEATURES.items():\n",
    "        if is_embedding:\n",
    "            lookup_layer = tf.keras.layers.Discretization(bin_boundaries=boundaries,output_mode='int')\n",
    "            embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=len(boundaries) + 1, output_dim=embedding_dim\n",
    "            )\n",
    "            encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        else:\n",
    "            lookup_layer = tf.keras.layers.Discretization(bin_boundaries=boundaries,output_mode='one_hot')\n",
    "            encoded_feature = lookup_layer(inputs[feature_name])\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    # 处理INTEGER离散特征\n",
    "    for feature_name, integer_vocab in INTEGER_CATEGORICAL_FEATURES.items():\n",
    "        lookup_layer = tf.keras.layers.IntegerLookup(vocabulary=integer_vocab)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=len(integer_vocab) + 1, output_dim=embedding_dim\n",
    "        )\n",
    "        encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        encoded_features.append(encoded_feature)\n",
    "    \n",
    "    # 处理STRING离散特征\n",
    "    for feature_name, string_vocab in STRING_CATEGORICAL_FEATURES.items():\n",
    "        lookup_layer = tf.keras.layers.StringLookup(vocabulary=string_vocab)\n",
    "        embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=len(string_vocab) + 1, output_dim=embedding_dim\n",
    "        )\n",
    "        encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        encoded_features.append(encoded_feature)\n",
    "    \n",
    "    print(f\"Total Features Size:: {len(encoded_features)}\")\n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-3\n",
    "NUM_EPOCH = 20\n",
    "\n",
    "def run_experiment(model, train_ds, val_ds, test_ds):\n",
    "    # optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE) # for mac M1/M2\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) # for mac M1/M2\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    metrics = [\n",
    "        tf.keras.metrics.TruePositives(name='tp'),\n",
    "        tf.keras.metrics.FalsePositives(name='fp'),\n",
    "        tf.keras.metrics.TrueNegatives(name='tn'),\n",
    "        tf.keras.metrics.FalseNegatives(name='fn'), \n",
    "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        verbose=1,\n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=NUM_EPOCH,\n",
    "        validation_data=val_ds, \n",
    "        verbose=2,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    # loss, auc = model.evaluate(test_ds, verbose=0)\n",
    "    # print(f\"Test AUC::{round(auc * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.2\n",
    "hidden_units = [128, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Features Size:: 196\n",
      "Start training the model...\n",
      "Epoch 1/20\n",
      "879/879 - 53s - loss: 0.6766 - tp: 19124.0000 - fp: 34636.0000 - tn: 45405.0000 - fn: 13300.0000 - accuracy: 0.5738 - precision: 0.3557 - recall: 0.5898 - auc: 0.6080 - val_loss: 0.6920 - val_tp: 1801.0000 - val_fp: 4452.0000 - val_tn: 4455.0000 - val_fn: 1178.0000 - val_accuracy: 0.5263 - val_precision: 0.2880 - val_recall: 0.6046 - val_auc: 0.5721 - 53s/epoch - 60ms/step\n",
      "Epoch 2/20\n",
      "879/879 - 24s - loss: 0.6381 - tp: 21462.0000 - fp: 31439.0000 - tn: 48602.0000 - fn: 10962.0000 - accuracy: 0.6230 - precision: 0.4057 - recall: 0.6619 - auc: 0.6850 - val_loss: 0.7583 - val_tp: 1899.0000 - val_fp: 4828.0000 - val_tn: 4079.0000 - val_fn: 1080.0000 - val_accuracy: 0.5029 - val_precision: 0.2823 - val_recall: 0.6375 - val_auc: 0.5596 - 24s/epoch - 28ms/step\n",
      "Epoch 3/20\n",
      "879/879 - 25s - loss: 0.6027 - tp: 22606.0000 - fp: 28416.0000 - tn: 51625.0000 - fn: 9818.0000 - accuracy: 0.6600 - precision: 0.4431 - recall: 0.6972 - auc: 0.7352 - val_loss: 0.7394 - val_tp: 1678.0000 - val_fp: 4147.0000 - val_tn: 4760.0000 - val_fn: 1301.0000 - val_accuracy: 0.5416 - val_precision: 0.2881 - val_recall: 0.5633 - val_auc: 0.5603 - 25s/epoch - 29ms/step\n",
      "Epoch 4/20\n",
      "879/879 - 29s - loss: 0.5593 - tp: 23897.0000 - fp: 25518.0000 - tn: 54523.0000 - fn: 8527.0000 - accuracy: 0.6973 - precision: 0.4836 - recall: 0.7370 - auc: 0.7829 - val_loss: 0.7319 - val_tp: 1463.0000 - val_fp: 3666.0000 - val_tn: 5241.0000 - val_fn: 1516.0000 - val_accuracy: 0.5640 - val_precision: 0.2852 - val_recall: 0.4911 - val_auc: 0.5588 - 29s/epoch - 34ms/step\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 16:41:25.714555: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:17: Filling up shuffle buffer (this may take a while): 102318 of 112465\n",
      "2024-02-23 16:41:26.773264: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 - 29s - loss: 0.5095 - tp: 25253.0000 - fp: 22573.0000 - tn: 57468.0000 - fn: 7171.0000 - accuracy: 0.7355 - precision: 0.5280 - recall: 0.7788 - auc: 0.8266 - val_loss: 0.7876 - val_tp: 1569.0000 - val_fp: 3948.0000 - val_tn: 4959.0000 - val_fn: 1410.0000 - val_accuracy: 0.5492 - val_precision: 0.2844 - val_recall: 0.5267 - val_auc: 0.5581 - 29s/epoch - 33ms/step\n",
      "Epoch 6/20\n",
      "879/879 - 27s - loss: 0.4581 - tp: 26309.0000 - fp: 19606.0000 - tn: 60435.0000 - fn: 6115.0000 - accuracy: 0.7713 - precision: 0.5730 - recall: 0.8114 - auc: 0.8641 - val_loss: 0.8215 - val_tp: 1571.0000 - val_fp: 3954.0000 - val_tn: 4953.0000 - val_fn: 1408.0000 - val_accuracy: 0.5489 - val_precision: 0.2843 - val_recall: 0.5274 - val_auc: 0.5575 - 27s/epoch - 30ms/step\n",
      "Epoch 7/20\n",
      "879/879 - 27s - loss: 0.4109 - tp: 27103.0000 - fp: 16927.0000 - tn: 63114.0000 - fn: 5321.0000 - accuracy: 0.8022 - precision: 0.6156 - recall: 0.8359 - auc: 0.8931 - val_loss: 0.8674 - val_tp: 1448.0000 - val_fp: 3820.0000 - val_tn: 5087.0000 - val_fn: 1531.0000 - val_accuracy: 0.5498 - val_precision: 0.2749 - val_recall: 0.4861 - val_auc: 0.5417 - 27s/epoch - 31ms/step\n",
      "Epoch 8/20\n",
      "879/879 - 27s - loss: 0.3699 - tp: 27885.0000 - fp: 15113.0000 - tn: 64928.0000 - fn: 4539.0000 - accuracy: 0.8253 - precision: 0.6485 - recall: 0.8600 - auc: 0.9143 - val_loss: 0.9091 - val_tp: 1335.0000 - val_fp: 3407.0000 - val_tn: 5500.0000 - val_fn: 1644.0000 - val_accuracy: 0.5750 - val_precision: 0.2815 - val_recall: 0.4481 - val_auc: 0.5470 - 27s/epoch - 30ms/step\n",
      "Epoch 9/20\n",
      "879/879 - 26s - loss: 0.3368 - tp: 28253.0000 - fp: 13072.0000 - tn: 66969.0000 - fn: 4171.0000 - accuracy: 0.8467 - precision: 0.6837 - recall: 0.8714 - auc: 0.9298 - val_loss: 0.9346 - val_tp: 1303.0000 - val_fp: 3276.0000 - val_tn: 5631.0000 - val_fn: 1676.0000 - val_accuracy: 0.5834 - val_precision: 0.2846 - val_recall: 0.4374 - val_auc: 0.5491 - 26s/epoch - 29ms/step\n",
      "Epoch 10/20\n",
      "879/879 - 25s - loss: 0.3053 - tp: 28695.0000 - fp: 11621.0000 - tn: 68420.0000 - fn: 3729.0000 - accuracy: 0.8635 - precision: 0.7118 - recall: 0.8850 - auc: 0.9425 - val_loss: 1.0570 - val_tp: 1347.0000 - val_fp: 3458.0000 - val_tn: 5449.0000 - val_fn: 1632.0000 - val_accuracy: 0.5718 - val_precision: 0.2803 - val_recall: 0.4522 - val_auc: 0.5478 - 25s/epoch - 29ms/step\n",
      "Epoch 11/20\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "879/879 - 25s - loss: 0.2825 - tp: 29007.0000 - fp: 10296.0000 - tn: 69745.0000 - fn: 3417.0000 - accuracy: 0.8781 - precision: 0.7380 - recall: 0.8946 - auc: 0.9511 - val_loss: 1.0590 - val_tp: 1247.0000 - val_fp: 3193.0000 - val_tn: 5714.0000 - val_fn: 1732.0000 - val_accuracy: 0.5856 - val_precision: 0.2809 - val_recall: 0.4186 - val_auc: 0.5449 - 25s/epoch - 28ms/step\n",
      "Epoch 11: early stopping\n",
      "Model training finished\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "\n",
    "    for units in hidden_units:\n",
    "        features = tf.keras.layers.Dense(units)(features)\n",
    "        features = tf.keras.layers.BatchNormalization()(features)\n",
    "        features = tf.keras.layers.ReLU()(features)\n",
    "        features = tf.keras.layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(features)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "# tf.keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n",
    "run_experiment(baseline_model, train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_model.save('./stock_selection_base_model')\n",
    "# reloaded_model = tf.keras.models.load_model('./stock_selection_base_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = df_to_dataset(test_data.iloc[:100, :], shuffle=False, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 4s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "test_labels = []\n",
    "for _, labels in test_ds:\n",
    "    test_labels.extend(labels.numpy())\n",
    "\n",
    "test_predictions = baseline_model.predict(test_ds).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df['true_label'] = test_labels\n",
    "test_df['prediction'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>1</td>\n",
       "      <td>0.846764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>0</td>\n",
       "      <td>0.793685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>0</td>\n",
       "      <td>0.782062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10277</th>\n",
       "      <td>0</td>\n",
       "      <td>0.778084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699</th>\n",
       "      <td>1</td>\n",
       "      <td>0.776973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8181</th>\n",
       "      <td>1</td>\n",
       "      <td>0.772075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6754</th>\n",
       "      <td>0</td>\n",
       "      <td>0.771943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7842</th>\n",
       "      <td>1</td>\n",
       "      <td>0.766099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6718</th>\n",
       "      <td>0</td>\n",
       "      <td>0.765828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7936</th>\n",
       "      <td>0</td>\n",
       "      <td>0.763873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7487</th>\n",
       "      <td>0</td>\n",
       "      <td>0.762679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11155</th>\n",
       "      <td>1</td>\n",
       "      <td>0.761576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>0</td>\n",
       "      <td>0.761016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1</td>\n",
       "      <td>0.757074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6883</th>\n",
       "      <td>1</td>\n",
       "      <td>0.754397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>1</td>\n",
       "      <td>0.752272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>0</td>\n",
       "      <td>0.751774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384</th>\n",
       "      <td>1</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4936</th>\n",
       "      <td>0</td>\n",
       "      <td>0.748669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7195</th>\n",
       "      <td>0</td>\n",
       "      <td>0.743114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       true_label  prediction\n",
       "5441            1    0.846764\n",
       "1407            0    0.793685\n",
       "5702            0    0.782062\n",
       "10277           0    0.778084\n",
       "7699            1    0.776973\n",
       "8181            1    0.772075\n",
       "6754            0    0.771943\n",
       "7842            1    0.766099\n",
       "6718            0    0.765828\n",
       "7936            0    0.763873\n",
       "7487            0    0.762679\n",
       "11155           1    0.761576\n",
       "2519            0    0.761016\n",
       "417             1    0.757074\n",
       "6883            1    0.754397\n",
       "2636            1    0.752272\n",
       "2087            0    0.751774\n",
       "4384            1    0.750000\n",
       "4936            0    0.748669\n",
       "7195            0    0.743114"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测最高的打分，实际label都不是1\n",
    "test_df.sort_values(by='prediction', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(true_positives_scores, bins=50, alpha=0.5, label='True Positives')\n",
    "# plt.hist(false_positives_scores, bins=50, alpha=0.5, label='False Positives')\n",
    "# plt.xlabel('Scores')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legitimate Transactions Detected (True Negatives):  3647\n",
      "Legitimate Transactions Incorrectly Detected (False Positives):  5345\n",
      "Fraudulent Transactions Missed (False Negatives):  946\n",
      "Fraudulent Transactions Detected (True Positives):  2069\n",
      "Total Fraudulent Transactions:  3015\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_cm(labels, predictions, p=0.5):\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "    print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "    print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "    print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "    print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "plot_cm(test_labels, test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
