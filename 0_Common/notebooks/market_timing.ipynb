{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import akshare as ak\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['688981', '688041', '601988', '601601', '600150']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 获取中证50（000016）的股票列表\n",
    "stock_code_list = ak.index_stock_cons('000016')['品种代码'].to_list()\n",
    "stock_code_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_order_signal(dataframe):\n",
    "    df = dataframe[['datetime', 'close']].copy()\n",
    "    df = df.sort_values(by=['datetime'])\n",
    "\n",
    "    # 计算时机\n",
    "    hold_period = 30\n",
    "    gap_period = 3\n",
    "    return_threshold = 0.1\n",
    "\n",
    "    # 初始化操作列\n",
    "    df['signal'] = 0  # 默认为0（无操作）\n",
    "\n",
    "    # 初始化下一个可能的买入时间，用于确保我们不会在hold_period内再次买入\n",
    "    next_possible_buy = 0\n",
    "\n",
    "    for start_index in range(len(df) - hold_period):\n",
    "        if start_index >= next_possible_buy:\n",
    "            # 获取当前价格和未来hold_period天内的价格数据\n",
    "            current_price = df.iloc[start_index]['close']\n",
    "            future_window = df.iloc[start_index:start_index + hold_period]\n",
    "            future_prices = future_window['close']\n",
    "            \n",
    "            # 如果未来hold_period天内有满足收益阈值的价格，确定买入和卖出时机\n",
    "            max_future_price = future_prices.max()\n",
    "            if max_future_price / current_price - 1 >= return_threshold:\n",
    "                # 寻找最高卖出点\n",
    "                sell_index = future_prices.idxmax()\n",
    "                \n",
    "                # 寻找买入点，它应该是在当前时间点之后且在最高卖出点之前的最低点\n",
    "                min_price_within_hold = future_prices[:sell_index].min()\n",
    "                buy_index = future_prices[future_prices == min_price_within_hold].index[0]\n",
    "\n",
    "                # 确保买入时间早于卖出时间\n",
    "                if buy_index < sell_index:\n",
    "                    # 标记买入和卖出信号\n",
    "                    df.at[buy_index, 'signal'] = 1\n",
    "                    df.at[sell_index, 'signal'] = -1\n",
    "                    \n",
    "                    # 更新next_possible_buy，以确保在此之前不会再次买入\n",
    "                    next_possible_buy = sell_index + 1  # 确保下次买入至少在当前卖出之后\n",
    "\n",
    "    # 针对卖出后立即进行买入的情况进行修正\n",
    "    processed_signal = df['signal'].copy()\n",
    "    # 使用滑动窗口遍历信号\n",
    "    for i in range(len(processed_signal) - gap_period + 1):\n",
    "        # 当前窗口\n",
    "        window = processed_signal[i:i+gap_period]\n",
    "        # 检查窗口中的值是否满足条件\n",
    "        if set(window) == {0, 1, -1}:\n",
    "            # 如果满足条件，则将窗口内的所有信号赋值为0\n",
    "            processed_signal[i:i+gap_period] = 0\n",
    "    df['signal'] = processed_signal\n",
    "    return df[['datetime', 'signal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code = '600011'\n",
    "# 4. 获取历史信息\n",
    "stock_history_info = ak.stock_zh_a_hist(symbol=stock_code, adjust='hfq').rename(\n",
    "            columns={\n",
    "                \"日期\": \"datetime\",\n",
    "                \"开盘\": \"open\",\n",
    "                \"最高\": \"high\",\n",
    "                \"最低\": \"low\",\n",
    "                \"收盘\": \"close\",\n",
    "                \"成交量\": \"volume\",\n",
    "                \"成交额\": \"turnover\",\n",
    "                \"振幅\": \"amplitude\",\n",
    "                \"涨跌幅\": \"change_pct\",\n",
    "                \"涨跌额\": \"change_amount\",\n",
    "                \"换手率\": \"turnover_rate\",\n",
    "            }\n",
    "        )\n",
    "stock_history_info.insert(0, 'stock_code', stock_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_label_info = generate_order_signal(stock_history_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构建单一案例\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 5. 生成Target信息\n",
    "# def generate_market_timing_target(dataframe):\n",
    "#     dataframe = dataframe.sort_values(by=[\"datetime\"])\n",
    "#     # 计算过去M=10天收益率的（mean & std）\n",
    "#     dataframe[\"daily_return\"] = dataframe[\"close\"].pct_change()\n",
    "#     dataframe[\"mean_return\"] = dataframe[\"daily_return\"].transform(lambda x: x.rolling(10).mean())\n",
    "#     dataframe[\"std_return\"] = dataframe[\"daily_return\"].transform(lambda x: x.rolling(10).std())\n",
    "#     # 计算未来N=5天的收益率\n",
    "#     dataframe[\"close_in_5_days\"] = dataframe[\"close\"].shift(-5)\n",
    "#     dataframe[\"return_5_days\"] = dataframe[\"close_in_5_days\"] / dataframe[\"close\"] - 1\n",
    "#     # 构建Target\n",
    "#     dataframe[\"target\"] = 0  # 默认设置为0\n",
    "#     dataframe.loc[dataframe[\"return_5_days\"] > dataframe[\"mean_return\"] + 2 * dataframe[\"std_return\"], \"target\"] = 1 # 买入信号\n",
    "#     dataframe.loc[dataframe[\"return_5_days\"] < dataframe[\"mean_return\"] - 2 * dataframe[\"std_return\"], \"target\"] = 2 # 卖出信号\n",
    "#     # # 删除有NaN的值\n",
    "#     dataframe.dropna(subset=[\"mean_return\", \"std_return\", \"close_in_5_days\"], inplace=True)\n",
    "#     # # 生成最终的Label表\n",
    "#     dataframe = dataframe[[\"datetime\", \"target\"]]\n",
    "#     return dataframe\n",
    "# stock_target_info = generate_market_timing_target(stock_history_info[['datetime', 'close']].copy())\n",
    "\n",
    "# # 6. 生成时间特征\n",
    "# def extract_time_features(datetime_series):\n",
    "#     dataframe = pd.DataFrame()\n",
    "#     dataframe['datetime'] = datetime_series\n",
    "#     datetime_series = pd.to_datetime(datetime_series)\n",
    "#     dataframe['weekday'] = datetime_series.dt.weekday  # 星期几（0=星期一，6=星期日）\n",
    "#     dataframe['day_of_week'] = datetime_series.dt.day_name()  # 星期几的名称\n",
    "#     dataframe['day_of_month'] = datetime_series.dt.day  # 一个月中的第几天\n",
    "#     dataframe['month'] = datetime_series.dt.month  # 月份\n",
    "#     dataframe['season'] = datetime_series.dt.month.map(lambda x: {\n",
    "#         1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "#         6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Autumn', 10: 'Autumn',\n",
    "#         11: 'Autumn', 12: 'Winter'\n",
    "#     }.get(x))\n",
    "#     return dataframe\n",
    "# stock_time_info = extract_time_features(stock_history_info['datetime'].copy())\n",
    "\n",
    "# # 7. 生成价格特征\n",
    "# def extract_price_features(dataframe):\n",
    "#     dataframe.set_index(pd.DatetimeIndex(dataframe['datetime']), inplace=True)\n",
    "#     dataframe.ta.cores = 0\n",
    "#     dataframe.ta.strategy()\n",
    "#     dataframe = dataframe[[i for i in dataframe.columns if i not in ['open', 'high', 'low', 'close', 'volume']]]\n",
    "#     dataframe = dataframe.reset_index(drop=True)\n",
    "#     return dataframe\n",
    "# stock_price_info = extract_price_features(stock_history_info[['datetime', 'open', 'high', 'low', 'close', 'volume']].copy())\n",
    "\n",
    "# # 8. 特征整合wide表\n",
    "# stock_wide_info = stock_individual_info.merge(stock_history_info, on=['stock_code'], how='left').merge(stock_target_info, on=['datetime'], how='inner').merge(stock_time_info, on=['datetime'], how='inner').merge(stock_price_info, on=['datetime'], how='inner')\n",
    "# stock_wide_info.fillna(0, inplace=True)\n",
    "\n",
    "# # 9. wide表数据保存\n",
    "# stock_wide_info.to_pickle(f'./wide_data/{stock_code}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# def plot_label(df):\n",
    "#     # Plot the close prices\n",
    "#     plt.figure(figsize=(14, 7))\n",
    "#     plt.plot(df['datetime'], df['close'], label='Close Price', color='blue')\n",
    "#     # Plot the buy signals\n",
    "#     buy_signals = df[df['signal'] == 1]\n",
    "#     plt.scatter(buy_signals['datetime'], buy_signals['close'], label='Buy Signal', marker='^', color='green', alpha=1)\n",
    "#     # Plot the sell signals\n",
    "#     sell_signals = df[df['signal'] == -1]\n",
    "#     plt.scatter(sell_signals['datetime'], sell_signals['close'], label='Sell Signal', marker='v', color='red', alpha=1)\n",
    "#     # Add title and labels\n",
    "#     plt.title('Stock Price with Buy/Sell Signals')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Price')\n",
    "#     # Rotate date labels for better readability\n",
    "#     plt.xticks(rotation=45)\n",
    "#     # Show the legend\n",
    "#     plt.legend()\n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "\n",
    "# plot_label(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 18:25:19.970168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow处理原始数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stock_label_info.merge(stock_history_info[['datetime', 'open', 'close', 'high', 'low', 'volume']], on=['datetime'], how='left')\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df.sort_values('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 选择固定时间区间的数据\n",
    "train_start_date = pd.to_datetime('2000-01-01')\n",
    "train_end_date = pd.to_datetime('2020-12-31')\n",
    "val_start_date = pd.to_datetime('2021-01-01')\n",
    "val_end_date = pd.to_datetime('2021-12-31')\n",
    "test_start_date = pd.to_datetime('2022-01-01')\n",
    "test_end_date = pd.to_datetime('2022-12-31')\n",
    "\n",
    "train_df = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "val_df = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "test_df = df[(pd.to_datetime(df['datetime']) >= test_start_date) & (pd.to_datetime(df['datetime']) <= test_end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.30</td>\n",
       "      <td>13.64</td>\n",
       "      <td>14.07</td>\n",
       "      <td>13.30</td>\n",
       "      <td>1234381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.60</td>\n",
       "      <td>13.11</td>\n",
       "      <td>13.68</td>\n",
       "      <td>13.03</td>\n",
       "      <td>212267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.02</td>\n",
       "      <td>13.18</td>\n",
       "      <td>13.28</td>\n",
       "      <td>12.76</td>\n",
       "      <td>111541</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.10</td>\n",
       "      <td>12.96</td>\n",
       "      <td>13.30</td>\n",
       "      <td>12.95</td>\n",
       "      <td>56542</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.90</td>\n",
       "      <td>13.46</td>\n",
       "      <td>13.73</td>\n",
       "      <td>12.87</td>\n",
       "      <td>188862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    open  close   high    low   volume  signal\n",
       "0  13.30  13.64  14.07  13.30  1234381       0\n",
       "1  13.60  13.11  13.68  13.03   212267       0\n",
       "2  13.02  13.18  13.28  12.76   111541       0\n",
       "3  13.10  12.96  13.30  12.95    56542       0\n",
       "4  12.90  13.46  13.73  12.87   188862       0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['open', 'close', 'high', 'low', 'volume', 'signal']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(train_df[['open', 'close', 'high', 'low', 'volume', 'signal']], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, shift,\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,feature_columns=[],\n",
    "               label_columns='signal'):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df[feature_columns]\n",
    "    self.val_df = val_df[feature_columns]\n",
    "    self.test_df = test_df[feature_columns]\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in\n",
    "                                    enumerate(label_columns)} # {'signal': 0}\n",
    "      \n",
    "    self.column_indices = {name: i for i, name in\n",
    "                           enumerate(self.train_df.columns)} # {'open': 0, 'close': 1, 'high': 2, 'low': 3, 'volume': 4, 'signal': 5}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.input_width)[self.input_slice]\n",
    "\n",
    "  def __repr__(self):\n",
    "    return '\\n'.join([\n",
    "        f'Input Width: {self.input_width}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "  \n",
    "  # def split_window(self, features):\n",
    "  #   inputs = features[:, self.input_slice, :]\n",
    "  #   labels = features[:, self.labels_slice, :]\n",
    "  #   if self.label_columns is not None:\n",
    "  #     labels = tf.stack(\n",
    "  #         [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "  #         axis=-1)\n",
    "\n",
    "  #   # Slicing doesn't preserve static shape information, so set the shapes\n",
    "  #   # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "  #   inputs.set_shape([None, self.input_width, None])\n",
    "  #   labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "  #   return inputs, labels\n",
    "  \n",
    "  def make_dataset(self, data):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data[:, :-1],\n",
    "        targets=data[:, -1],\n",
    "        sequence_length=self.input_width,\n",
    "        sequence_stride=1,\n",
    "        shuffle=False,\n",
    "        batch_size=32,)\n",
    "\n",
    "    # ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "\n",
    "  @property\n",
    "  def train(self):\n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "  @property\n",
    "  def val(self):\n",
    "    return self.make_dataset(self.val_df)\n",
    "\n",
    "  @property\n",
    "  def test(self):\n",
    "    return self.make_dataset(self.test_df)\n",
    "\n",
    "  @property\n",
    "  def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "      # No example batch was found, so get one from the `.train` dataset\n",
    "      result = next(iter(self.train))\n",
    "      # And cache it for next time\n",
    "      self._example = result\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Input Width: 30\n",
       "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
       " 24 25 26 27 28 29]\n",
       "Label column name(s): ['signal']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2 = WindowGenerator(input_width=30, shift=1, feature_columns=['open', 'close', 'high', 'low', 'volume', 'signal'], label_columns=['signal'])\n",
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TakeDataset element_spec=(TensorSpec(shape=(None, None, 5), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.train.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape (batch, time, features): (32, 30, 5)\n",
      "Labels shape (batch, time, features): (32,)\n"
     ]
    }
   ],
   "source": [
    "for example_inputs, example_labels in w2.train.take(1):\n",
    "  print(f'Inputs shape (batch, time, features): {example_inputs.shape}')\n",
    "  print(f'Labels shape (batch, time, features): {example_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Total: 151865, Normal: 92464,Positive: 31636, Negative:27765 \n",
      "\n",
      "Validation:\n",
      "Total: 11619, Normal: 7301,Positive: 2230, Negative:2088 \n",
      "\n",
      "Test:\n",
      "Total: 11936, Normal: 7498,Positive: 2120, Negative:2318 \n",
      "\n",
      "Weight for class 0: 0.82\n",
      "Weight for class 1: 2.40\n",
      "Weight for class 2: 2.73\n"
     ]
    }
   ],
   "source": [
    "train_0, train_1, train_2 = np.bincount(train_data['target'])\n",
    "train_total = train_0 + train_1 + train_2\n",
    "print('Train:\\nTotal: {}, Normal: {},Positive: {}, Negative:{} \\n'.format(train_total, train_0, train_1, train_2))\n",
    "\n",
    "val_0, val_1, val_2 = np.bincount(validation_data['target'])\n",
    "val_total = val_0 + val_1 + val_2\n",
    "print('Validation:\\nTotal: {}, Normal: {},Positive: {}, Negative:{} \\n'.format(val_total, val_0, val_1, val_2))\n",
    "\n",
    "test_0, test_1, test_2 = np.bincount(test_data['target'])\n",
    "test_total = test_0 + test_1 + test_2\n",
    "print('Test:\\nTotal: {}, Normal: {},Positive: {}, Negative:{} \\n'.format(test_total, test_0, test_1, test_2))\n",
    "\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / train_0) * (train_total / 2.0)\n",
    "weight_for_1 = (1 / train_1) * (train_total / 2.0)\n",
    "weight_for_2 = (1 / train_2) * (train_total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "print('Weight for class 2: {:.2f}'.format(weight_for_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_boundaries(series, num_bins=20):\n",
    "    return pd.qcut(series, num_bins, retbins=True)[1].tolist()\n",
    "\n",
    "TARGET_FEATURE_NAME = \"target\"\n",
    "TARGET_FEATURE_LABELS = [\"0\", \"1\", \"2\"]\n",
    "TARGET_FEATURE_LENGTH = len(TARGET_FEATURE_LABELS)\n",
    "\n",
    "# 连续特征分桶\n",
    "NUMERIC_FEATURES_WITH_BOUNDARIES = {\n",
    "    'open': get_numeric_boundaries(train_data['open']),\n",
    "    'close': get_numeric_boundaries(train_data['close']),\n",
    "    'high': get_numeric_boundaries(train_data['high']),\n",
    "    'low': get_numeric_boundaries(train_data['low']),\n",
    "    'volume': get_numeric_boundaries(train_data['volume']),\n",
    "    'turnover': get_numeric_boundaries(train_data['turnover']),\n",
    "    'amplitude': get_numeric_boundaries(train_data['amplitude']),\n",
    "    'change_pct': get_numeric_boundaries(train_data['change_pct']),\n",
    "    'change_amount': get_numeric_boundaries(train_data['change_amount']),\n",
    "    'turnover_rate': get_numeric_boundaries(train_data['turnover_rate'])\n",
    "}\n",
    "NUMERIC_FEATURE_NAMES = list(NUMERIC_FEATURES_WITH_BOUNDARIES.keys())\n",
    "\n",
    "# 离散特征embedding\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"industry\": sorted(list(train_data[\"industry\"].unique())),\n",
    "}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop(TARGET_FEATURE_NAME)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "train_ds = df_to_dataset(train_data)\n",
    "val_ds = df_to_dataset(test_data)\n",
    "test_ds = df_to_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"string\"\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    embedding_dim = 4\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES: # 处理连续特征\n",
    "            embedding_size = len(NUMERIC_FEATURES_WITH_BOUNDARIES[feature_name]) * 2\n",
    "            embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=embedding_size, output_dim=embedding_dim\n",
    "            )\n",
    "            lookup_layer = tf.keras.layers.Discretization(bin_boundaries=NUMERIC_FEATURES_WITH_BOUNDARIES[feature_name],output_mode='int')\n",
    "            encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        elif feature_name in CATEGORICAL_FEATURE_NAMES: # 处理类别特征\n",
    "            embedding_size = len(CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]) * 2\n",
    "            embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=embedding_size, output_dim=embedding_dim\n",
    "            )\n",
    "            lookup_layer = tf.keras.layers.Hashing(num_bins=embedding_size)\n",
    "            encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        \n",
    "        # print(encoded_feature)\n",
    "        encoded_features.append(encoded_feature)\n",
    "    \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-3\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "def run_experiment(model, train_ds, val_ds, test_ds):\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_sparse_categorical_accuracy',\n",
    "        verbose=1,\n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=NUM_EPOCH, \n",
    "        validation_data=val_ds, \n",
    "        verbose=2,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    # loss, auc = model.evaluate(test_ds, verbose=0)\n",
    "    # print(f\"Test AUC::{round(auc * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.2\n",
    "hidden_units = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/20\n",
      "4746/4746 - 4s - loss: 1.6707 - sparse_categorical_accuracy: 0.3124 - val_loss: 1.0877 - val_sparse_categorical_accuracy: 0.3660 - 4s/epoch - 939us/step\n",
      "Epoch 2/20\n",
      "4746/4746 - 4s - loss: 1.6441 - sparse_categorical_accuracy: 0.3202 - val_loss: 1.1016 - val_sparse_categorical_accuracy: 0.2666 - 4s/epoch - 794us/step\n",
      "Epoch 3/20\n",
      "4746/4746 - 4s - loss: 1.6419 - sparse_categorical_accuracy: 0.3321 - val_loss: 1.0973 - val_sparse_categorical_accuracy: 0.3288 - 4s/epoch - 797us/step\n",
      "Epoch 4/20\n",
      "4746/4746 - 4s - loss: 1.6400 - sparse_categorical_accuracy: 0.3355 - val_loss: 1.1016 - val_sparse_categorical_accuracy: 0.2805 - 4s/epoch - 796us/step\n",
      "Epoch 5/20\n",
      "4746/4746 - 4s - loss: 1.6392 - sparse_categorical_accuracy: 0.3363 - val_loss: 1.0998 - val_sparse_categorical_accuracy: 0.2874 - 4s/epoch - 793us/step\n",
      "Epoch 6/20\n",
      "4746/4746 - 4s - loss: 1.6374 - sparse_categorical_accuracy: 0.3379 - val_loss: 1.1015 - val_sparse_categorical_accuracy: 0.2721 - 4s/epoch - 795us/step\n",
      "Epoch 7/20\n",
      "4746/4746 - 4s - loss: 1.6368 - sparse_categorical_accuracy: 0.3324 - val_loss: 1.0844 - val_sparse_categorical_accuracy: 0.3567 - 4s/epoch - 801us/step\n",
      "Epoch 8/20\n",
      "4746/4746 - 4s - loss: 1.6358 - sparse_categorical_accuracy: 0.3327 - val_loss: 1.0900 - val_sparse_categorical_accuracy: 0.3263 - 4s/epoch - 794us/step\n",
      "Epoch 9/20\n",
      "4746/4746 - 4s - loss: 1.6347 - sparse_categorical_accuracy: 0.3350 - val_loss: 1.1082 - val_sparse_categorical_accuracy: 0.2517 - 4s/epoch - 791us/step\n",
      "Epoch 10/20\n",
      "4746/4746 - 4s - loss: 1.6347 - sparse_categorical_accuracy: 0.3325 - val_loss: 1.0968 - val_sparse_categorical_accuracy: 0.2987 - 4s/epoch - 794us/step\n",
      "Epoch 11/20\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "4746/4746 - 4s - loss: 1.6332 - sparse_categorical_accuracy: 0.3312 - val_loss: 1.1031 - val_sparse_categorical_accuracy: 0.2523 - 4s/epoch - 795us/step\n",
      "Epoch 11: early stopping\n",
      "Model training finished\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_model(output_bias=None):\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "\n",
    "    for units in hidden_units:\n",
    "        features = tf.keras.layers.Dense(units)(features)\n",
    "        features = tf.keras.layers.BatchNormalization()(features)\n",
    "        features = tf.keras.layers.ReLU()(features)\n",
    "        features = tf.keras.layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    # outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(features)\n",
    "    outputs = tf.keras.layers.Dense(units=TARGET_FEATURE_LENGTH, activation=\"softmax\")(features)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "# tf.keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n",
    "run_experiment(baseline_model, train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/20\n",
      "4746/4746 - 6s - loss: 1.6681 - sparse_categorical_accuracy: 0.3398 - val_loss: 1.1178 - val_sparse_categorical_accuracy: 0.2932 - 6s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "4746/4746 - 5s - loss: 1.6443 - sparse_categorical_accuracy: 0.3438 - val_loss: 1.1131 - val_sparse_categorical_accuracy: 0.2723 - 5s/epoch - 955us/step\n",
      "Epoch 3/20\n",
      "4746/4746 - 5s - loss: 1.6425 - sparse_categorical_accuracy: 0.3458 - val_loss: 1.0958 - val_sparse_categorical_accuracy: 0.3335 - 5s/epoch - 965us/step\n",
      "Epoch 4/20\n",
      "4746/4746 - 5s - loss: 1.6409 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.0939 - val_sparse_categorical_accuracy: 0.3311 - 5s/epoch - 958us/step\n",
      "Epoch 5/20\n",
      "4746/4746 - 5s - loss: 1.6398 - sparse_categorical_accuracy: 0.3436 - val_loss: 1.1007 - val_sparse_categorical_accuracy: 0.3138 - 5s/epoch - 968us/step\n",
      "Epoch 6/20\n",
      "4746/4746 - 5s - loss: 1.6381 - sparse_categorical_accuracy: 0.3452 - val_loss: 1.0999 - val_sparse_categorical_accuracy: 0.2998 - 5s/epoch - 964us/step\n",
      "Epoch 7/20\n",
      "4746/4746 - 5s - loss: 1.6364 - sparse_categorical_accuracy: 0.3419 - val_loss: 1.1153 - val_sparse_categorical_accuracy: 0.2637 - 5s/epoch - 982us/step\n",
      "Epoch 8/20\n",
      "4746/4746 - 5s - loss: 1.6363 - sparse_categorical_accuracy: 0.3393 - val_loss: 1.1134 - val_sparse_categorical_accuracy: 0.2625 - 5s/epoch - 964us/step\n",
      "Epoch 9/20\n",
      "4746/4746 - 5s - loss: 1.6351 - sparse_categorical_accuracy: 0.3448 - val_loss: 1.1002 - val_sparse_categorical_accuracy: 0.3118 - 5s/epoch - 954us/step\n",
      "Epoch 10/20\n",
      "4746/4746 - 5s - loss: 1.6341 - sparse_categorical_accuracy: 0.3436 - val_loss: 1.0853 - val_sparse_categorical_accuracy: 0.3626 - 5s/epoch - 962us/step\n",
      "Epoch 11/20\n",
      "4746/4746 - 5s - loss: 1.6338 - sparse_categorical_accuracy: 0.3440 - val_loss: 1.0841 - val_sparse_categorical_accuracy: 0.3405 - 5s/epoch - 978us/step\n",
      "Epoch 12/20\n",
      "4746/4746 - 5s - loss: 1.6331 - sparse_categorical_accuracy: 0.3405 - val_loss: 1.1041 - val_sparse_categorical_accuracy: 0.3057 - 5s/epoch - 957us/step\n",
      "Epoch 13/20\n",
      "4746/4746 - 5s - loss: 1.6325 - sparse_categorical_accuracy: 0.3419 - val_loss: 1.1179 - val_sparse_categorical_accuracy: 0.2736 - 5s/epoch - 961us/step\n",
      "Epoch 14/20\n",
      "4746/4746 - 5s - loss: 1.6319 - sparse_categorical_accuracy: 0.3406 - val_loss: 1.1119 - val_sparse_categorical_accuracy: 0.2787 - 5s/epoch - 966us/step\n",
      "Epoch 15/20\n",
      "4746/4746 - 5s - loss: 1.6308 - sparse_categorical_accuracy: 0.3438 - val_loss: 1.1023 - val_sparse_categorical_accuracy: 0.3018 - 5s/epoch - 962us/step\n",
      "Epoch 16/20\n",
      "4746/4746 - 5s - loss: 1.6308 - sparse_categorical_accuracy: 0.3386 - val_loss: 1.1042 - val_sparse_categorical_accuracy: 0.2974 - 5s/epoch - 960us/step\n",
      "Epoch 17/20\n",
      "4746/4746 - 5s - loss: 1.6298 - sparse_categorical_accuracy: 0.3420 - val_loss: 1.1048 - val_sparse_categorical_accuracy: 0.2870 - 5s/epoch - 986us/step\n",
      "Epoch 18/20\n",
      "4746/4746 - 5s - loss: 1.6294 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.0928 - val_sparse_categorical_accuracy: 0.3164 - 5s/epoch - 968us/step\n",
      "Epoch 19/20\n",
      "4746/4746 - 5s - loss: 1.6288 - sparse_categorical_accuracy: 0.3435 - val_loss: 1.1020 - val_sparse_categorical_accuracy: 0.2905 - 5s/epoch - 958us/step\n",
      "Epoch 20/20\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "4746/4746 - 5s - loss: 1.6283 - sparse_categorical_accuracy: 0.3405 - val_loss: 1.1034 - val_sparse_categorical_accuracy: 0.2932 - 5s/epoch - 963us/step\n",
      "Epoch 20: early stopping\n",
      "Model training finished\n"
     ]
    }
   ],
   "source": [
    "def create_wide_and_deep_model():\n",
    "    inputs = create_model_inputs()\n",
    "    wide = encode_inputs(inputs)\n",
    "    wide = tf.keras.layers.BatchNormalization()(wide)\n",
    "\n",
    "    deep = encode_inputs(inputs)\n",
    "    for units in hidden_units:\n",
    "        deep = tf.keras.layers.Dense(units)(deep)\n",
    "        deep = tf.keras.layers.BatchNormalization()(deep)\n",
    "        deep = tf.keras.layers.ReLU()(deep)\n",
    "        deep = tf.keras.layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = tf.keras.layers.concatenate([wide, deep])\n",
    "    # outputs = tf.keras.layers.Dense(units=1)(merged)\n",
    "    outputs = tf.keras.layers.Dense(units=TARGET_FEATURE_LENGTH, activation=\"softmax\")(merged)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "wide_and_deep_model = create_wide_and_deep_model()\n",
    "# keras.utils.plot_model(wide_and_deep_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "run_experiment(wide_and_deep_model,train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./hh_quant_tf_wdl_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./hh_quant_tf_wdl_model/assets\n"
     ]
    }
   ],
   "source": [
    "wide_and_deep_model.save('./hh_quant_tf_wdl_model')\n",
    "reloaded_model = tf.keras.models.load_model('./hh_quant_tf_wdl_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_to_dataset(test_data.iloc[:100, :], shuffle=False, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = reloaded_model.predict(samples)\n",
    "prob = tf.nn.softmax(tf.squeeze(predictions))\n",
    "\n",
    "# print(\n",
    "#     \"This particular pet had a %.4f percent probability \"\n",
    "#     \"of getting adopted.\" % (100 * prob)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 1,\n",
       "       2, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 1,\n",
       "       1, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 2])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.numpy().argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
