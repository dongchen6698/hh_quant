{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import akshare as ak\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.options.display.max_rows=None\n",
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['688981', '688041', '601988', '601601', '600150']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 获取中证50（000016）的股票列表\n",
    "stock_code_list = ak.index_stock_cons('000016')['品种代码'].to_list()\n",
    "stock_code_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Info...: 100%|██████████| 50/50 [06:45<00:00,  8.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# 2. 构建单一案例\n",
    "# stock_code = '600028'\n",
    "for stock_code in tqdm(stock_code_list, desc='Processing Info...'):\n",
    "    # 3. 获取个股信息\n",
    "    stock_individual_info = pd.DataFrame([ak.stock_individual_info_em(symbol=stock_code).set_index('item').to_dict()['value']]).rename(columns={\n",
    "                        \"总市值\": \"total_market_cap\",\n",
    "                        \"流通市值\": \"circulating_market_cap\",\n",
    "                        \"行业\": \"industry\",\n",
    "                        \"上市时间\": \"listing_date\",\n",
    "                        \"股票代码\": \"stock_code\",\n",
    "                        \"股票简称\": \"stock_name\",\n",
    "                        \"总股本\": \"total_shares\",\n",
    "                        \"流通股\": \"circulating_shares\",\n",
    "                    })\n",
    "    stock_individual_info = stock_individual_info[['stock_code', 'industry', 'total_shares', 'circulating_shares', 'total_market_cap', 'circulating_market_cap']]\n",
    "\n",
    "    # 4. 获取历史信息\n",
    "    stock_history_info = ak.stock_zh_a_hist(symbol=stock_code, adjust='').rename(\n",
    "                columns={\n",
    "                    \"日期\": \"datetime\",\n",
    "                    \"开盘\": \"open\",\n",
    "                    \"最高\": \"high\",\n",
    "                    \"最低\": \"low\",\n",
    "                    \"收盘\": \"close\",\n",
    "                    \"成交量\": \"volume\",\n",
    "                    \"成交额\": \"turnover\",\n",
    "                    \"振幅\": \"amplitude\",\n",
    "                    \"涨跌幅\": \"change_pct\",\n",
    "                    \"涨跌额\": \"change_amount\",\n",
    "                    \"换手率\": \"turnover_rate\",\n",
    "                }\n",
    "            )\n",
    "    stock_history_info.insert(0, 'stock_code', stock_code)\n",
    "\n",
    "    # 5. 生成Target信息\n",
    "    def generate_market_timing_target(dataframe):\n",
    "        dataframe = dataframe.sort_values(by=[\"datetime\"])\n",
    "        # 计算过去M=10天收益率的（mean & std）\n",
    "        dataframe[\"daily_return\"] = dataframe[\"close\"].pct_change()\n",
    "        dataframe[\"mean_return\"] = dataframe[\"daily_return\"].transform(lambda x: x.rolling(10).mean())\n",
    "        dataframe[\"std_return\"] = dataframe[\"daily_return\"].transform(lambda x: x.rolling(10).std())\n",
    "        # 计算未来N=5天的收益率\n",
    "        dataframe[\"close_in_5_days\"] = dataframe[\"close\"].shift(-5)\n",
    "        dataframe[\"return_5_days\"] = dataframe[\"close_in_5_days\"] / dataframe[\"close\"] - 1\n",
    "        # 构建Target\n",
    "        dataframe[\"target\"] = 0  # 默认设置为0\n",
    "        dataframe.loc[dataframe[\"return_5_days\"] > dataframe[\"mean_return\"] + 2 * dataframe[\"std_return\"], \"target\"] = 1 # 买入信号\n",
    "        dataframe.loc[dataframe[\"return_5_days\"] < dataframe[\"mean_return\"] - 2 * dataframe[\"std_return\"], \"target\"] = 2 # 卖出信号\n",
    "        # # 删除有NaN的值\n",
    "        dataframe.dropna(subset=[\"mean_return\", \"std_return\", \"close_in_5_days\"], inplace=True)\n",
    "        # # 生成最终的Label表\n",
    "        dataframe = dataframe[[\"datetime\", \"target\"]]\n",
    "        return dataframe\n",
    "    stock_target_info = generate_market_timing_target(stock_history_info[['datetime', 'close']].copy())\n",
    "\n",
    "    # 6. 生成时间特征\n",
    "    def extract_time_features(datetime_series):\n",
    "        dataframe = pd.DataFrame()\n",
    "        dataframe['datetime'] = datetime_series\n",
    "        datetime_series = pd.to_datetime(datetime_series)\n",
    "        dataframe['weekday'] = datetime_series.dt.weekday  # 星期几（0=星期一，6=星期日）\n",
    "        dataframe['day_of_week'] = datetime_series.dt.day_name()  # 星期几的名称\n",
    "        dataframe['day_of_month'] = datetime_series.dt.day  # 一个月中的第几天\n",
    "        dataframe['month'] = datetime_series.dt.month  # 月份\n",
    "        dataframe['season'] = datetime_series.dt.month.map(lambda x: {\n",
    "            1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "            6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Autumn', 10: 'Autumn',\n",
    "            11: 'Autumn', 12: 'Winter'\n",
    "        }.get(x))\n",
    "        return dataframe\n",
    "    stock_time_info = extract_time_features(stock_history_info['datetime'].copy())\n",
    "\n",
    "    # 7. 生成价格特征\n",
    "    def extract_price_features(dataframe):\n",
    "        dataframe.set_index(pd.DatetimeIndex(dataframe['datetime']), inplace=True)\n",
    "        dataframe.ta.cores = 0\n",
    "        dataframe.ta.strategy()\n",
    "        dataframe = dataframe[[i for i in dataframe.columns if i not in ['open', 'high', 'low', 'close', 'volume']]]\n",
    "        dataframe = dataframe.reset_index(drop=True)\n",
    "        return dataframe\n",
    "    stock_price_info = extract_price_features(stock_history_info[['datetime', 'open', 'high', 'low', 'close', 'volume']].copy())\n",
    "\n",
    "    # 8. 特征整合wide表\n",
    "    stock_wide_info = stock_individual_info.merge(stock_history_info, on=['stock_code'], how='left').merge(stock_target_info, on=['datetime'], how='inner').merge(stock_time_info, on=['datetime'], how='inner').merge(stock_price_info, on=['datetime'], how='inner')\n",
    "    stock_wide_info.fillna(0, inplace=True)\n",
    "\n",
    "    # 9. wide表数据保存\n",
    "    stock_wide_info.to_pickle(f'./wide_data/{stock_code}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Local Info...: 100%|██████████| 50/50 [00:00<00:00, 76.90it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_wide_list = []\n",
    "for stock_code in tqdm(stock_code_list, desc='Loading Local Info...'):\n",
    "    df = pd.read_pickle(f'./wide_data/{stock_code}.pkl')\n",
    "    stock_wide_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(stock_wide_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 17:42:07.913803: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# 使用tensorflow处理原始数据\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 选择固定时间区间的数据\n",
    "train_start_date = pd.to_datetime('2000-01-01')\n",
    "train_end_date = pd.to_datetime('2020-12-31')\n",
    "val_start_date = pd.to_datetime('2021-01-01')\n",
    "val_end_date = pd.to_datetime('2021-12-31')\n",
    "test_start_date = pd.to_datetime('2022-01-01')\n",
    "test_end_date = pd.to_datetime('2022-12-31')\n",
    "\n",
    "train_data = df[(pd.to_datetime(df['datetime']) >= train_start_date) & (pd.to_datetime(df['datetime']) <= train_end_date)]\n",
    "validation_data = df[(pd.to_datetime(df['datetime']) >= val_start_date) & (pd.to_datetime(df['datetime']) <= val_end_date)]\n",
    "test_data = df[(pd.to_datetime(df['datetime']) >= test_start_date) & (pd.to_datetime(df['datetime']) <= test_end_date)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "Total: 151865, Normal: 92464,Positive: 31636, Negative:27765 \n",
      "\n",
      "Validation:\n",
      "Total: 11619, Normal: 7301,Positive: 2230, Negative:2088 \n",
      "\n",
      "Test:\n",
      "Total: 11936, Normal: 7498,Positive: 2120, Negative:2318 \n",
      "\n",
      "Weight for class 0: 0.82\n",
      "Weight for class 1: 2.40\n",
      "Weight for class 2: 2.73\n"
     ]
    }
   ],
   "source": [
    "train_0, train_1, train_2 = np.bincount(train_data['target'])\n",
    "train_total = train_0 + train_1 + train_2\n",
    "print('Train:\\nTotal: {}, Normal: {},Positive: {}, Negative:{} \\n'.format(train_total, train_0, train_1, train_2))\n",
    "\n",
    "val_0, val_1, val_2 = np.bincount(validation_data['target'])\n",
    "val_total = val_0 + val_1 + val_2\n",
    "print('Validation:\\nTotal: {}, Normal: {},Positive: {}, Negative:{} \\n'.format(val_total, val_0, val_1, val_2))\n",
    "\n",
    "test_0, test_1, test_2 = np.bincount(test_data['target'])\n",
    "test_total = test_0 + test_1 + test_2\n",
    "print('Test:\\nTotal: {}, Normal: {},Positive: {}, Negative:{} \\n'.format(test_total, test_0, test_1, test_2))\n",
    "\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_for_0 = (1 / train_0) * (train_total / 2.0)\n",
    "weight_for_1 = (1 / train_1) * (train_total / 2.0)\n",
    "weight_for_2 = (1 / train_2) * (train_total / 2.0)\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))\n",
    "print('Weight for class 2: {:.2f}'.format(weight_for_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_boundaries(series, num_bins=20):\n",
    "    return pd.qcut(series, num_bins, retbins=True)[1].tolist()\n",
    "\n",
    "TARGET_FEATURE_NAME = \"target\"\n",
    "TARGET_FEATURE_LABELS = [\"0\", \"1\", \"2\"]\n",
    "TARGET_FEATURE_LENGTH = len(TARGET_FEATURE_LABELS)\n",
    "\n",
    "# 连续特征分桶\n",
    "NUMERIC_FEATURES_WITH_BOUNDARIES = {\n",
    "    'open': get_numeric_boundaries(train_data['open']),\n",
    "    'close': get_numeric_boundaries(train_data['close']),\n",
    "    'high': get_numeric_boundaries(train_data['high']),\n",
    "    'low': get_numeric_boundaries(train_data['low']),\n",
    "    'volume': get_numeric_boundaries(train_data['volume']),\n",
    "    'turnover': get_numeric_boundaries(train_data['turnover']),\n",
    "    'amplitude': get_numeric_boundaries(train_data['amplitude']),\n",
    "    'change_pct': get_numeric_boundaries(train_data['change_pct']),\n",
    "    'change_amount': get_numeric_boundaries(train_data['change_amount']),\n",
    "    'turnover_rate': get_numeric_boundaries(train_data['turnover_rate'])\n",
    "}\n",
    "NUMERIC_FEATURE_NAMES = list(NUMERIC_FEATURES_WITH_BOUNDARIES.keys())\n",
    "\n",
    "# 离散特征embedding\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"industry\": sorted(list(train_data[\"industry\"].unique())),\n",
    "}\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop(TARGET_FEATURE_NAME)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "train_ds = df_to_dataset(train_data)\n",
    "val_ds = df_to_dataset(test_data)\n",
    "test_ds = df_to_dataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"float32\"\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = tf.keras.layers.Input(\n",
    "                name=feature_name, shape=(), dtype=\"string\"\n",
    "            )\n",
    "    return inputs\n",
    "\n",
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    embedding_dim = 4\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES: # 处理连续特征\n",
    "            embedding_size = len(NUMERIC_FEATURES_WITH_BOUNDARIES[feature_name]) * 2\n",
    "            embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=embedding_size, output_dim=embedding_dim\n",
    "            )\n",
    "            lookup_layer = tf.keras.layers.Discretization(bin_boundaries=NUMERIC_FEATURES_WITH_BOUNDARIES[feature_name],output_mode='int')\n",
    "            encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        elif feature_name in CATEGORICAL_FEATURE_NAMES: # 处理类别特征\n",
    "            embedding_size = len(CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]) * 2\n",
    "            embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=embedding_size, output_dim=embedding_dim\n",
    "            )\n",
    "            lookup_layer = tf.keras.layers.Hashing(num_bins=embedding_size)\n",
    "            encoded_feature = embedding(lookup_layer(inputs[feature_name]))\n",
    "        \n",
    "        # print(encoded_feature)\n",
    "        encoded_features.append(encoded_feature)\n",
    "    \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-3\n",
    "NUM_EPOCH = 50\n",
    "\n",
    "def run_experiment(model, train_ds, val_ds, test_ds):\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_sparse_categorical_accuracy',\n",
    "        verbose=1,\n",
    "        patience=10,\n",
    "        mode='max',\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_ds, \n",
    "        epochs=NUM_EPOCH, \n",
    "        validation_data=val_ds, \n",
    "        verbose=2,\n",
    "        callbacks=[early_stopping],\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    # loss, auc = model.evaluate(test_ds, verbose=0)\n",
    "    # print(f\"Test AUC::{round(auc * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.2\n",
    "hidden_units = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/20\n",
      "4746/4746 - 4s - loss: 1.6707 - sparse_categorical_accuracy: 0.3124 - val_loss: 1.0877 - val_sparse_categorical_accuracy: 0.3660 - 4s/epoch - 939us/step\n",
      "Epoch 2/20\n",
      "4746/4746 - 4s - loss: 1.6441 - sparse_categorical_accuracy: 0.3202 - val_loss: 1.1016 - val_sparse_categorical_accuracy: 0.2666 - 4s/epoch - 794us/step\n",
      "Epoch 3/20\n",
      "4746/4746 - 4s - loss: 1.6419 - sparse_categorical_accuracy: 0.3321 - val_loss: 1.0973 - val_sparse_categorical_accuracy: 0.3288 - 4s/epoch - 797us/step\n",
      "Epoch 4/20\n",
      "4746/4746 - 4s - loss: 1.6400 - sparse_categorical_accuracy: 0.3355 - val_loss: 1.1016 - val_sparse_categorical_accuracy: 0.2805 - 4s/epoch - 796us/step\n",
      "Epoch 5/20\n",
      "4746/4746 - 4s - loss: 1.6392 - sparse_categorical_accuracy: 0.3363 - val_loss: 1.0998 - val_sparse_categorical_accuracy: 0.2874 - 4s/epoch - 793us/step\n",
      "Epoch 6/20\n",
      "4746/4746 - 4s - loss: 1.6374 - sparse_categorical_accuracy: 0.3379 - val_loss: 1.1015 - val_sparse_categorical_accuracy: 0.2721 - 4s/epoch - 795us/step\n",
      "Epoch 7/20\n",
      "4746/4746 - 4s - loss: 1.6368 - sparse_categorical_accuracy: 0.3324 - val_loss: 1.0844 - val_sparse_categorical_accuracy: 0.3567 - 4s/epoch - 801us/step\n",
      "Epoch 8/20\n",
      "4746/4746 - 4s - loss: 1.6358 - sparse_categorical_accuracy: 0.3327 - val_loss: 1.0900 - val_sparse_categorical_accuracy: 0.3263 - 4s/epoch - 794us/step\n",
      "Epoch 9/20\n",
      "4746/4746 - 4s - loss: 1.6347 - sparse_categorical_accuracy: 0.3350 - val_loss: 1.1082 - val_sparse_categorical_accuracy: 0.2517 - 4s/epoch - 791us/step\n",
      "Epoch 10/20\n",
      "4746/4746 - 4s - loss: 1.6347 - sparse_categorical_accuracy: 0.3325 - val_loss: 1.0968 - val_sparse_categorical_accuracy: 0.2987 - 4s/epoch - 794us/step\n",
      "Epoch 11/20\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "4746/4746 - 4s - loss: 1.6332 - sparse_categorical_accuracy: 0.3312 - val_loss: 1.1031 - val_sparse_categorical_accuracy: 0.2523 - 4s/epoch - 795us/step\n",
      "Epoch 11: early stopping\n",
      "Model training finished\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_model(output_bias=None):\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "\n",
    "    for units in hidden_units:\n",
    "        features = tf.keras.layers.Dense(units)(features)\n",
    "        features = tf.keras.layers.BatchNormalization()(features)\n",
    "        features = tf.keras.layers.ReLU()(features)\n",
    "        features = tf.keras.layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    # outputs = tf.keras.layers.Dense(units=1, activation=\"sigmoid\")(features)\n",
    "    outputs = tf.keras.layers.Dense(units=TARGET_FEATURE_LENGTH, activation=\"softmax\")(features)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "# tf.keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")\n",
    "run_experiment(baseline_model, train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/20\n",
      "4746/4746 - 6s - loss: 1.6681 - sparse_categorical_accuracy: 0.3398 - val_loss: 1.1178 - val_sparse_categorical_accuracy: 0.2932 - 6s/epoch - 1ms/step\n",
      "Epoch 2/20\n",
      "4746/4746 - 5s - loss: 1.6443 - sparse_categorical_accuracy: 0.3438 - val_loss: 1.1131 - val_sparse_categorical_accuracy: 0.2723 - 5s/epoch - 955us/step\n",
      "Epoch 3/20\n",
      "4746/4746 - 5s - loss: 1.6425 - sparse_categorical_accuracy: 0.3458 - val_loss: 1.0958 - val_sparse_categorical_accuracy: 0.3335 - 5s/epoch - 965us/step\n",
      "Epoch 4/20\n",
      "4746/4746 - 5s - loss: 1.6409 - sparse_categorical_accuracy: 0.3446 - val_loss: 1.0939 - val_sparse_categorical_accuracy: 0.3311 - 5s/epoch - 958us/step\n",
      "Epoch 5/20\n",
      "4746/4746 - 5s - loss: 1.6398 - sparse_categorical_accuracy: 0.3436 - val_loss: 1.1007 - val_sparse_categorical_accuracy: 0.3138 - 5s/epoch - 968us/step\n",
      "Epoch 6/20\n",
      "4746/4746 - 5s - loss: 1.6381 - sparse_categorical_accuracy: 0.3452 - val_loss: 1.0999 - val_sparse_categorical_accuracy: 0.2998 - 5s/epoch - 964us/step\n",
      "Epoch 7/20\n",
      "4746/4746 - 5s - loss: 1.6364 - sparse_categorical_accuracy: 0.3419 - val_loss: 1.1153 - val_sparse_categorical_accuracy: 0.2637 - 5s/epoch - 982us/step\n",
      "Epoch 8/20\n",
      "4746/4746 - 5s - loss: 1.6363 - sparse_categorical_accuracy: 0.3393 - val_loss: 1.1134 - val_sparse_categorical_accuracy: 0.2625 - 5s/epoch - 964us/step\n",
      "Epoch 9/20\n",
      "4746/4746 - 5s - loss: 1.6351 - sparse_categorical_accuracy: 0.3448 - val_loss: 1.1002 - val_sparse_categorical_accuracy: 0.3118 - 5s/epoch - 954us/step\n",
      "Epoch 10/20\n",
      "4746/4746 - 5s - loss: 1.6341 - sparse_categorical_accuracy: 0.3436 - val_loss: 1.0853 - val_sparse_categorical_accuracy: 0.3626 - 5s/epoch - 962us/step\n",
      "Epoch 11/20\n",
      "4746/4746 - 5s - loss: 1.6338 - sparse_categorical_accuracy: 0.3440 - val_loss: 1.0841 - val_sparse_categorical_accuracy: 0.3405 - 5s/epoch - 978us/step\n",
      "Epoch 12/20\n",
      "4746/4746 - 5s - loss: 1.6331 - sparse_categorical_accuracy: 0.3405 - val_loss: 1.1041 - val_sparse_categorical_accuracy: 0.3057 - 5s/epoch - 957us/step\n",
      "Epoch 13/20\n",
      "4746/4746 - 5s - loss: 1.6325 - sparse_categorical_accuracy: 0.3419 - val_loss: 1.1179 - val_sparse_categorical_accuracy: 0.2736 - 5s/epoch - 961us/step\n",
      "Epoch 14/20\n",
      "4746/4746 - 5s - loss: 1.6319 - sparse_categorical_accuracy: 0.3406 - val_loss: 1.1119 - val_sparse_categorical_accuracy: 0.2787 - 5s/epoch - 966us/step\n",
      "Epoch 15/20\n",
      "4746/4746 - 5s - loss: 1.6308 - sparse_categorical_accuracy: 0.3438 - val_loss: 1.1023 - val_sparse_categorical_accuracy: 0.3018 - 5s/epoch - 962us/step\n",
      "Epoch 16/20\n",
      "4746/4746 - 5s - loss: 1.6308 - sparse_categorical_accuracy: 0.3386 - val_loss: 1.1042 - val_sparse_categorical_accuracy: 0.2974 - 5s/epoch - 960us/step\n",
      "Epoch 17/20\n",
      "4746/4746 - 5s - loss: 1.6298 - sparse_categorical_accuracy: 0.3420 - val_loss: 1.1048 - val_sparse_categorical_accuracy: 0.2870 - 5s/epoch - 986us/step\n",
      "Epoch 18/20\n",
      "4746/4746 - 5s - loss: 1.6294 - sparse_categorical_accuracy: 0.3390 - val_loss: 1.0928 - val_sparse_categorical_accuracy: 0.3164 - 5s/epoch - 968us/step\n",
      "Epoch 19/20\n",
      "4746/4746 - 5s - loss: 1.6288 - sparse_categorical_accuracy: 0.3435 - val_loss: 1.1020 - val_sparse_categorical_accuracy: 0.2905 - 5s/epoch - 958us/step\n",
      "Epoch 20/20\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "4746/4746 - 5s - loss: 1.6283 - sparse_categorical_accuracy: 0.3405 - val_loss: 1.1034 - val_sparse_categorical_accuracy: 0.2932 - 5s/epoch - 963us/step\n",
      "Epoch 20: early stopping\n",
      "Model training finished\n"
     ]
    }
   ],
   "source": [
    "def create_wide_and_deep_model():\n",
    "    inputs = create_model_inputs()\n",
    "    wide = encode_inputs(inputs)\n",
    "    wide = tf.keras.layers.BatchNormalization()(wide)\n",
    "\n",
    "    deep = encode_inputs(inputs)\n",
    "    for units in hidden_units:\n",
    "        deep = tf.keras.layers.Dense(units)(deep)\n",
    "        deep = tf.keras.layers.BatchNormalization()(deep)\n",
    "        deep = tf.keras.layers.ReLU()(deep)\n",
    "        deep = tf.keras.layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = tf.keras.layers.concatenate([wide, deep])\n",
    "    # outputs = tf.keras.layers.Dense(units=1)(merged)\n",
    "    outputs = tf.keras.layers.Dense(units=TARGET_FEATURE_LENGTH, activation=\"softmax\")(merged)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "wide_and_deep_model = create_wide_and_deep_model()\n",
    "# keras.utils.plot_model(wide_and_deep_model, show_shapes=True, rankdir=\"LR\")\n",
    "\n",
    "run_experiment(wide_and_deep_model,train_ds, val_ds, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./hh_quant_tf_wdl_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./hh_quant_tf_wdl_model/assets\n"
     ]
    }
   ],
   "source": [
    "wide_and_deep_model.save('./hh_quant_tf_wdl_model')\n",
    "reloaded_model = tf.keras.models.load_model('./hh_quant_tf_wdl_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_to_dataset(test_data.iloc[:100, :], shuffle=False, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/10 [==>...........................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = reloaded_model.predict(samples)\n",
    "prob = tf.nn.softmax(tf.squeeze(predictions))\n",
    "\n",
    "# print(\n",
    "#     \"This particular pet had a %.4f percent probability \"\n",
    "#     \"of getting adopted.\" % (100 * prob)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 1,\n",
       "       2, 0, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 2, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 1,\n",
       "       1, 2, 1, 2, 1, 1, 2, 1, 0, 1, 1, 2])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.numpy().argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
